import asyncio
import os
import pandas as pd
import re
from dotenv import load_dotenv

# 1. ç’°å¢ƒè¨­å®š
env_path = r"C:\dev\ishikawa-Chatbot\ishikawa-Chatbot.env"
if os.path.exists(env_path):
    load_dotenv(env_path)
else:
    load_dotenv()

from langsmith import Client, aevaluate
from langsmith.schemas import Run, Example

try:
    from core.database import SupabaseClientManager 
    from services.llm import LLMService
    from services.search import SearchService
    from core.constants import PARAMS 
    from services import prompts 
except ImportError as e:
    print(f"âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    exit(1)

INPUT_FILE = "è³ªå•ã¨ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ã®å›ç­”é›†.xlsx"
DATASET_NAME = "SGU_Evaluation_Clean_v1"  # ã•ã£ãä½œã£ãŸæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
NUM_TEST_QUESTIONS = 10 

# ==========================================
# 1. ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆå‡¦ç† (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ + Rerank + LitM)
# ==========================================
async def chatbot_pipeline(inputs: dict, config: str) -> dict:
    llm = LLMService()
    db = SupabaseClientManager(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_SERVICE_KEY"))
    search = SearchService(llm)

    question = inputs.get("question")
    if not question: return {"answer": "è³ªå•ãªã—"}

    # configãŒ 'after' ãªã®ã§ use_full ã¯ True ã«ãªã‚Šã¾ã™
    use_full = (config == 'after')
    
    try:
        # 1. ã‚¯ã‚¨ãƒªæ‹¡å¼µ (Afterã®ã¿)
        query = await search.expand_query(question) if use_full else question
        
        # 2. æ¤œç´¢ (å–å¾—æ•°ã‚’20â†’10ã«æ¸›ã‚‰ã—ã¦APIç¯€ç´„)
        emb = await llm.get_embedding(query)
        raw_docs = db.client.rpc("match_documents_hybrid", {
            "p_collection_name": "student-knowledge-base",
            "p_query_text": query,
            "p_query_embedding": emb,
            "p_match_count": 10  # ã€ä¿®æ­£ã€‘APIåˆ¶é™å›é¿ã®ãŸã‚20ã‹ã‚‰10ã¸å‰Šæ¸›
        }).execute().data or []

        # 3. åŠ å·¥å‡¦ç†
        if use_full and raw_docs:
            # Afterãƒ¢ãƒ¼ãƒ‰: é‡è¤‡ã‚«ãƒƒãƒˆã€Rerankã€é…ç½®å¤‰æ›´ã‚’å®Ÿè¡Œ
            docs = search.filter_diversity(raw_docs)
            docs = await search.rerank(question, docs, top_k=5)
            docs = search.reorder_litm(docs)
        else:
            # Beforeãƒ¢ãƒ¼ãƒ‰: ä¸Šä½5ä»¶ã‚’ãã®ã¾ã¾å–å¾—
            docs = raw_docs[:5]

        # 4. ç”Ÿæˆ
        context_str = "\n\n".join([d.get('content','') for d in docs])
        prompt = f"è³ªå•: {question}\n\n<context>\n{context_str}\n</context>"
        
        res_stream = await llm.generate_stream(prompt, prompts.SYSTEM_GENERATION)
        answer = ""
        async for chunk in res_stream:
            if chunk.text: answer += chunk.text
        
        return {
            "answer": answer.strip(),
            "contexts": [d.get('content','') for d in docs] 
        }
        
    except Exception as e:
        return {"answer": f"Error: {e}"}

# ==========================================
# 2. è‡ªå‹•æ¡ç‚¹
# ==========================================
async def quality_evaluator(run: Run, example: Example) -> dict:
    llm = LLMService()
    student_ans = run.outputs.get("answer", "")
    question = example.inputs.get("question", "")
    ground_truth = example.outputs.get("answer", "")

    # æ­£è§£ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯æ¡ç‚¹ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰
    if not ground_truth:
        return {"key": "accuracy", "score": 0.0}

    prompt = f"""
    [è³ªå•]: {question}
    [æ­£è§£]: {ground_truth}
    [å›ç­”]: {student_ans}
    
    ä¸Šè¨˜ã‚’0-10ç‚¹ã§æ¡ç‚¹ã—ã€æœ€å¾Œã« "Score: æ•°å€¤" ã¨æ›¸ã„ã¦ãã ã•ã„ã€‚
    """
    try:
        res = await llm.generate_stream(prompt)
        text = ""
        async for chunk in res:
            if chunk.text: text += chunk.text
        
        match = re.search(r'Score:\s*(\d+)', text)
        score = int(match.group(1)) if match else 0
        return {"key": "accuracy", "score": score / 10.0}
    except:
        return {"key": "accuracy", "score": 0.0}

# ==========================================
# 3. ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========================================
async def main():
    print(f"ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {INPUT_FILE}")
    ls_client = Client()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèªï¼ˆãªã‘ã‚Œã°ä½œã‚‹ãŒã€åŸºæœ¬ã¯ã‚ã‚‹ã¯ãšï¼‰
    if not ls_client.has_dataset(dataset_name=DATASET_NAME):
        print(f"ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...")
        df = pd.read_excel(INPUT_FILE).head(NUM_TEST_QUESTIONS)
        ds = ls_client.create_dataset(dataset_name=DATASET_NAME)
        for _, row in df.iterrows():
            ls_client.create_example(
                inputs={"question": str(row["Question"])},
                outputs={"answer": str(row.get("Answer", ""))},
                dataset_id=ds.id
            )
    else:
        print(f"âœ… æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ '{DATASET_NAME}' ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    
    # ã€é‡è¦ã€‘ãƒ¢ãƒ¼ãƒ‰ã‚’ 'after' (æ”¹å–„ç‰ˆ) ã«è¨­å®š
    for mode in ['before']:
        print(f"\nğŸš€ å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰: [{mode.upper()}] (Rerank & LitM) ã‚’å®Ÿè¡Œä¸­...")
        
        async def target_wrapper(inputs):
            # ã€ä¿®æ­£ã€‘APIã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚å¾…æ©Ÿæ™‚é–“ã‚’å»¶é•· (15ç§’)
            print("â³ APIåˆ¶é™å›é¿ã®ãŸã‚ 15ç§’ å¾…æ©Ÿä¸­...")
            await asyncio.sleep(15) 
            return await chatbot_pipeline(inputs, mode)

        try:
            await aevaluate(
                target_wrapper, 
                data=DATASET_NAME, 
                evaluators=[quality_evaluator],
                # ã€é‡è¦ã€‘å®Ÿé¨“åã‚’å¤‰æ›´ã—ã¦ Baseline ã¨åŒºåˆ¥ã™ã‚‹
                experiment_prefix=f"After_Rerank_LitM", 
                max_concurrency=1
            )
        except Exception as e:
            print(f"âŒ è©•ä¾¡ã‚¨ãƒ©ãƒ¼ ({mode}): {e}")

    print("\nâœ¨ æ”¹å–„ç‰ˆã®è¨ˆæ¸¬å®Œäº†ï¼LangSmithã§ Baseline ã¨æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    asyncio.run(main())