import logging
import uuid
import json
import asyncio
import re
import os
from typing import List, Dict, Any, AsyncGenerator, Optional
from concurrent.futures import ThreadPoolExecutor
from difflib import SequenceMatcher
import typing_extensions as typing

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold, GenerationConfig
from fastapi import Request
from dotenv import load_dotenv

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from core.config import GEMINI_API_KEY
from core import database as core_database
from models.schemas import ChatQuery
from services.utils import format_urls_as_links

# -----------------------------------------------------------------------------
# 1. è¨­å®š & å®šæ•°å®šç¾©
# -----------------------------------------------------------------------------
load_dotenv()
genai.configure(api_key=GEMINI_API_KEY)

# ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
USE_MODEL = "gemini-2.5-flash" 

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
PARAMS = {
    "QA_SIMILARITY_THRESHOLD": 0.92, 
    "RERANK_SCORE_THRESHOLD": 5.5,   
    "MAX_HISTORY_LENGTH": 10,        
}

# ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®š
SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
}

# ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
AI_MESSAGES = {
    "NOT_FOUND": (
        "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã”è³ªå•ã«é–¢ã—ã¦ã€å­¦å†…ã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰"
        "ç¢ºå®Ÿãªæ ¹æ‹ ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
        "ä¸æ­£ç¢ºãªå›ç­”ã‚’é¿ã‘ã‚‹ãŸã‚ã€å¤§å­¦çª“å£ã¸ç›´æ¥ãŠå•ã„åˆã‚ã›ã„ãŸã ãã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
    ),
    "RATE_LIMIT": "ç¾åœ¨ã‚¢ã‚¯ã‚»ã‚¹ãŒé›†ä¸­ã—ã¦ãŠã‚Šã€å›ç­”ç”Ÿæˆã«æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã¾ã™ã€‚1åˆ†ã»ã©å¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
    "SYSTEM_ERROR": "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
    "BLOCKED": "ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãŒã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«æŠµè§¦ã—ãŸãŸã‚ã€è¡¨ç¤ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
}

# ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«
executor = ThreadPoolExecutor(max_workers=4)

# -----------------------------------------------------------------------------
# 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ & ã‚¹ã‚­ãƒ¼ãƒå®šç¾©
# -----------------------------------------------------------------------------

# ãƒªãƒ©ãƒ³ã‚¯å‡ºåŠ›ç”¨ã®å‹å®šç¾©
class RankedItem(typing.TypedDict):
    id: int
    score: float
    reason: str

class RerankResponse(typing.TypedDict):
    ranked_items: list[RankedItem]

# ãƒªãƒ©ãƒ³ã‚¯ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
PROMPT_RERANK = """
ã‚ãªãŸã¯å¤§å­¦ã®å­¦ç”Ÿèª²ã‚¹ã‚¿ãƒƒãƒ•ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå›ç­”æ ¹æ‹ ã¨ã—ã¦é©åˆ‡ã‹0-10ç‚¹ã§å³å¯†ã«æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚

è©•ä¾¡åŸºæº–:
- 10ç‚¹: è³ªå•ã®æ ¸å¿ƒçš„ãªç­”ãˆï¼ˆæ—¥ä»˜ã€é‡‘é¡ã€æ‰‹é †ãªã©ï¼‰ãŒç›´æ¥æ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚
- 5-9ç‚¹: éƒ¨åˆ†çš„ã«é–¢é€£ã™ã‚‹æƒ…å ±ã€ã¾ãŸã¯ç­”ãˆã‚’å°ãå‡ºã™ãŸã‚ã®å‰æçŸ¥è­˜ãŒå«ã¾ã‚Œã‚‹ã€‚
- 0-4ç‚¹: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ä¸€è‡´ã™ã‚‹ãŒã€æ–‡è„ˆãŒç•°ãªã‚‹ã€‚

è³ªå•: {query}

å€™è£œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:
{candidates_text}
"""

# ãƒãƒ«ãƒã‚¯ã‚¨ãƒªç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
PROMPT_MULTI_QUERY = """
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®ç¶²ç¾…æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã®ã€Œ3ã¤ã®ç•°ãªã‚‹æ¤œç´¢ã‚¯ã‚¨ãƒªã€ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
ä»¥ä¸‹ã®è¦³ç‚¹ã§ã‚¯ã‚¨ãƒªã‚’ä½œæˆã—ã€Pythonã®ãƒªã‚¹ãƒˆå½¢å¼ ["query1", "query2", "query3"] ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

1. **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ç”¨**: è³ªå•ã«å«ã¾ã‚Œã‚‹é‡è¦å˜èªï¼ˆåè©ï¼‰ã®ç¾…åˆ—
2. **æ„å‘³æ¤œç´¢ç”¨ï¼ˆå…·ä½“çš„ï¼‰**: è³ªå•ã®æ„å›³ã‚’æ±²ã¿å–ã‚Šã€ã‚ˆã‚Šå…·ä½“çš„ã«ã—ãŸæ–‡ç« 
3. **é–¢é€£èªæ¤œç´¢ç”¨**: å°‚é–€ç”¨èªã‚„é¡ç¾©èªã€è¨€ã„æ›ãˆè¡¨ç¾ã‚’å«ã‚ãŸæ–‡ç« 

è³ªå•: "{user_query}"
"""

# å›ç­”ç”Ÿæˆç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
PROMPT_SYSTEM_GENERATION = """
ã‚ãªãŸã¯æœ­å¹Œå­¦é™¢å¤§å­¦ã®å­¦ç”Ÿã‚µãƒãƒ¼ãƒˆAIã§ã™ã€‚
æä¾›ã•ã‚ŒãŸ <context> å†…ã®æƒ…å ±**ã®ã¿**ã‚’ä½¿ç”¨ã—ã¦ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

# é‡è¦ãªãƒ«ãƒ¼ãƒ«ï¼ˆå³å®ˆï¼‰
1. **æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ï¼ˆChain of Thoughtï¼‰**:
   å›ç­”ã‚’å‡ºåŠ›ã™ã‚‹å‰ã«ã€å¿…ãšã€Œè³ªå•ã®åˆ†æã€ã€Œé–¢é€£æƒ…å ±ã®æŠ½å‡ºã€ã€ŒçŸ›ç›¾ã®ç¢ºèªã€ã‚’å†…éƒ¨çš„ã«è¡Œã£ã¦ãã ã•ã„ã€‚
   æ–‡è„ˆãŒä¸æ˜ç­ãªå ´åˆã¯ã€æ¨æ¸¬ã›ãšã€Œæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€ã¨åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚

2. **æ ¹æ‹ ã®æ˜ç¤º**:
   å›ç­”ã™ã‚‹å…¨ã¦ã®äº‹å®Ÿã«ã¤ã„ã¦ã€æ ¹æ‹ ã¨ãªã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã‚’æ–‡æœ«ã« `[1]` ã®å½¢å¼ã§ä»˜è¨˜ã—ã¦ãã ã•ã„ã€‚
   ä¾‹: ã€Œæˆæ¥­æ–™ã®ç´å…¥æœŸé™ã¯5æœˆæœ«ã§ã™[1]ã€‚ã€
   
3. **ãƒˆãƒ¼ãƒ³ & ãƒãƒŠãƒ¼**:
   - å­¦ç”Ÿã«å¯„ã‚Šæ·»ã£ãŸã€ä¸å¯§ã§è¦ªã—ã¿ã‚„ã™ã„ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã€‚
   - çµè«–ã‚’å…ˆã«è¿°ã¹ã‚‹ï¼ˆPREPæ³•ï¼‰ã€‚
   - é‡è¦ãªæ—¥ä»˜ã€é‡‘é¡ã€å ´æ‰€ã¯**å¤ªå­—**ã«ã™ã‚‹ã€‚
"""

# -----------------------------------------------------------------------------
# 3. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# -----------------------------------------------------------------------------
def get_or_create_session_id(request: Request) -> str:
    session_id = request.session.get('chat_session_id')
    if not session_id:
        session_id = str(uuid.uuid4())
        request.session['chat_session_id'] = session_id
    return session_id

def log_context(session_id: str, message: str, level: str = "info"):
    msg = f"[Session: {session_id}] {message}"
    getattr(logging, level, logging.info)(msg)

def send_sse(data: Dict[str, Any]) -> str:
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

async def api_request_with_retry(func, *args, **kwargs):
    max_retries = 3
    default_delay = 2
    for attempt in range(max_retries):
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            error_str = str(e)
            if "429" in error_str or "Quota" in error_str:
                if attempt == max_retries - 1:
                    logging.error(f"API Quota Exceeded after {max_retries} retries.")
                    raise e
                wait_time = default_delay * (2 ** attempt)
                logging.warning(f"Rate limit hit. Waiting {wait_time:.1f}s. Retrying...")
                await asyncio.sleep(wait_time)
            else:
                raise e

class ChatHistoryManager:
    def __init__(self):
        self._histories: Dict[str, List[Dict[str, str]]] = {}

    def add(self, session_id: str, role: str, content: str):
        if session_id not in self._histories:
            self._histories[session_id] = []
        self._histories[session_id].append({"role": role, "content": content})
        if len(self._histories[session_id]) > PARAMS["MAX_HISTORY_LENGTH"]:
            self._histories[session_id] = self._histories[session_id][-PARAMS["MAX_HISTORY_LENGTH"]:]

history_manager = ChatHistoryManager()

# -----------------------------------------------------------------------------
# 4. ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯: é«˜åº¦ãªæ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# -----------------------------------------------------------------------------
class SearchPipeline:
    @staticmethod
    async def generate_multi_queries(user_query: str) -> List[str]:
        prompt = PROMPT_MULTI_QUERY.format(user_query=user_query)
        try:
            model = genai.GenerativeModel(USE_MODEL)
            resp = await api_request_with_retry(
                model.generate_content_async, prompt, safety_settings=SAFETY_SETTINGS
            )
            match = re.search(r'\[.*\]', resp.text, re.DOTALL)
            if match:
                queries = json.loads(match.group())
                return [q for q in queries if isinstance(q, str)]
            return [user_query]
        except Exception as e:
            logging.warning(f"Multi-query generation failed: {e}")
            return [user_query]

    @staticmethod
    async def rerank(query: str, documents: List[Dict], top_k: int = 5) -> List[Dict]:
        if not documents:
            return []
        
        candidates_text = ""
        for i, doc in enumerate(documents):
            meta = doc.get('metadata', {})
            content = doc.get('content', '')
            candidates_text += f"ID:{i} [Source:{meta.get('source', '?')}]\n{content}\n\n"

        formatted_prompt = PROMPT_RERANK.format(query=query, candidates_text=candidates_text)

        try:
            model = genai.GenerativeModel(USE_MODEL)
            resp = await api_request_with_retry(
                model.generate_content_async,
                formatted_prompt,
                generation_config=GenerationConfig(
                    response_mime_type="application/json",
                    response_schema=RerankResponse
                ),
                safety_settings=SAFETY_SETTINGS
            )
            
            data = json.loads(resp.text)
            reranked = []
            seen_indices = set()

            for item in data.get("ranked_items", []):
                idx = item.get("id")
                score = item.get("score")
                if idx is not None and 0 <= idx < len(documents) and idx not in seen_indices:
                    if score >= PARAMS["RERANK_SCORE_THRESHOLD"]:
                        doc = documents[idx]
                        doc['rerank_score'] = score
                        doc['rerank_reason'] = item.get("reason", "")
                        reranked.append(doc)
                        seen_indices.add(idx)
            
            reranked.sort(key=lambda x: x['rerank_score'], reverse=True)
            return reranked[:top_k]
        except Exception as e:
            logging.error(f"Rerank Error: {e}")
            return documents[:top_k]

    @staticmethod
    async def filter_diversity(documents: List[Dict], threshold: float = 0.65) -> List[Dict]:
        loop = asyncio.get_running_loop()
        unique_docs = []
        def _calc_sim(a, b):
            return SequenceMatcher(None, a, b).ratio()
        for doc in documents:
            content = doc.get('content', '')
            is_duplicate = False
            for selected in unique_docs:
                sim = await loop.run_in_executor(executor, _calc_sim, content, selected.get('content', ''))
                if sim > threshold:
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_docs.append(doc)
        return unique_docs

    @staticmethod
    def reorder_documents(documents: List[Dict]) -> List[Dict]:
        if not documents:
            return []
        first_half = documents[0::2]
        second_half = documents[1::2][::-1]
        return first_half + second_half

def _build_references(response_text: str, sources_map: Dict[int, str]) -> str:
    unique_refs = []
    seen_sources = set()
    cited_ids = set(map(int, re.findall(r'\[(\d+)\]', response_text)))
    for idx, src in sources_map.items():
        if src in seen_sources: continue
        if idx in cited_ids or idx <= 2:
            unique_refs.append(f"* [{idx}] {src}")
            seen_sources.add(src)
    if unique_refs:
        return "\n\n### å‚ç…§å…ƒãƒ‡ãƒ¼ã‚¿\n" + "\n".join(unique_refs)
    return ""

# -----------------------------------------------------------------------------
# 5. ãƒ¡ã‚¤ãƒ³: ãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯
# -----------------------------------------------------------------------------
async def enhanced_chat_logic(request: Request, chat_req: ChatQuery):
    session_id = get_or_create_session_id(request)
    feedback_id = str(uuid.uuid4())
    user_input = chat_req.query.strip()
    full_resp = ""
    
    # ---------------------------------------------------------
    # 0.0s: åˆæœŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    # ---------------------------------------------------------
    yield send_sse({'feedback_id': feedback_id, 'status_message': 'ğŸ¤” è³ªå•ã®æ„å›³ã‚’åˆ†è§£ã—ã¦ã„ã¾ã™...'})

    try:
        # ---------------------------------------------------------
        # Step 1: Multi-Query Generation
        # ---------------------------------------------------------
        queries = await SearchPipeline.generate_multi_queries(user_input)
        queries.append(user_input)
        queries = list(set(queries))
        
        # ---------------------------------------------------------
        # 1.5s: æ¤œç´¢é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        # ---------------------------------------------------------
        # å®Ÿéš›ã®ã‚¯ã‚¨ãƒªæ•°ã«åˆã‚ã›ã¦è¡¨ç¤ºï¼ˆä¾‹: "3ã¤ã®è¦–ç‚¹..."ï¼‰
        yield send_sse({'status_message': f'ğŸ“š {len(queries)}ã¤ã®è¦–ç‚¹ã§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¨ªæ–­æ¤œç´¢ä¸­...'})

        # ---------------------------------------------------------
        # Step 2: Parallel Hybrid Search
        # ---------------------------------------------------------
        all_raw_docs = []
        embedding_tasks = [
            genai.embed_content_async(
                model=chat_req.embedding_model,
                content=q,
                task_type="retrieval_query"
            ) for q in queries
        ]
        embeddings_results = await asyncio.gather(*embedding_tasks)
        
        for q, raw_emb_result in zip(queries, embeddings_results):
            query_embedding = raw_emb_result["embedding"]
            
            # QA DBãƒã‚§ãƒƒã‚¯ï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã¿ï¼‰
            if q == user_input:
                if qa_hits := core_database.db_client.search_fallback_qa(query_embedding, match_count=1):
                    top_qa = qa_hits[0]
                    if top_qa.get('similarity', 0) >= PARAMS["QA_SIMILARITY_THRESHOLD"]:
                        resp = format_urls_as_links(f"ã‚ˆãã‚ã‚‹ã”è³ªå•ã«å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n\n---\n{top_qa['content']}")
                        history_manager.add(session_id, "assistant", resp)
                        yield send_sse({'content': resp, 'show_feedback': True, 'feedback_id': feedback_id})
                        return

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢
            docs = core_database.db_client.search_documents_hybrid(
                collection_name=chat_req.collection,
                query_text=q,
                query_embedding=query_embedding,
                match_count=15 
            )
            all_raw_docs.extend(docs)

        if not all_raw_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        # ---------------------------------------------------------
        # Step 3: Filtering & Reranking
        # ---------------------------------------------------------
        unique_docs = await SearchPipeline.filter_diversity(all_raw_docs)
        
        # ---------------------------------------------------------
        # 2.5s: ç²¾èª­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        # ---------------------------------------------------------
        # ãƒ’ãƒƒãƒˆã—ãŸå®Ÿéš›ã®ä»¶æ•°ã‚’è¡¨ç¤º
        yield send_sse({'status_message': f'ğŸ§ ãƒ’ãƒƒãƒˆã—ãŸ{len(unique_docs)}ä»¶ã®æ–‡çŒ®ã‚’ç²¾èª­ã—ã¦ã„ã¾ã™...'})
        
        reranked_docs = await SearchPipeline.rerank(user_input, unique_docs[:25], top_k=8)
        relevant_docs = SearchPipeline.reorder_documents(reranked_docs)

        if not relevant_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        # ---------------------------------------------------------
        # 6.0s: æ¤œè¨¼ãƒ»å›ç­”ä½œæˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        # ---------------------------------------------------------
        yield send_sse({'status_message': 'âœï¸ æƒ…å ±ã«çŸ›ç›¾ãŒãªã„ã‹æ¤œè¨¼ã—ã€å›ç­”ã‚’ä½œæˆä¸­...'})
        
        # ---------------------------------------------------------
        # Step 4: Generation
        # ---------------------------------------------------------
        context_parts = []
        sources_map = {}
        for idx, doc in enumerate(relevant_docs, 1):
            src = doc.get('metadata', {}).get('source', 'ä¸æ˜')
            title = doc.get('metadata', {}).get('title', '')
            sources_map[idx] = f"{title} ({src})"
            context_parts.append(f"<doc id='{idx}' source='{src}'>\n{doc.get('content','')}\n</doc>")
        
        context_str = "\n".join(context_parts)
        full_system_prompt = f"{PROMPT_SYSTEM_GENERATION}\n<context>\n{context_str}\n</context>"

        model = genai.GenerativeModel(USE_MODEL)
        stream = await api_request_with_retry(
            model.generate_content_async,
            [full_system_prompt, f"è³ªå•: {user_input}"],
            stream=True,
            safety_settings=SAFETY_SETTINGS
        )
        
        accumulated_text = ""
        # ---------------------------------------------------------
        # 8.0s: å›ç­”ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹
        # ---------------------------------------------------------
        async for chunk in stream:
            # â˜…ä¿®æ­£: ç©ºã®ãƒãƒ£ãƒ³ã‚¯ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã¾ãªã„å®Œäº†ä¿¡å·ãªã©ï¼‰ã«ã‚ˆã‚‹ã‚¨ãƒ©ãƒ¼ã‚’å›é¿
            try:
                # chunk.text ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã ã‘ã§æ¤œè¨¼ãŒè¡Œã‚ã‚Œã‚‹ãŸã‚ã€try-exceptã§å›²ã‚€
                if chunk.text:
                    accumulated_text += chunk.text
                    yield send_sse({'content': chunk.text})
            except Exception:
                # ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã¦ã„ãªã„ãƒãƒ£ãƒ³ã‚¯ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ç­‰ï¼‰ã¯ç„¡è¦–ã—ã¦æ¬¡ã¸
                pass
        
        full_resp = accumulated_text
        
        if not full_resp:
             yield send_sse({'content': AI_MESSAGES["BLOCKED"]})
             return

        if "æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" not in full_resp:
            refs_text = _build_references(full_resp, sources_map)
            if refs_text:
                yield send_sse({'content': refs_text})
                full_resp += refs_text
        
        history_manager.add(session_id, "assistant", full_resp)

    except Exception as e:
        log_context(session_id, f"Pipeline Error: {e}", "error")
        if not full_resp:
            yield send_sse({'content': AI_MESSAGES["SYSTEM_ERROR"]})
            
    finally:
        yield send_sse({'show_feedback': True, 'feedback_id': feedback_id})

# -----------------------------------------------------------------------------
# 6. åˆ†ææ©Ÿèƒ½
# -----------------------------------------------------------------------------
async def analyze_feedback_trends(logs: List[Dict[str, Any]]) -> AsyncGenerator[str, None]:
    if not logs:
        yield send_sse({'content': 'åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚'})
        return
    summary = "\n".join([f"- è©•ä¾¡:{l.get('rating','-')} | {l.get('comment','-')[:100]}" for l in logs[:50]])
    prompt = f"ä»¥ä¸‹ã®ãƒ­ã‚°ã‚’åˆ†æã—ã¦ãã ã•ã„:\n{summary}"
    try:
        model = genai.GenerativeModel(USE_MODEL)
        stream = await api_request_with_retry(model.generate_content_async, prompt, stream=True)
        async for chunk in stream:
            if chunk.text:
                yield send_sse({'content': chunk.text})
    except Exception as e:
        yield send_sse({'content': f'åˆ†æã‚¨ãƒ©ãƒ¼: {e}'})