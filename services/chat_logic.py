import logging
import uuid
import json
import asyncio
import re
import os
from typing import List, Dict, Any, AsyncGenerator, Optional
from concurrent.futures import ThreadPoolExecutor
from difflib import SequenceMatcher
import typing_extensions as typing

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold, GenerationConfig
from fastapi import Request
from dotenv import load_dotenv

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from core.config import GEMINI_API_KEY
from core import database as core_database
from models.schemas import ChatQuery
from services.utils import format_urls_as_links

# -----------------------------------------------------------------------------
# 1. è¨­å®š & å®šæ•°å®šç¾©
# -----------------------------------------------------------------------------
load_dotenv()
genai.configure(api_key=GEMINI_API_KEY)

# ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
USE_MODEL = "gemini-2.5-flash"

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
PARAMS = {
    "QA_SIMILARITY_THRESHOLD": 0.90, # FAQã®å³ç­”ãƒ©ã‚¤ãƒ³
    "RERANK_SCORE_THRESHOLD": 4.0,   # ãƒªãƒ©ãƒ³ã‚¯è¶³åˆ‡ã‚Šãƒ©ã‚¤ãƒ³
    "MAX_HISTORY_LENGTH": 20,
}

# ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®š
SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
}

# ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
AI_MESSAGES = {
    "NOT_FOUND": (
        "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã”è³ªå•ã«é–¢é€£ã™ã‚‹ç¢ºå®Ÿãªæƒ…å ±ãŒè³‡æ–™å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        "å¤§å­¦çª“å£ã¸ç›´æ¥ãŠå•ã„åˆã‚ã›ã„ãŸã ãã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
    ),
    "RATE_LIMIT": "ç¾åœ¨ã‚¢ã‚¯ã‚»ã‚¹ãŒé›†ä¸­ã—ã¦ã„ã¾ã™ã€‚1åˆ†ã»ã©å¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
    "SYSTEM_ERROR": "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
    "BLOCKED": "ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãŒã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«æŠµè§¦ã—ãŸãŸã‚è¡¨ç¤ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
}

executor = ThreadPoolExecutor(max_workers=4)

# -----------------------------------------------------------------------------
# 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾©
# -----------------------------------------------------------------------------

class RankedItem(typing.TypedDict):
    id: int
    score: float
    reason: str

class RerankResponse(typing.TypedDict):
    ranked_items: list[RankedItem]

PROMPT_RERANK = """
ã‚ãªãŸã¯æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡AIã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå›ç­”ã®æ ¹æ‹ ã¨ã—ã¦ã©ã‚Œã»ã©é©åˆ‡ã‹ã€0ç‚¹ã‹ã‚‰10ç‚¹ã§æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚
è³ªå•: {query}
å€™è£œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:
{candidates_text}
"""

PROMPT_SYSTEM_GENERATION = """
ã‚ãªãŸã¯å¤§å­¦ã®å­¦ç”Ÿç”Ÿæ´»æ”¯æ´ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚
æä¾›ã•ã‚ŒãŸ <context> ã‚¿ã‚°å†…ã®æƒ…å ±**ã®ã¿**ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

# é‡è¦ãªãƒ«ãƒ¼ãƒ«ï¼ˆå³å®ˆï¼‰
1. **å¼•ç”¨ã®ç¾©å‹™**:
   - å›ç­”ã«ä½¿ç”¨ã—ãŸæƒ…å ±ã¯ã€å¿…ãšæ–‡æœ«ã« `` ã®å½¢å¼ã§æƒ…å ±æºIDã‚’ä»˜è¨˜ã—ã¦ãã ã•ã„ã€‚
   - ä¾‹: ã€Œæˆæ¥­æ–™ã®ç´å…¥æœŸé™ã¯4æœˆæœ«ã§ã™ã€‚ã€

2. **ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®ç¦æ­¢**:
   - <context> ã«æ›¸ã‹ã‚Œã¦ã„ãªã„ã“ã¨ã¯ã€ä¸€èˆ¬å¸¸è­˜ã§ã‚ã£ã¦ã‚‚ã€Œæƒ…å ±ãŒãªã„ãŸã‚ã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚
"""

# -----------------------------------------------------------------------------
# 3. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ & ã‚¯ãƒ©ã‚¹
# -----------------------------------------------------------------------------

def get_session_id(request: Request, query_obj: ChatQuery) -> str:
    """
    ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’å®‰å…¨ã«å–å¾—ã—ã¾ã™ã€‚
    ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚¯ã‚¨ãƒªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã©ã¡ã‚‰ã‹ã‹ã‚‰IDã‚’è§£æ±ºã—ã¾ã™ã€‚
    """
    # 1. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰æ˜ç¤ºçš„ã«é€ã‚‰ã‚ŒãŸIDã‚’å„ªå…ˆ
    if query_obj and query_obj.session_id:
        return query_obj.session_id
    
    # 2. ã‚¯ãƒƒã‚­ãƒ¼/ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã‹ã‚‰å–å¾—
    if hasattr(request, "session"):
        sid = request.session.get('chat_session_id')
        if not sid:
            sid = str(uuid.uuid4())
            request.session['chat_session_id'] = sid
        return sid
        
    # 3. æ–°è¦ç”Ÿæˆ
    return str(uuid.uuid4())

def log_context(session_id: str, message: str, level: str = "info"):
    msg = f"[Session: {session_id}] {message}"
    getattr(logging, level, logging.info)(msg)

def send_sse(data: Dict[str, Any]) -> str:
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

async def api_request_with_retry(func, *args, **kwargs):
    max_retries = 3
    default_delay = 4
    for attempt in range(max_retries):
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            error_str = str(e)
            if "429" in error_str or "Quota" in error_str:
                if attempt == max_retries - 1:
                    logging.error(f"API Quota Exceeded: {e}")
                    raise e
                match = re.search(r"retry in (\d+\.?\d*)s", error_str)
                wait_time = float(match.group(1)) + 1.0 if match else default_delay * (2 ** attempt)
                logging.warning(f"Rate limit hit. Waiting {wait_time:.1f}s...")
                await asyncio.sleep(wait_time)
            else:
                raise e

# --- â˜…HistoryManager (Lazy Propertyã§åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼å›é¿)â˜… ---
class ChatHistoryManager:
    def __init__(self):
        # ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã§ã®DBã‚¢ã‚¯ã‚»ã‚¹ã‚’å›é¿
        pass
    
    @property
    def supabase(self):
        # å®Ÿéš›ã«ä½¿ã†ã¨ãã«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—
        if core_database.db_client is None or getattr(core_database.db_client, 'client', None) is None:
            logging.error("Database client is not initialized.")
            return None
        return core_database.db_client.client

    def add(self, session_id: str, role: str, content: str):
        if not self.supabase: return
        try:
            self.supabase.table("chat_history").insert({
                "session_id": session_id,
                "role": role,
                "content": content
            }).execute()
        except Exception as e:
            logging.error(f"History add failed: {e}")

    def get_context_string(self, session_id: str, limit: int = 10) -> str:
        if not self.supabase: return ""
        try:
            res = self.supabase.table("chat_history")\
                .select("role, content, created_at")\
                .eq("session_id", session_id)\
                .order("created_at", desc=True)\
                .limit(limit)\
                .execute()
            if not res.data: return ""
            # å¤ã„é †ã«ä¸¦ã³æ›¿ãˆ
            history = sorted(res.data, key=lambda x: x['created_at'])
            return "\n".join([f"{h['role']}: {h['content']}" for h in history])
        except Exception as e:
            logging.error(f"History fetch failed: {e}")
            return ""

history_manager = ChatHistoryManager()

# -----------------------------------------------------------------------------
# 4. æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# -----------------------------------------------------------------------------
class SearchPipeline:
    @staticmethod
    async def rerank(query: str, documents: List[Dict], top_k: int = 5) -> List[Dict]:
        if not documents: return []
        
        candidates_text = ""
        for i, doc in enumerate(documents):
            meta = doc.get('metadata', {})
            snippet = doc.get('content', '')[:300].replace('\n', ' ')
            candidates_text += f"ID:{i} [Source:{meta.get('source', '?')}]\n{snippet}\n\n"

        formatted_prompt = PROMPT_RERANK.format(query=query, candidates_text=candidates_text)

        try:
            model = genai.GenerativeModel(USE_MODEL)
            resp = await api_request_with_retry(
                model.generate_content_async,
                formatted_prompt,
                generation_config=GenerationConfig(
                    response_mime_type="application/json",
                    response_schema=RerankResponse
                ),
                safety_settings=SAFETY_SETTINGS
            )
            data = json.loads(resp.text)
            reranked = []
            for item in data.get("ranked_items", []):
                idx = item.get("id")
                score = item.get("score")
                if idx is not None and 0 <= idx < len(documents):
                    if score >= PARAMS["RERANK_SCORE_THRESHOLD"]:
                        doc = documents[idx]
                        doc['rerank_score'] = score
                        reranked.append(doc)
            reranked.sort(key=lambda x: x['rerank_score'], reverse=True)
            return reranked[:top_k]
        except Exception as e:
            logging.error(f"Rerank Error: {e}")
            return documents[:top_k]

    @staticmethod
    async def filter_diversity(documents: List[Dict], threshold: float = 0.7) -> List[Dict]:
        loop = asyncio.get_running_loop()
        unique_docs = []
        def _calc_sim(a, b): return SequenceMatcher(None, a, b).ratio()

        for doc in documents:
            content = doc.get('content', '')
            is_duplicate = False
            for selected in unique_docs:
                sim = await loop.run_in_executor(executor, _calc_sim, content, selected.get('content', ''))
                if sim > threshold:
                    is_duplicate = True; break
            if not is_duplicate: unique_docs.append(doc)
        return unique_docs

    @staticmethod
    def reorder_documents(documents: List[Dict]) -> List[Dict]:
        if not documents: return []
        first_half = documents[0::2]
        second_half = documents[1::2][::-1]
        return first_half + second_half

def _build_references(response_text: str, sources_map: Dict[int, str]) -> str:
    unique_refs = []
    seen_sources = set()
    # å½¢å¼ã¾ãŸã¯ [1] å½¢å¼ã«å¯¾å¿œ
    cited_ids = set(map(int, re.findall(r'\[(?:cite:\s*)?(\d+)\]', response_text)))
    
    for idx, src in sources_map.items():
        if idx in cited_ids or idx <= 2:
            if src in seen_sources: continue
            unique_refs.append(f"* {src}")
            seen_sources.add(src)
            
    if unique_refs:
        return "\n\n### å‚ç…§å…ƒãƒ‡ãƒ¼ã‚¿\n" + "\n".join(unique_refs)
    return ""

# -----------------------------------------------------------------------------
# 5. ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯ (å¼•æ•°é †åºã‚’ä¿®æ­£)
# -----------------------------------------------------------------------------
async def enhanced_chat_logic(request: Request, query_obj: ChatQuery):
    """
    æ³¨æ„: api/chat.py ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹éš›ã€(request, query_obj) ã®é †åºã§æ¸¡ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚
    """
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã®è§£æ±º
    session_id = get_session_id(request, query_obj)
    
    feedback_id = str(uuid.uuid4())
    user_input = query_obj.query.strip()
    
    # é–‹å§‹é€šçŸ¥
    yield send_sse({
        'feedback_id': feedback_id, 
        'status_message': 'ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™...',
        'type': 'status'
    })

    full_resp = ""

    try:
        # Step 1: Embedding
        embedding_task = asyncio.create_task(
            genai.embed_content_async(
                model=query_obj.embedding_model,
                content=user_input,
                task_type="retrieval_query"
            )
        )
        
        try:
            raw_emb_result = await embedding_task
            query_embedding = raw_emb_result["embedding"]
        except Exception as e:
            log_context(session_id, f"Embedding Failed: {e}", "error")
            yield send_sse({'content': AI_MESSAGES["SYSTEM_ERROR"]})
            return

        # Step 2: FAQ Check
        if core_database.db_client: # å®‰å…¨ç­–
            qa_hits = core_database.db_client.search_fallback_qa(query_embedding, match_count=1)
            if qa_hits and qa_hits[0].get('similarity', 0) >= PARAMS["QA_SIMILARITY_THRESHOLD"]:
                top_qa = qa_hits[0]
                resp = format_urls_as_links(f"ã‚ˆãã‚ã‚‹ã”è³ªå•ã«æƒ…å ±ãŒã‚ã‚Šã¾ã—ãŸã€‚\n\n---\n{top_qa['content']}")
                history_manager.add(session_id, "assistant", resp)
                yield send_sse({'content': resp, 'show_feedback': True, 'feedback_id': feedback_id})
                return

            # Step 3: Search
            raw_docs = core_database.db_client.search_documents_hybrid(
                collection_name=query_obj.collection,
                query_text=user_input, 
                query_embedding=query_embedding,
                match_count=30
            )
        else:
            raw_docs = []

        if not raw_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        yield send_sse({'status_message': 'ğŸ§ AIãŒæ–‡çŒ®ã‚’èª­ã‚“ã§é¸å®šä¸­...', 'type': 'status'})

        # Step 4: Pipeline
        unique_docs = await SearchPipeline.filter_diversity(raw_docs)
        reranked_docs = await SearchPipeline.rerank(user_input, unique_docs[:15], top_k=query_obj.top_k)
        relevant_docs = SearchPipeline.reorder_documents(reranked_docs)

        if not relevant_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        # Step 5: Generation
        yield send_sse({'status_message': 'âœï¸ å›ç­”ã‚’åŸ·ç­†ã—ã¦ã„ã¾ã™...', 'type': 'status'})
        
        context_parts = []
        sources_map = {}
        
        for idx, doc in enumerate(relevant_docs, 1):
            src = doc.get('metadata', {}).get('source', 'ä¸æ˜')
            sources_map[idx] = src
            context_parts.append(f"<doc id='{idx}' src='{src}'>\n{doc.get('content','')}\n</doc>")
        
        context_str = "\n".join(context_parts)
        history_str = history_manager.get_context_string(session_id)
        
        full_system_prompt = f"""{PROMPT_SYSTEM_GENERATION}
        
### æ¤œç´¢ã•ã‚ŒãŸè³‡æ–™
{context_str}

### ã“ã‚Œã¾ã§ã®ä¼šè©±
{history_str}
"""
        model = genai.GenerativeModel(USE_MODEL)
        stream = await api_request_with_retry(
            model.generate_content_async,
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {user_input}",
            stream=True,
            safety_settings=SAFETY_SETTINGS
        )
        
        yield send_sse({'status_message': '', 'type': 'status'})

        async for chunk in stream:
            if chunk.text:
                full_resp += chunk.text
                yield send_sse({'content': chunk.text})
        
        if not full_resp:
             yield send_sse({'content': AI_MESSAGES["BLOCKED"]})
             return

        # Step 6: References
        if "æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" not in full_resp:
            refs_text = _build_references(full_resp, sources_map)
            if refs_text:
                yield send_sse({'content': refs_text})
                full_resp += refs_text
        
        history_manager.add(session_id, "assistant", full_resp)

    except Exception as e:
        log_context(session_id, f"Critical Pipeline Error: {e}", "error")
        if not full_resp:
            yield send_sse({'content': AI_MESSAGES["SYSTEM_ERROR"]})
            
    finally:
        yield send_sse({'show_feedback': True, 'feedback_id': feedback_id})

# -----------------------------------------------------------------------------
# 6. åˆ†ææ©Ÿèƒ½
# -----------------------------------------------------------------------------
async def analyze_feedback_trends(logs: List[Dict[str, Any]]) -> AsyncGenerator[str, None]:
    if not logs:
        yield send_sse({'content': 'ãƒ‡ãƒ¼ã‚¿ãªã—'})
        return
    summary = "\n".join([f"- {l.get('rating','-')} | {l.get('comment','-')[:50]}" for l in logs[:30]])
    try:
        model = genai.GenerativeModel(USE_MODEL)
        stream = await api_request_with_retry(
            model.generate_content_async, 
            f"åˆ†æã¨æ”¹å–„ææ¡ˆ:\n{summary}", 
            stream=True
        )
        async for chunk in stream:
            if chunk.text: yield send_sse({'content': chunk.text})
    except Exception as e:
        yield send_sse({'content': str(e)})