import logging
import uuid
import json
import asyncio
import re
from typing import List, Dict, Any, AsyncGenerator, Optional
from concurrent.futures import ThreadPoolExecutor
from difflib import SequenceMatcher

import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from fastapi import Request

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from core.config import GEMINI_API_KEY
from core import database as core_database
from models.schemas import ChatQuery
from services.utils import format_urls_as_links

# -----------------------------------------------------------------------------
# è¨­å®š & å®šæ•°
# -----------------------------------------------------------------------------
genai.configure(api_key=GEMINI_API_KEY)

# â˜…ä¿®æ­£: 2026å¹´ç¾åœ¨ã®æœ€æ–°å®‰å®šç‰ˆã‚’æŒ‡å®š
USE_MODEL = "gemini-2.5-flash"

PARAMS = {
    "QA_SIMILARITY_THRESHOLD": 0.95,
    "RERANK_SCORE_THRESHOLD": 6.5,
    "MAX_HISTORY_LENGTH": 20,
}

SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
}

AI_MESSAGES = {
    "NOT_FOUND": (
        "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã”è³ªå•ã«é–¢é€£ã™ã‚‹ç¢ºå®Ÿãªæƒ…å ±ãŒè³‡æ–™å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        "å¤§å­¦çª“å£ã¸ç›´æ¥ãŠå•ã„åˆã‚ã›ã„ãŸã ãã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
    ),
    "ERROR": "ç¾åœ¨ã‚¢ã‚¯ã‚»ã‚¹ãŒé›†ä¸­ã—ã¦ãŠã‚Šå›ç­”ã§ãã¾ã›ã‚“ã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
}

executor = ThreadPoolExecutor(max_workers=4)

# -----------------------------------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# -----------------------------------------------------------------------------
def get_or_create_session_id(request: Request) -> str:
    session_id = request.session.get('chat_session_id')
    if not session_id:
        session_id = str(uuid.uuid4())
        request.session['chat_session_id'] = session_id
    return session_id

def log_context(session_id: str, message: str, level: str = "info"):
    msg = f"[Session: {session_id}] {message}"
    getattr(logging, level, logging.info)(msg)

def send_sse(data: Dict[str, Any]) -> str:
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

def clean_and_parse_json(text: str) -> Dict[str, Any]:
    text = re.sub(r'^```json\s*', '', text)
    text = re.sub(r'^```\s*', '', text)
    text = re.sub(r'\s*```$', '', text)
    try:
        return json.loads(text.strip())
    except json.JSONDecodeError:
        return {}

async def api_request_with_retry(func, *args, **kwargs):
    """
    APIåˆ¶é™(429)å¯¾ç­–: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰å¾…æ©Ÿæ™‚é–“ã‚’è§£æã—ã¦ãƒªãƒˆãƒ©ã‚¤
    """
    max_retries = 3
    default_delay = 5  # è§£æã§ããªã‹ã£ãŸå ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¾…æ©Ÿæ™‚é–“
    for attempt in range(max_retries):
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            error_str = str(e)
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã« 429 ã‚„ Quota ãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰ãƒªãƒˆãƒ©ã‚¤å‡¦ç†ã¸
            if "429" in error_str or "Quota" in error_str:
                if attempt == max_retries - 1:
                    logging.error(f"API Quota Exceeded after {max_retries} retries.")
                    raise e
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ "retry in 55.2s" ã®ã‚ˆã†ãªç§’æ•°ã‚’æŠ½å‡º
                wait_time = default_delay
                match = re.search(r"retry in (\d+\.?\d*)s", error_str)
                if match:
                    # æŒ‡ç¤ºã•ã‚ŒãŸç§’æ•° + 1ç§’ï¼ˆå¿µã®ãŸã‚ï¼‰å¾…æ©Ÿ
                    wait_time = float(match.group(1)) + 1.0
                else:
                    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ• (5s, 10s...)
                    wait_time = default_delay * (2 ** attempt)

                logging.warning(f"Rate limit hit. Google requested wait: {wait_time:.1f}s. Retrying...")
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å¾…ãŸã›ã™ããªã„ã‚ˆã†ã€ãƒ­ã‚°ã«ã¯å‡ºã™ãŒå‡¦ç†ã¯ç¶™ç¶š
                await asyncio.sleep(wait_time)
            else:
                raise e

class ChatHistoryManager:
    def __init__(self):
        self._histories: Dict[str, List[Dict[str, str]]] = {}

    def add(self, session_id: str, role: str, content: str):
        if session_id not in self._histories:
            self._histories[session_id] = []
        self._histories[session_id].append({"role": role, "content": content})
        if len(self._histories[session_id]) > PARAMS["MAX_HISTORY_LENGTH"]:
            self._histories[session_id] = self._histories[session_id][-PARAMS["MAX_HISTORY_LENGTH"]:]

history_manager = ChatHistoryManager()

# -----------------------------------------------------------------------------
# ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯: æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# -----------------------------------------------------------------------------
class SearchPipeline:
    @staticmethod
    async def optimize_query(user_query: str, session_id: str) -> str:
        """HyDE + Query Expansion"""
        prompt = f"""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«åŸºã¥ã„ã¦ã€å¤§å­¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«æœ€é©ãªã€Œæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        å°‚é–€ç”¨èªã¸ã®è¨€ã„æ›ãˆï¼ˆä¾‹: "å–ã‚Šæ¶ˆã—" -> "å±¥ä¿®ä¸­æ­¢"ï¼‰ã‚’å«ã‚ã€å‡ºåŠ›ã¯æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã«ã—ã¦ãã ã•ã„ã€‚
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: "{user_query}"
        """
        try:
            model = genai.GenerativeModel(USE_MODEL)
            # ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§å‘¼ã³å‡ºã—
            resp = await api_request_with_retry(
                model.generate_content_async, prompt, safety_settings=SAFETY_SETTINGS
            )
            optimized = resp.text.strip()
            log_context(session_id, f"ã‚¯ã‚¨ãƒªæ‹¡å¼µ: {optimized}")
            return optimized
        except Exception:
            return user_query

    @staticmethod
    async def rerank(query: str, documents: List[Dict], top_k: int = 5) -> List[Dict]:
        """æ¤œç´¢çµæœã®ãƒªãƒ©ãƒ³ã‚¯å‡¦ç†"""
        if not documents:
            return []
        
        candidates_text = ""
        for i, doc in enumerate(documents):
            meta = doc.get('metadata', {})
            snippet = doc.get('content', '')[:2000].replace('\n', ' ')
            candidates_text += f"ID:{i} [Source:{meta.get('source', '?')}]\n{snippet}\n\n"

        prompt = f"""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå›ç­”æ ¹æ‹ ã¨ã—ã¦é©åˆ‡ã‹0-10ç‚¹ã§æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚
        è³ªå•: {query}
        å€™è£œ:
        {candidates_text}
        å‡ºåŠ›å½¢å¼(JSON): {{ "ranked_items": [{{ "id": int, "score": float, "reason": str }}] }}
        """
        try:
            model = genai.GenerativeModel(USE_MODEL)
            resp = await api_request_with_retry(
                model.generate_content_async, prompt, safety_settings=SAFETY_SETTINGS
            )
            data = clean_and_parse_json(resp.text)
            reranked = []
            for item in data.get("ranked_items", []):
                idx, score = int(item.get("id", -1)), float(item.get("score", 0))
                if 0 <= idx < len(documents) and score >= PARAMS["RERANK_SCORE_THRESHOLD"]:
                    doc = documents[idx]
                    doc['rerank_score'] = score
                    reranked.append(doc)
            reranked.sort(key=lambda x: x['rerank_score'], reverse=True)
            return reranked[:top_k]
        except Exception as e:
            logging.error(f"Rerank Error: {e}")
            return documents[:top_k]

    @staticmethod
    async def filter_diversity(documents: List[Dict], threshold: float = 0.7) -> List[Dict]:
        """MMRé¢¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé‡è¤‡æ’é™¤ï¼‰"""
        loop = asyncio.get_running_loop()
        unique_docs = []
        def _calc_sim(a, b):
            return SequenceMatcher(None, a, b).ratio()

        for doc in documents:
            content = doc.get('content', '')
            is_duplicate = False
            for selected in unique_docs:
                sim = await loop.run_in_executor(executor, _calc_sim, content, selected.get('content', ''))
                if sim > threshold:
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_docs.append(doc)
        return unique_docs

# -----------------------------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³: ãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯
# -----------------------------------------------------------------------------
async def enhanced_chat_logic(request: Request, chat_req: ChatQuery):
    session_id = get_or_create_session_id(request)
    feedback_id = str(uuid.uuid4())
    user_input = chat_req.query.strip()
    yield send_sse({'feedback_id': feedback_id})

    try:
        # --- 1. æ¤œç´¢ãƒ•ã‚§ãƒ¼ã‚º ---
        yield send_sse({'status_message': 'ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™...'})
        # [å‰Šæ¸›ãƒã‚¤ãƒ³ãƒˆ1] ã‚¯ã‚¨ãƒªæ‹¡å¼µ (optimize_query) ã‚’å»ƒæ­¢
        # task_query = asyncio.create_task(SearchPipeline.optimize_query(user_input, session_id))
        search_query = user_input  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’ãã®ã¾ã¾ä½¿ã†

        # [å‰Šæ¸›ãƒã‚¤ãƒ³ãƒˆ2] Embeddingã‚’1å›ã ã‘å®Ÿè¡Œã—ã€FAQã¨æ–‡æ›¸æ¤œç´¢ã®ä¸¡æ–¹ã§ä½¿ã„å›ã™
        # Note: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        embedding_task = asyncio.create_task(
            genai.embed_content_async(
                model=chat_req.embedding_model,
                content=user_input,
                task_type="retrieval_query"
            )
        )

        # A. FAQãƒã‚§ãƒƒã‚¯ (åŸ‹ã‚è¾¼ã¿å®Œäº†ã‚’å¾…ã¤)
        try:
            # Embeddingã‚¿ã‚¹ã‚¯ã®çµæœå–å¾—
            raw_emb_result = await embedding_task
            query_embedding = raw_emb_result["embedding"]

            # A. FAQ (Q&A) ãƒã‚§ãƒƒã‚¯
            if qa_hits := core_database.db_client.search_fallback_qa(query_embedding, match_count=1):
                top_qa = qa_hits[0]
                if top_qa.get('similarity', 0) >= PARAMS["QA_SIMILARITY_THRESHOLD"]:
                    # FAQãƒ’ãƒƒãƒˆæ™‚ã¯ã“ã“ã§çµ‚äº†ã€‚ãƒªãƒ©ãƒ³ã‚¯ã‚‚å›ç­”ç”Ÿæˆã‚‚èµ°ã‚‰ãªã„ã®ã§APIæ¶ˆè²»ã¯æœ€å°
                    resp = format_urls_as_links(f"ã‚ˆãã‚ã‚‹ã”è³ªå•ã«å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n\n---\n{top_qa['content']}")
                    history_manager.add(session_id, "assistant", resp)
                    yield send_sse({'content': resp, 'show_feedback': True, 'feedback_id': feedback_id})
                    return
        except Exception as e:
            log_context(session_id, f"FAQ Search/Embed Error: {e}", "warning")
            # ä¸‡ãŒä¸€Embeddingã«å¤±æ•—ã—ã¦ã„ãŸã‚‰ã€ã“ã®å¾Œã®æ¤œç´¢ã‚‚ã§ããªã„ãŸã‚ã‚¨ãƒ©ãƒ¼çµ‚äº†
            if 'query_embedding' not in locals():
                yield send_sse({'content': AI_MESSAGES["ERROR"]})
                return

        # B. DBæ¤œç´¢
        # ã‚¯ã‚¨ãƒªæ‹¡å¼µã‚’ã—ã¦ã„ãªã„ã®ã§ã€ã•ãã»ã©å–å¾—ã—ãŸ query_embedding ã‚’ãã®ã¾ã¾æµç”¨ (å†åº¦ã®APIã‚³ãƒ¼ãƒ«ä¸è¦)
        raw_docs = core_database.db_client.search_documents_hybrid(
            collection_name=chat_req.collection,
            query_text=search_query,       # ç”Ÿã®è³ªå•æ–‡
            query_embedding=query_embedding, # ã•ã£ãã®ãƒ™ã‚¯ãƒˆãƒ«
            match_count=30                 # ãƒªãƒ©ãƒ³ã‚¯å‰ãªã®ã§å°‘ã—åºƒã‚ã«å–ã‚‹
        )
        yield send_sse({'status_message': 'ğŸ§ AIãŒæ–‡çŒ®ã‚’èª­ã‚“ã§é¸å®šä¸­...'})
        unique_docs = await SearchPipeline.filter_diversity(raw_docs)
        # ---------------------------------------------------------
        # [ä¿®æ­£] ãƒªãƒ©ãƒ³ã‚¯ã‚’å®Ÿè¡Œï¼ˆAPIåˆ¶é™æ™‚ã®æ•‘æ¸ˆæªç½®ä»˜ãï¼‰
        # ---------------------------------------------------------
        relevant_docs = []
        try:
            # APIãŒç”Ÿãã¦ã„ã‚Œã°ã€ãƒªãƒ©ãƒ³ã‚¯ã‚’å®Ÿè¡Œã—ã¦ç²¾åº¦ã‚’é«˜ã‚ã‚‹
            # å€™è£œã‚’15ä»¶æ¸¡ã—ã€ä¸Šä½ top_k ä»¶ã«çµã‚Šè¾¼ã‚€
            relevant_docs = await SearchPipeline.rerank(user_input, unique_docs[:15], top_k=chat_req.top_k)
        except Exception as e:
            # â˜…ã“ã“ãŒé‡è¦: APIåˆ¶é™(429)ãªã©ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã®ã€Œå‘½ç¶±ã€
            log_context(session_id, f"Rerank API Failed (Fallback used): {e}", "warning")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç„¡ç†ã«ãƒªãƒ©ãƒ³ã‚¯ã›ãšã€DBæ¤œç´¢ã®ã‚¹ã‚³ã‚¢é †ï¼ˆä¸Šä½5ä»¶ï¼‰ã‚’ãã®ã¾ã¾ä½¿ã†
            # ã“ã‚Œã«ã‚ˆã‚Šã€APIã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã‚‚å›ç­”ä¸èƒ½ã«ãªã‚‰ãšã€æœ€ä½é™ã®çµæœã‚’è¿”ã›ã‚‹
            relevant_docs = unique_docs[:5]

        # --- 2. å›ç­”ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º ---
        if not relevant_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
        else:
            yield send_sse({'status_message': 'âœï¸ å›ç­”ã‚’åŸ·ç­†ã—ã¦ã„ã¾ã™...'})
            context_parts = []
            sources_map = {}
            for idx, doc in enumerate(relevant_docs, 1):
                src = doc.get('metadata', {}).get('source', 'ä¸æ˜')
                sources_map[idx] = src
                context_parts.append(f"<doc id='{idx}' src='{src}'>\n{doc.get('content','')}\n</doc>")
            system_prompt = f"""
            ã‚ãªãŸã¯æœ­å¹Œå­¦é™¢å¤§å­¦ã®å­¦ç”Ÿã‚µãƒãƒ¼ãƒˆAIã§ã™ã€‚
            ä»¥ä¸‹ã®<context>å†…ã®æƒ…å ±**ã®ã¿**ã‚’ä½¿ç”¨ã—ã¦ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

            # å›ç­”ã®ãƒ«ãƒ¼ãƒ«
            1. **æ ¹æ‹ ã®ç´ä»˜ã‘**:
            æ–‡ç« ä¸­ã®é‡è¦ãªäº‹å®Ÿã«ã¯ã€æ–‡æœ«ã« `[1]` ã®ã‚ˆã†ã«**çŸ­ã„ç•ªå·ã®ã¿**ã‚’ä»˜è¨˜ã—ã¦ãã ã•ã„ã€‚
            2. **å½¢å¼**:
            - å­¦ç”Ÿã«å¯„ã‚Šæ·»ã£ãŸã€ä¸å¯§ã§è¦ªã—ã¿ã‚„ã™ã„ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã€‚
            - èª­ã¿ã‚„ã™ã„ã‚ˆã†ã«ç®‡æ¡æ›¸ãã‚„**å¤ªå­—**ã‚’æ´»ç”¨ã™ã‚‹ã€‚
            - æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã‚‹ã€‚
            <context>
            {chr(10).join(context_parts)}
            </context>
            """
            model = genai.GenerativeModel(USE_MODEL)
            # [2å›ç›®ã®ç”ŸæˆAPIã‚³ãƒ¼ãƒ«] å›ç­”ç”Ÿæˆ
            stream = await api_request_with_retry(
                model.generate_content_async,
                [system_prompt, f"è³ªå•: {user_input}"],
                stream=True,
                safety_settings=SAFETY_SETTINGS
            )
            full_resp = ""
            async for chunk in stream:
                if chunk.text:
                    full_resp += chunk.text
                    yield send_sse({'content': chunk.text})
            # å‚ç…§å…ƒã®è¿½è¨˜
            if "æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" not in full_resp:
                refs_header = "\n\n## å‚ç…§å…ƒ\n"
                unique_refs = []
                seen_sources = set()

                for idx, src in sources_map.items():
                    if src in seen_sources: continue
                    # æœ¬æ–‡ã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹ã‹ã€ä¸Šä½3ä»¶ã¾ã§ã¯è¡¨ç¤º
                    if f"[{idx}]" in full_resp or idx <= 3:
                        unique_refs.append(f"* [{idx}] {src}")
                        seen_sources.add(src)

                if unique_refs:
                    refs_text = refs_header + "\n".join(unique_refs)
                    yield send_sse({'content': refs_text})
                    full_resp += refs_text
            history_manager.add(session_id, "assistant", full_resp)

    except Exception as e:
        log_context(session_id, f"Critical Error: {e}", "error")
        if "429" in str(e) or "Quota" in str(e):
             yield send_sse({'content': "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ç¾åœ¨ã‚¢ã‚¯ã‚»ã‚¹ãŒé›†ä¸­ã—ã¦ã„ã¾ã™ã€‚æã‚Œå…¥ã‚Šã¾ã™ãŒã€1åˆ†ã»ã©å¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"})
        else:
             yield send_sse({'content': AI_MESSAGES["ERROR"]})
    finally:
        yield send_sse({'show_feedback': True, 'feedback_id': feedback_id})

# -----------------------------------------------------------------------------
# ç®¡ç†è€…ç”¨æ©Ÿèƒ½
# -----------------------------------------------------------------------------
async def analyze_feedback_trends(logs: List[Dict[str, Any]]) -> AsyncGenerator[str, None]:
    if not logs:
        yield send_sse({'content': 'åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚'})
        return

    summary = "\n".join([f"- è©•ä¾¡:{l.get('rating','-')} | {l.get('comment','-')[:100]}" for l in logs[:50]])
    prompt = f"""
    ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆåˆ©ç”¨ãƒ­ã‚°ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownã§ä½œæˆã—ã¦ãã ã•ã„ã€‚
    ãƒ‡ãƒ¼ã‚¿:
    {summary}
    é …ç›®: 1.ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒˆãƒ¬ãƒ³ãƒ‰, 2.ä½è©•ä¾¡ã®åŸå› , 3.æ”¹å–„æ¡ˆ
    """
    try:
        model = genai.GenerativeModel(USE_MODEL)
        stream = await api_request_with_retry(model.generate_content_async, prompt, stream=True)
        async for chunk in stream:
            if chunk.text:
                yield send_sse({'content': chunk.text})
    except Exception as e:
        yield send_sse({'content': f'åˆ†æã‚¨ãƒ©ãƒ¼: {e}'})