import logging
import uuid
import json
import asyncio
import re
from typing import List, Dict, Any, AsyncGenerator, Optional
import typing_extensions as typing
from concurrent.futures import ThreadPoolExecutor

from fastapi import Request
import google.generativeai as genai
from google.generativeai.types import (
    GenerationConfig,
    HarmCategory,
    HarmBlockThreshold
)
from difflib import SequenceMatcher

# -----------------------------------------------
# å¤–éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»è¨­å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# -----------------------------------------------
from core.config import GEMINI_API_KEY
from core import database as core_database
from models.schemas import ChatQuery
from services.utils import format_urls_as_links

# APIã‚­ãƒ¼è¨­å®š
genai.configure(api_key=GEMINI_API_KEY)

# -----------------------------------------------
# å®šæ•°ãƒ»è¨­å®šå€¤
# -----------------------------------------------
STRICT_THRESHOLD = 0.80
QA_SIMILARITY_THRESHOLD = 0.95
RERANK_SCORE_THRESHOLD = 6.5    # å“è³ªã®æ‹…ä¿ã®ãŸã‚å°‘ã—å³ã—ã‚ã«è¨­å®š
MAX_HISTORY_LENGTH = 20

# ä¸Šé™ã‚’å°‘ã—çµã‚Šã€ç²¾åº¦ä½ä¸‹ï¼ˆLost in the Middleï¼‰ã‚’é˜²ã
MAX_CONTEXT_CHAR_LENGTH = 60000

AI_NOT_FOUND_TOKEN = "[[NO_RELEVANT_INFO_FOUND]]"
AI_NOT_FOUND_MESSAGE_USER = (
    "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã”è³ªå•ã«é–¢é€£ã™ã‚‹ç¢ºå®Ÿãªæƒ…å ±ãŒè³‡æ–™å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    "ä¸æ­£ç¢ºãªå›ç­”ã‚’é¿ã‘ã‚‹ãŸã‚ã€å¤§å­¦çª“å£ã¸ç›´æ¥ãŠå•ã„åˆã‚ã›ã„ãŸã ãã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
)

SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
}

# CPUãƒã‚¦ãƒ³ãƒ‰ãªå‡¦ç†ï¼ˆé¡ä¼¼åº¦è¨ˆç®—ãªã©ï¼‰ç”¨ã®Executor
executor = ThreadPoolExecutor(max_workers=4)

# -----------------------------------------------
# æ§‹é€ åŒ–å‡ºåŠ›ç”¨ã®å‹å®šç¾©
# -----------------------------------------------
class AmbiguityAnalysis(typing.TypedDict):
    is_ambiguous: bool
    response_text: str
    candidates: List[str]

class RerankItem(typing.TypedDict):
    id: int
    score: float
    reason: str

class RerankResult(typing.TypedDict):
    ranked_items: List[RerankItem]

# -----------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ & ãƒ˜ãƒ«ãƒ‘ãƒ¼
# -----------------------------------------------
def log_context(session_id: str, message: str, level: str = "info"):
    msg = f"[Session: {session_id}] {message}"
    if level == "error": logging.error(msg)
    elif level == "warning": logging.warning(msg)
    else: logging.info(msg)

class ChatHistoryManager:
    def __init__(self):
        self._histories: Dict[str, List[Dict[str, str]]] = {}

    def add_to_history(self, session_id: str, role: str, content: str):
        if session_id not in self._histories:
            self._histories[session_id] = []
        history = self._histories[session_id]
        history.append({"role": role, "content": content})
        if len(history) > MAX_HISTORY_LENGTH:
            self._histories[session_id] = history[-MAX_HISTORY_LENGTH:]

history_manager = ChatHistoryManager()

def get_or_create_session_id(request: Request) -> str:
    session_id = request.session.get('chat_session_id')
    if not session_id:
        session_id = str(uuid.uuid4())
        request.session['chat_session_id'] = session_id
    return session_id

def _compute_similarity(text1: str, text2: str) -> float:
    """CPUãƒã‚¦ãƒ³ãƒ‰ãªé¡ä¼¼åº¦è¨ˆç®—"""
    return SequenceMatcher(None, text1, text2).ratio()

async def filter_results_by_diversity_async(results: List[Dict[str, Any]], threshold: float = 0.6) -> List[Dict[str, Any]]:
    """MMRé¢¨ãƒ•ã‚£ãƒ«ã‚¿ã®éåŒæœŸãƒ©ãƒƒãƒ‘ãƒ¼ (ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„)"""
    loop = asyncio.get_running_loop()
    unique_results = []
    for doc in results:
        content = doc.get('content', '')
        is_duplicate = False
        # æ—¢å­˜ã®é¸æŠæ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨æ¯”è¼ƒ
        for selected_doc in unique_results:
            # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§é¡ä¼¼åº¦è¨ˆç®—ã‚’å®Ÿè¡Œ
            similarity = await loop.run_in_executor(
                executor,
                _compute_similarity,
                content,
                selected_doc.get('content', '')
            )
            if similarity > threshold:
                is_duplicate = True
                break
        if not is_duplicate:
            unique_results.append(doc)
    return unique_results

def clean_json_string(json_str: str) -> str:
    """GeminiãŒMarkdownã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’å«ã‚ã¦è¿”ã—ãŸå ´åˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    cleaned = re.sub(r'^```json\s*', '', json_str)
    cleaned = re.sub(r'^```\s*', '', cleaned)
    cleaned = re.sub(r'\s*```$', '', cleaned)
    return cleaned.strip()

# -----------------------------------------------
# AI ãƒ­ã‚¸ãƒƒã‚¯é–¢æ•°ç¾¤
# -----------------------------------------------

async def check_ambiguity_and_suggest_options(query: str, session_id: str) -> Dict[str, Any]:
    # å˜ç´”ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹åˆ¤å®šï¼ˆé«˜é€ŸåŒ–ï¼‰
    if len(query) > 10 and any(x in query for x in ["æ–¹æ³•", "å ´æ‰€", "ç”³è«‹", "ã«ã¤ã„ã¦", "æ•™ãˆ"]):
        return {"is_ambiguous": False}

    prompt = f"""
    ã‚ãªãŸã¯å¤§å­¦ã®ãƒ˜ãƒ«ãƒ—ãƒ‡ã‚¹ã‚¯AIã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ãŒã€Œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãƒ¬ãƒ™ãƒ«ã€ã§æ›–æ˜§ã‹ã€
    ãã‚Œã¨ã‚‚ã€Œå›ç­”å¯èƒ½ãªæ–‡ç« ã€ã«ãªã£ã¦ã„ã‚‹ã‹ã‚’å³å¯†ã«åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: "{query}"
    # æŒ‡ç¤º
    - å…·ä½“çš„ã§æ„å›³ãŒæ˜ç¢ºãªã‚‰ is_ambiguous: false
    - å˜èªã®ã¿ï¼ˆä¾‹:ã€Œå¥¨å­¦é‡‘ã€ã€Œå±¥ä¿®ã€ï¼‰ã‚„ä¸»èªãƒ»ç›®çš„èªä¸è¶³ãªã‚‰ is_ambiguous: true
    - candidatesã«ã¯ã€ãã®å˜èªã‹ã‚‰æƒ³å®šã•ã‚Œã‚‹å…·ä½“çš„ãªè³ªå•æ–‡ã‚’3ã€œ4ã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚
    """
    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
        response = await model.generate_content_async(
            prompt,
            generation_config=GenerationConfig(
                response_mime_type="application/json",
                response_schema=AmbiguityAnalysis
            ),
            safety_settings=SAFETY_SETTINGS
        )
        return json.loads(clean_json_string(response.text))
    except Exception as e:
        log_context(session_id, f"æ›–æ˜§æ€§åˆ¤å®šã‚¹ã‚­ãƒƒãƒ—: {e}", "warning")
        return {"is_ambiguous": False}

async def generate_search_optimized_query(user_query: str, session_id: str) -> str:
    """HyDE + Query Expansion: å°‚é–€ç”¨èªè£œå®Œã¨æ¤œç´¢ã‚¯ã‚¨ãƒªæœ€é©åŒ–"""
    prompt = f"""
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«åŸºã¥ã„ã¦ã€å¤§å­¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆã‚·ãƒ©ãƒã‚¹ã€å­¦å‰‡ã€FAQï¼‰ã‹ã‚‰æœ€é©ãªæƒ…å ±ã‚’å¼•ãå‡ºã™ãŸã‚ã®ã€Œæ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã¨ã€Œä»®èª¬çš„ãªå›ç­”ã®ä¸€éƒ¨ã€ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: "{user_query}"
    # å½¹å‰²
    å°‚é–€ç”¨èªï¼ˆä¾‹: "å–ã‚Šæ¶ˆã—" -> "å±¥ä¿®ä¸­æ­¢", "ä¼‘ã¿" -> "ä¼‘æ¥­æœŸé–“"ï¼‰ã¸ã®è¨€ã„æ›ãˆã‚’å«ã‚ã¦ãã ã•ã„ã€‚
    å‡ºåŠ›ã¯æ¤œç´¢ã«ä½¿ç”¨ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã«ã—ã¦ãã ã•ã„ã€‚
    """
    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
        response = await model.generate_content_async(prompt, safety_settings=SAFETY_SETTINGS)
        optimized = response.text.strip()
        log_context(session_id, f"ã‚¯ã‚¨ãƒªæ‹¡å¼µ: {user_query} -> {optimized}")
        return optimized
    except Exception:
        return user_query

async def rerank_documents_with_gemini(query: str, documents: List[Dict[str, Any]], session_id: str, top_k: int = 5) -> List[Dict[str, Any]]:
    if not documents:
        return []

    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æ‹¡å¼µï¼ˆã‚ˆã‚Šæ­£ç¢ºãªãƒªãƒ©ãƒ³ã‚¯ã®ãŸã‚ï¼‰
    candidates_text = ""
    for i, doc in enumerate(documents):
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚å«ã‚ã¦åˆ¤æ–­ææ–™ã«ã™ã‚‹
        meta = doc.get('metadata', {})
        source = meta.get('source', 'unknown')
        title = meta.get('title', 'No Title')
        # é‡è¦ãªæƒ…å ±ãŒå¾ŒåŠã«ã‚ã‚‹å ´åˆã‚‚è€ƒæ…®ã—ã€2000æ–‡å­—ã¾ã§å–å¾—
        content_snippet = doc.get('content', '')[:2000].replace('\n', ' ')
        candidates_text += f"ID:{i} [Source:{source}] [Title:{title}]\nContent: {content_snippet}\n\n"

    prompt = f"""
    ã‚ãªãŸã¯å„ªç§€ãªæ¤œç´¢ãƒªãƒ©ãƒ³ã‚«ãƒ¼ï¼ˆRe-rankerï¼‰ã§ã™ã€‚
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€æä¾›ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã€Œå›ç­”æ ¹æ‹ ã¨ã—ã¦é©åˆ‡ã‹ã€ã‚’0.0ã€œ10.0ç‚¹ã§æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
    {query}
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå€™è£œ
    {candidates_text}
    # æ¡ç‚¹åŸºæº–ï¼ˆå“è³ªé‡è¦–ï¼‰
    - 10ç‚¹: è³ªå•ã«å¯¾ã™ã‚‹ç›´æ¥çš„ãªå›ç­”ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€æœ€æ–°ã‹ã¤æ­£ç¢ºã§ã‚ã‚‹ã€‚
    - 5-9ç‚¹: é–¢é€£æƒ…å ±ã¯å«ã¾ã‚Œã‚‹ãŒã€éƒ¨åˆ†çš„ã«æ¨è«–ãŒå¿…è¦ã§ã‚ã‚‹ã€‚
    - 0-4ç‚¹: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ä¸€è‡´ã™ã‚‹ãŒã€æ–‡è„ˆãŒç•°ãªã‚‹ã€ã¾ãŸã¯å¤ã„æƒ…å ±ã§ã‚ã‚‹ã€‚
    # å‡ºåŠ›
    JSONå½¢å¼ã§ã€å„IDã«å¯¾ã™ã‚‹scoreã¨reasonï¼ˆæ¡ç‚¹ç†ç”±ï¼‰ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    """

    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
        response = await model.generate_content_async(
            prompt,
            generation_config=GenerationConfig(
                response_mime_type="application/json",
                response_schema=RerankResult
            ),
            safety_settings=SAFETY_SETTINGS
        )
        result_json = json.loads(clean_json_string(response.text))
        ranked_items = result_json.get("ranked_items", [])
        reranked_docs = []
        for item in ranked_items:
            idx = int(item["id"])
            score = float(item["score"])
            if 0 <= idx < len(documents):
                doc = documents[idx]
                doc['rerank_score'] = score
                reranked_docs.append(doc)
        # ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆ
        reranked_docs.sort(key=lambda x: x.get('rerank_score', 0), reverse=True)
        return reranked_docs[:top_k]

    except Exception as e:
        log_context(session_id, f"ãƒªãƒ©ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", "error")
        return documents[:top_k]

# -----------------------------------------------
# ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯ (Status Updateå¯¾å¿œç‰ˆ)
# -----------------------------------------------

async def enhanced_chat_logic(request: Request, chat_req: ChatQuery):
    user_input = chat_req.query.strip()
    session_id = get_or_create_session_id(request)
    feedback_id = str(uuid.uuid4())
    yield f"data: {json.dumps({'feedback_id': feedback_id})}\n\n"

    try:
        # --- PHASE 1: æ„å›³ç†è§£ ---
        yield f"data: {json.dumps({'status_message': 'ğŸ¤” è³ªå•ã®æ„å›³ã‚’åˆ†æã—ã¦ã„ã¾ã™...'})}\n\n"

        # Step 1: æ›–æ˜§æ€§ãƒã‚§ãƒƒã‚¯
        ambiguity_res = await check_ambiguity_and_suggest_options(user_input, session_id)
        if ambiguity_res.get("is_ambiguous"):
            suggestion = ambiguity_res.get("response_text", "ã‚‚ã†å°‘ã—å…·ä½“çš„ã«æ•™ãˆã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ")
            candidates = ambiguity_res.get("candidates", [])
            resp_content = suggestion
            if candidates:
                resp_content += "\n\n**ã‚‚ã—ã‹ã—ã¦:**\n" + "\n".join([f"- {c}" for c in candidates])
            yield f"data: {json.dumps({'content': resp_content})}\n\n"
            yield f"data: {json.dumps({'show_feedback': True, 'feedback_id': feedback_id})}\n\n"
            return

        # --- PHASE 2: ã‚¯ã‚¨ãƒªæ‹¡å¼µ ---
        yield f"data: {json.dumps({'status_message': 'ğŸ” æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æœ€é©åŒ–ã—ã¦ã„ã¾ã™...'})}\n\n"

        # Step 2: ä¸¦åˆ—å‡¦ç† (Embedding & Query Expansion)
        log_context(session_id, "æ¤œç´¢ãƒ•ã‚§ãƒ¼ã‚ºé–‹å§‹")
        # ã‚¿ã‚¹ã‚¯å®šç¾©
        task_embed_raw = asyncio.create_task(
            genai.embed_content_async(
                model=chat_req.embedding_model,
                content=user_input,
                task_type="retrieval_query"
            )
        )
        task_transform = asyncio.create_task(generate_search_optimized_query(user_input, session_id))

        # --- FAQ ãƒã‚§ãƒƒã‚¯ ---
        try:
            raw_emb_res = await task_embed_raw
            raw_embedding = raw_emb_res["embedding"]
            qa_results = core_database.db_client.search_fallback_qa(
                embedding=raw_embedding,
                match_count=3
            )
            if qa_results:
                top_qa = qa_results[0]
                # å“è³ªé‡è¦–ã®ãŸã‚ã€FAQã®ä¸€è‡´é–¾å€¤ã¯é«˜ã‚ã«è¨­å®š
                if top_qa.get('similarity', 0) >= QA_SIMILARITY_THRESHOLD:
                    task_transform.cancel()
                    resp_text = format_urls_as_links(f"ã‚ˆãã‚ã‚‹ã”è³ªå•ã«å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n\n---\n{top_qa['content']}")
                    history_manager.add_to_history(session_id, "assistant", resp_text)
                    yield f"data: {json.dumps({'content': resp_text})}\n\n"
                    yield f"data: {json.dumps({'show_feedback': True, 'feedback_id': feedback_id})}\n\n"
                    return
        except Exception as e:
            log_context(session_id, f"FAQ Search Error: {e}", "warning")

        # --- Document Search ---
        search_query_text = await task_transform
        # --- PHASE 3: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ ---
        yield f"data: {json.dumps({'status_message': 'ğŸ“š å­¦å†…ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™...'})}\n\n"
        try:
            opt_emb_res = await genai.embed_content_async(
                model=chat_req.embedding_model,
                content=search_query_text,
                task_type="retrieval_query"
            )
            query_embedding = opt_emb_res["embedding"]
        except Exception:
            yield f"data: {json.dumps({'content': 'æ¤œç´¢å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ™‚é–“ã‚’ãŠã„ã¦ãŠè©¦ã—ãã ã•ã„ã€‚'})}\n\n"
            return

        # æ¤œç´¢å®Ÿè¡Œ
        relevant_docs = []
        try:
            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆåºƒã‚ã«å–å¾—ï¼‰
            raw_docs = core_database.db_client.search_documents_hybrid(
                collection_name=chat_req.collection,
                query_text=search_query_text,
                query_embedding=query_embedding,
                match_count=30  # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰ã¯å¤šã‚ã«ç¢ºä¿
            )
            # å¤šæ§˜æ€§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆéåŒæœŸå®Ÿè¡Œã§ãƒ–ãƒ­ãƒƒã‚¯å›é¿ï¼‰
            unique_docs = await filter_results_by_diversity_async(raw_docs, threshold=0.7)
            # --- PHASE 4: ãƒªãƒ©ãƒ³ã‚¯ (AIã«ã‚ˆã‚‹ç²¾æŸ») ---
            yield f"data: {json.dumps({'status_message': 'ğŸ§ æ–‡çŒ®ã®é‡è¦åº¦ã‚’AIãŒæ¡ç‚¹ä¸­...'})}\n\n"

            # ãƒªãƒ©ãƒ³ã‚¯å®Ÿè¡Œ (Gemini)
            # ä¸Šä½å€™è£œã®ã¿ã‚’LLMã«æ¸¡ã™
            rerank_candidates = unique_docs[:12]
            if rerank_candidates:
                reranked_docs = await rerank_documents_with_gemini(
                    query=user_input,
                    documents=rerank_candidates,
                    session_id=session_id,
                    top_k=chat_req.top_k
                )
                # ã‚¹ã‚³ã‚¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                for d in reranked_docs:
                    if d.get('rerank_score', 0) >= RERANK_SCORE_THRESHOLD:
                        relevant_docs.append(d)
        except Exception as e:
            log_context(session_id, f"Retrieval Error: {e}", "error")

        # --- å›ç­”ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º ---
        if not relevant_docs:
            yield f"data: {json.dumps({'content': AI_NOT_FOUND_MESSAGE_USER})}\n\n"
        else:
            # --- PHASE 5: åŸ·ç­†é–‹å§‹ ---
            yield f"data: {json.dumps({'status_message': 'âœï¸ å›ç­”ã‚’åŸ·ç­†ã—ã¦ã„ã¾ã™...'})}\n\n"

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ï¼ˆå¼•ç”¨ç•ªå·ã®ä»˜ä¸ï¼‰
            context_text = ""
            sources_map = {} # {1: "å±¥ä¿®ã‚¬ã‚¤ãƒ‰ p.10", 2: ...}
            for idx, d in enumerate(relevant_docs, 1):
                content = d.get('content', '')
                source = d.get('metadata', {}).get('source', 'ä¸æ˜')
                # ãƒãƒƒãƒ”ãƒ³ã‚°ä¿å­˜
                sources_map[idx] = source
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                if len(context_text) + len(content) < MAX_CONTEXT_CHAR_LENGTH:
                    context_text += f"<document index='{idx}' source='{source}'>\n{content}\n</document>\n\n"

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è³ªã‚’é«˜ã‚ã‚‹ï¼ˆChain of Thoughtèª˜å°ï¼‰
            system_prompt = f"""
            ã‚ãªãŸã¯æœ­å¹Œå­¦é™¢å¤§å­¦ã®å…¬å¼å­¦ç”Ÿã‚µãƒãƒ¼ãƒˆAIã§ã™ã€‚
            æä¾›ã•ã‚ŒãŸ<context>å†…ã®æƒ…å ±**ã®ã¿**ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã‹ã¤è«–ç†çš„ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

            # å›ç­”ä½œæˆãƒ«ãƒ¼ãƒ«
            1. **æ ¹æ‹ ã®æ˜ç¤º**: å›ç­”å†…ã®äº‹å®Ÿã«ã¯ã€å¿…ãšæƒ…å ±ã®å‡ºæ‰€ã¨ãªã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç•ªå·ã‚’ `[1]` `[2]` ã®å½¢å¼ã§æ–‡ä¸­ã«ä»˜è¨˜ã—ã¦ãã ã•ã„ã€‚
            - è‰¯ã„ä¾‹: ã€Œå±¥ä¿®ç™»éŒ²ã®ä¿®æ­£æœŸé–“ã¯4æœˆ15æ—¥ã¾ã§ã§ã™[1]ã€‚ã€
            - æ‚ªã„ä¾‹: å‡ºæ‰€ã‚’æ›¸ã‹ãªã„ã€ã¾ãŸã¯æ–‡æœ«ã«ã¾ã¨ã‚ã‚‹ã ã‘ã€‚
            2. **æ­£ç¢ºæ€§å„ªå…ˆ**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ç­”ãˆãŒãªã„å ´åˆã¯ã€æ­£ç›´ã«ã€Œæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚æ¨æ¸¬ã§ç­”ãˆã‚‹ã“ã¨ã¯ç¦æ­¢ã§ã™ã€‚
            3. **ãƒˆãƒ¼ãƒ³**: ä¸å¯§ã§è¦ªã—ã¿ã‚„ã™ã„ã€ã—ã‹ã—äº‹å‹™çš„ã«æ­£ç¢ºãªã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã€‚
            4. **æ§‹é€ **: èª­ã¿ã‚„ã™ã„ã‚ˆã†ã«ç®‡æ¡æ›¸ãã‚„å¤ªå­—ã‚’æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚
            <context>
            {context_text}
            </context>
            """
            # å‚ç…§å…ƒãƒªã‚¹ãƒˆã®ä½œæˆï¼ˆå›ç­”å¾Œã«ä»˜ä¸ã™ã‚‹ãŸã‚ï¼‰
            references_text = "\n\n---\n**å‚è€ƒè³‡æ–™:**\n" + "\n".join([f"- [{k}] {v}" for k, v in sources_map.items()])

            user_prompt = f"è³ªå•: {user_input}"
            try:
                model = genai.GenerativeModel(chat_req.model)
                stream = await model.generate_content_async(
                    [system_prompt, user_prompt],
                    stream=True,
                    safety_settings=SAFETY_SETTINGS
                )
                full_response = ""
                async for chunk in stream:
                    if chunk.text:
                        text_chunk = chunk.text
                        full_response += text_chunk
                        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã¯ãã®ã¾ã¾æµã™
                        yield f"data: {json.dumps({'content': text_chunk})}\n\n"
                # å›ç­”å®Œäº†å¾Œã€æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆä»¥å¤–ã¯å‚ç…§å…ƒã‚’è¿½è¨˜
                if AI_NOT_FOUND_TOKEN not in full_response and "æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" not in full_response:
                     yield f"data: {json.dumps({'content': references_text})}\n\n"
                     # å±¥æ­´ã«ã¯å‚ç…§å…ƒä»˜ãã§ä¿å­˜
                     history_manager.add_to_history(session_id, "assistant", full_response + references_text)
                else:
                    history_manager.add_to_history(session_id, "assistant", full_response)

            except Exception as e:
                log_context(session_id, f"Generation Error: {e}", "error")
                yield f"data: {json.dumps({'content': 'å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚'})}\n\n"

    except Exception as e:
        log_context(session_id, f"Critical Error: {e}", "error")
        yield f"data: {json.dumps({'content': 'ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ç®¡ç†è€…ã«é€£çµ¡ã—ã¦ãã ã•ã„ã€‚'})}\n\n"
    finally:
        yield f"data: {json.dumps({'show_feedback': True, 'feedback_id': feedback_id})}\n\n"