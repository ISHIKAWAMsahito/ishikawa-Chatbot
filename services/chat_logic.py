import logging
import uuid
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, AsyncGenerator
from fastapi import Request

# LangSmith ãƒˆãƒ¬ãƒ¼ã‚¹ç”¨
from langsmith import traceable
from langsmith.run_helpers import get_current_run_tree

# ä¾å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from core import database as core_database
from core.constants import PARAMS, AI_MESSAGES
from models.schemas import ChatQuery
from services.llm import LLMService
from services.search import SearchService
from services.storage import StorageService
from services.utils import (
    get_or_create_session_id, 
    send_sse, 
    log_context, 
    ChatHistoryManager, 
    format_references 
)
# â˜…ä¿®æ­£: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from services import prompts

# DIï¼ˆä¾å­˜æ€§ã®æ³¨å…¥ï¼‰ã®æº–å‚™
llm_service = LLMService()
search_service = SearchService(llm_service)
storage_service = StorageService()
history_manager = ChatHistoryManager(max_length=PARAMS["MAX_HISTORY_LENGTH"])

@traceable(name="Chat_Pipeline_Parent", run_type="chain")
async def enhanced_chat_logic(request: Request, chat_req: ChatQuery) -> AsyncGenerator[str, None]:
    """
    RAGãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯
    """
    session_id = get_or_create_session_id(request)
    user_input = chat_req.question or chat_req.query
    
    collection_name = getattr(chat_req, "collection", "student-knowledge-base")
    top_k = getattr(chat_req, "top_k", 5)
    embedding_model = getattr(chat_req, "embedding_model", "models/gemini-embedding-001")

    # LangSmithç”¨ã®RunTreeå–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç”¨ï¼‰
    run_tree = get_current_run_tree()

    log_context(session_id, f"Start processing query: {user_input}")

    # æ—¥æ™‚å–å¾—ï¼ˆJSTï¼‰
    JST = timezone(timedelta(hours=9), 'JST')
    now = datetime.now(JST)
    current_date_str = now.strftime("%Yå¹´%mæœˆ%dæ—¥")

    search_results = []
    
    try:
        # 1. å±¥æ­´ã®è¿½åŠ 
        history_manager.add(session_id, "user", user_input)
        
        yield send_sse({'status_message': 'ğŸ” è³ªå•ã‚’åˆ†æã—ã¦ã„ã¾ã™...'})

        # 2. æ¤œç´¢ (Search Service)
        
        # ã¾ãšã‚¯ã‚¨ãƒªæ‹¡å¼µ
        expanded_query = await search_service.expand_query(user_input)
        
        # æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ (Search -> Rerank -> LitM -> Filter)
        search_result_obj = await search_service.search(
            query=expanded_query, 
            session_id=session_id,
            collection_name=collection_name,
            top_k=top_k,
            embedding_model=embedding_model
        )
        search_results = search_result_obj.get("documents", [])
        
        # ãƒ’ãƒƒãƒˆã—ãªã‹ã£ãŸå ´åˆã€å…ƒã®ã‚¯ã‚¨ãƒªã§å†è©¦è¡Œ (å®‰å…¨ç­–)
        if not search_results and expanded_query != user_input:
             search_result_obj = await search_service.search(
                query=user_input,
                session_id=session_id,
                collection_name=collection_name,
                top_k=top_k,
                embedding_model=embedding_model
             )
             search_results = search_result_obj.get("documents", [])

        if not search_results:
             yield send_sse({'content': AI_MESSAGES.get("NOT_FOUND", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")})
             # å±¥æ­´ã«ä¿å­˜ã—ã¦çµ‚äº†
             history_manager.add(session_id, "assistant", "é–¢é€£æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
             yield send_sse({'done': True, 'feedback_id': str(uuid.uuid4())})
             return

        yield send_sse({'status_message': 'âœï¸ å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...'})

        # 3. å›ç­”ç”Ÿæˆ (LLM Service)
        chat_history = history_manager.get_history(session_id)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        context_parts = []
        for idx, doc in enumerate(search_results, 1):
            doc_content = doc.get('content', '')
            context_parts.append(f"<doc id='{idx}'>{doc_content}</doc>")
        context_str = "\n".join(context_parts)

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™
        try:
            full_system_prompt = prompts.SYSTEM_GENERATION.format(
                context_text=context_str,
                current_date=current_date_str
            )
        except Exception:
             # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼ç­‰ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
             full_system_prompt = f"ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n{context_str}"

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å›ç­”ã®é–‹å§‹
        ai_response_full = ""
        
        async for chunk in llm_service.generate_response_stream(
            query=user_input,
            context_docs=search_results, 
            history=chat_history,
            system_prompt=full_system_prompt
        ):
            text_chunk = chunk if isinstance(chunk, str) else chunk.get("content", "")
            ai_response_full += text_chunk
            yield send_sse({'content': text_chunk})

        # 4. å‚ç…§å…ƒãƒªã‚¹ãƒˆã®ç”Ÿæˆã¨é€ä¿¡
        references_text = format_references(search_results)
        
        if references_text:
            yield send_sse({'content': references_text})
            ai_response_full += references_text

        # 5. å±¥æ­´ã«AIã®å›ç­”ã‚’ä¿å­˜
        history_manager.add(session_id, "assistant", ai_response_full)

        # 6. å®Œäº†ã‚·ã‚°ãƒŠãƒ«
        feedback_id = str(uuid.uuid4())
        yield send_sse({'done': True, 'feedback_id': feedback_id})

    except Exception as e:
        log_context(session_id, f"Critical Pipeline Error: {e}", "error")
        if run_tree:
            run_tree.end(error=str(e))
            
        error_str = str(e)
        msg = AI_MESSAGES.get("SYSTEM_ERROR", "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
        yield send_sse({'content': f"\n\n{msg} (Error: {error_str})"})
        
    finally:
        log_context(session_id, "Response generation finished.")

# â˜…ã“ã®é–¢æ•°ãŒä¸è¶³ã—ã¦ã„ãŸãŸã‚ã‚¨ãƒ©ãƒ¼ã«ãªã£ã¦ã„ã¾ã—ãŸã€‚å¿…ãšãƒ•ã‚¡ã‚¤ãƒ«ã®æœ«å°¾ã«å«ã¾ã‚Œã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
@traceable(name="Feedback_Analysis_Job", run_type="chain")
async def analyze_feedback_trends(logs: List[Dict[str, Any]]) -> AsyncGenerator[str, None]:
    """
    ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æç”¨ãƒ­ã‚¸ãƒƒã‚¯
    """
    if not logs:
        yield send_sse({'content': 'åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚'})
        return
    
    summary = "\n".join([f"- è©•ä¾¡:{l.get('rating','-')} | {l.get('comment','-')[:100]}" for l in logs[:50]])
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰å–å¾—
    prompt = prompts.FEEDBACK_ANALYSIS.format(summary=summary)

    try:
        stream = await llm_service.generate_stream(prompt)
        async for chunk in stream:
            text = chunk.text if hasattr(chunk, 'text') else str(chunk)
            if text:
                yield send_sse({'content': text})
    except Exception as e:
        yield send_sse({'content': f'åˆ†æã‚¨ãƒ©ãƒ¼: {e}'})