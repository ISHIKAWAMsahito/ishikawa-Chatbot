import logging
import uuid
import json
import asyncio
import re
import os
from typing import List, Dict, Any, AsyncGenerator, Optional
from concurrent.futures import ThreadPoolExecutor
from difflib import SequenceMatcher
import typing_extensions as typing

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold, GenerationConfig
from fastapi import Request
from dotenv import load_dotenv

# â˜…è¿½åŠ : Supabaseé–¢é€£
from supabase import create_client, Client

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from core.config import GEMINI_API_KEY
# â˜…è¿½åŠ : configã‹ã‚‰Supabaseã®ã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€ã¨ä»®å®š
# (ã‚‚ã—core/config.pyã«ãªã„å ´åˆã¯ã€os.getenv("SUPABASE_URL")ãªã©ã§ç›´æ¥å–å¾—ã—ã¦ãã ã•ã„)
from core.config import SUPABASE_URL, SUPABASE_SERVICE_KEY 
from core import database as core_database
from models.schemas import ChatQuery
from services.utils import format_urls_as_links

# -----------------------------------------------------------------------------
# 1. è¨­å®š & å®šæ•°å®šç¾©
# -----------------------------------------------------------------------------
load_dotenv()
genai.configure(api_key=GEMINI_API_KEY)

# â˜…è¿½åŠ : Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
# ãƒã‚±ãƒƒãƒˆåï¼ˆéå…¬é–‹ãƒ†ãƒ¼ãƒ–ãƒ«ã«ç›¸å½“ï¼‰
STORAGE_BUCKET_NAME = "images" 
supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

# ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
USE_MODEL = "gemini-2.5-flash"

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
PARAMS = {
    "QA_SIMILARITY_THRESHOLD": 0.90,
    "RERANK_SCORE_THRESHOLD": 6.0,
    "MAX_HISTORY_LENGTH": 20,
}

# ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®š
SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
}

AI_MESSAGES = {
    "NOT_FOUND": (
        "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã”è³ªå•ã«é–¢é€£ã™ã‚‹ç¢ºå®Ÿãªæƒ…å ±ãŒè³‡æ–™å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        "å¤§å­¦çª“å£ã¸ç›´æ¥ãŠå•ã„åˆã‚ã›ã„ãŸã ãã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
    ),
    "RATE_LIMIT": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ç¾åœ¨ã‚¢ã‚¯ã‚»ã‚¹ãŒé›†ä¸­ã—ã¦ã„ã¾ã™ã€‚1åˆ†ã»ã©å¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
    "SYSTEM_ERROR": "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
    "BLOCKED": "ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãŒã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«æŠµè§¦ã—ãŸãŸã‚ã€è¡¨ç¤ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚è¨€ã„å›ã—ã‚’å¤‰ãˆã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
}

# ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«
executor = ThreadPoolExecutor(max_workers=4)

# -----------------------------------------------------------------------------
# 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ & ã‚¹ã‚­ãƒ¼ãƒå®šç¾©
# -----------------------------------------------------------------------------
class RankedItem(typing.TypedDict):
    id: int
    score: float
    reason: str

class RerankResponse(typing.TypedDict):
    ranked_items: list[RankedItem]

PROMPT_RERANK = """
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå›ç­”æ ¹æ‹ ã¨ã—ã¦é©åˆ‡ã‹0-10ç‚¹ã§æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚
è³ªå•: {query}
å€™è£œ:
{candidates_text}
"""

PROMPT_SYSTEM_GENERATION = """
ã‚ãªãŸã¯æœ­å¹Œå­¦é™¢å¤§å­¦ã®ã‚µãƒãƒ¼ãƒˆAIã§ã™ã€‚
ä»¥ä¸‹ã®<context>å†…ã®æƒ…å ±**ã®ã¿**ã‚’ä½¿ç”¨ã—ã¦ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

# å›ç­”ã®ãƒ«ãƒ¼ãƒ«
1. **æ ¹æ‹ ã®ç´ä»˜ã‘**:
   æ–‡ç« ä¸­ã®é‡è¦ãªäº‹å®Ÿã«ã¯ã€æ–‡æœ«ã« `[1]` ã®ã‚ˆã†ã«**çŸ­ã„ç•ªå·ã®ã¿**ã‚’ä»˜è¨˜ã—ã¦ãã ã•ã„ã€‚
2. **å½¢å¼**:
   - å­¦ç”Ÿã«å¯„ã‚Šæ·»ã£ãŸã€ä¸å¯§ã§è¦ªã—ã¿ã‚„ã™ã„ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã€‚
   - èª­ã¿ã‚„ã™ã„ã‚ˆã†ã«ç®‡æ¡æ›¸ãã‚„**å¤ªå­—**ã‚’æ´»ç”¨ã™ã‚‹ã€‚
   - æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã‚‹ã€‚
"""

# -----------------------------------------------------------------------------
# 3. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# -----------------------------------------------------------------------------
def get_or_create_session_id(request: Request) -> str:
    session_id = request.session.get('chat_session_id')
    if not session_id:
        session_id = str(uuid.uuid4())
        request.session['chat_session_id'] = session_id
    return session_id

def log_context(session_id: str, message: str, level: str = "info"):
    msg = f"[Session: {session_id}] {message}"
    getattr(logging, level, logging.info)(msg)

def send_sse(data: Dict[str, Any]) -> str:
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

async def api_request_with_retry(func, *args, **kwargs):
    max_retries = 3
    default_delay = 5
    for attempt in range(max_retries):
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            error_str = str(e)
            if "429" in error_str or "Quota" in error_str:
                if attempt == max_retries - 1:
                    raise e
                wait_time = default_delay * (2 ** attempt)
                match = re.search(r"retry in (\d+\.?\d*)s", error_str)
                if match:
                    wait_time = float(match.group(1)) + 1.0
                await asyncio.sleep(wait_time)
            else:
                raise e

class ChatHistoryManager:
    def __init__(self):
        self._histories: Dict[str, List[Dict[str, str]]] = {}

    def add(self, session_id: str, role: str, content: str):
        if session_id not in self._histories:
            self._histories[session_id] = []
        self._histories[session_id].append({"role": role, "content": content})
        if len(self._histories[session_id]) > PARAMS["MAX_HISTORY_LENGTH"]:
            self._histories[session_id] = self._histories[session_id][-PARAMS["MAX_HISTORY_LENGTH"]:]

history_manager = ChatHistoryManager()

# -----------------------------------------------------------------------------
# 4. ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯: æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ & â˜…å‚ç…§ãƒªãƒ³ã‚¯ç”Ÿæˆ
# -----------------------------------------------------------------------------

# â˜…è¿½åŠ : Supabaseç½²åä»˜ãURLç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
def _generate_signed_url_sync(filename: str) -> Optional[str]:
    """
    Supabase Storageã‹ã‚‰ç½²åä»˜ãURLã‚’å–å¾—ã™ã‚‹ï¼ˆåŒæœŸé–¢æ•°ï¼‰ã€‚
    txtãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ã€åŒåã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«(png, jpg)ã®å­˜åœ¨ã‚‚ãƒã‚§ãƒƒã‚¯ã—ã¦URLåŒ–ã‚’è©¦ã¿ã‚‹ã€‚
    """
    try:
        # 1. ãã®ã¾ã¾ã®ãƒ•ã‚¡ã‚¤ãƒ«åã§ãƒˆãƒ©ã‚¤
        # create_signed_url returns Dict with 'signedURL' key usually
        res = supabase.storage.from_(STORAGE_BUCKET_NAME).create_signed_url(filename, 3600)
        if res and 'signedURL' in res:
            return res['signedURL']
    except Exception:
        pass

    # 2. æ‹¡å¼µå­ãŒ .txt ã®å ´åˆã€ç”»åƒãƒ•ã‚¡ã‚¤ãƒ« (.png, .jpg) ãŒã‚ã‚‹ã‹è©¦è¡Œã™ã‚‹
    # ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç”»åƒã‹ã‚‰OCRã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚‹å ´åˆã¸ã®å¯¾å¿œï¼‰
    if filename.endswith(".txt"):
        base_name = os.path.splitext(filename)[0]
        for ext in [".png", ".jpg", ".jpeg", ".pdf"]:
            try:
                image_filename = f"{base_name}{ext}"
                res = supabase.storage.from_(STORAGE_BUCKET_NAME).create_signed_url(image_filename, 3600)
                if res and 'signedURL' in res:
                    return res['signedURL']
            except Exception:
                continue
    
    return None

async def _build_references_async(response_text: str, sources_map: Dict[int, str]) -> str:
    """
    å›ç­”ç”Ÿæˆå¾Œã«å‚ç…§å…ƒãƒªãƒ³ã‚¯ã‚’ä½œæˆã™ã‚‹ï¼ˆéåŒæœŸä¸¦åˆ—å‡¦ç†ç‰ˆï¼‰ã€‚
    Supabaseã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’ä¸¦åˆ—åŒ–ã—ã¦é«˜é€ŸåŒ–ã‚’å›³ã‚‹ã€‚
    """
    unique_refs = []
    seen_sources = set()
    
    # å‡¦ç†å¯¾è±¡ã®ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
    target_items = []
    for idx, src in sources_map.items():
        if src in seen_sources: continue
        # ãƒ†ã‚­ã‚¹ãƒˆå†…ã§å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ã€ã¾ãŸã¯ä¸Šä½3ä»¶ãªã‚‰è¡¨ç¤ºå¯¾è±¡
        if f"[{idx}]" in response_text or idx <= 3:
            target_items.append((idx, src))
            seen_sources.add(src)
    
    if not target_items:
        return ""

    # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã§ä¸¦åˆ—ã«URLç™ºè¡Œ
    loop = asyncio.get_running_loop()
    tasks = []
    for _, src in target_items:
        tasks.append(loop.run_in_executor(executor, _generate_signed_url_sync, src))
    
    # å…¨ã¦ã®URLå–å¾—ã‚’å¾…æ©Ÿ
    signed_urls = await asyncio.gather(*tasks)
    
    # çµæœã®æ•´å½¢
    for (idx, src), url in zip(target_items, signed_urls):
        if url:
            # ç½²åä»˜ãURLãŒå–å¾—ã§ããŸå ´åˆ: ãƒªãƒ³ã‚¯åŒ–
            # ãƒ•ã‚¡ã‚¤ãƒ«åãŒè¦‹ã‚„ã™ã„ã‚ˆã†ã« basename ã®ã¿ã‚’è¡¨ç¤ºã—ã¦ã‚‚è‰¯ã„ãŒã€ã“ã“ã§ã¯è­˜åˆ¥ã®ãŸã‚srcå…¨ä½“ã‚’è¡¨ç¤º
            display_name = os.path.basename(src)
            unique_refs.append(f"* [{idx}] [{display_name}]({url}) â³ãƒªãƒ³ã‚¯æœ‰åŠ¹æœŸé™:1æ™‚é–“")
        else:
            # å–å¾—å¤±æ•—ã—ãŸå ´åˆ: é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
            unique_refs.append(f"* [{idx}] {src}")

    if unique_refs:
        return "\n\n## å‚ç…§å…ƒ (ã‚¯ãƒªãƒƒã‚¯ã§è³‡æ–™ã‚’è¡¨ç¤º)\n" + "\n".join(unique_refs)
    return ""

class SearchPipeline:
    @staticmethod
    async def rerank(query: str, documents: List[Dict], top_k: int = 5) -> List[Dict]:
        if not documents:
            return []
        
        candidates_text = ""
        for i, doc in enumerate(documents):
            meta = doc.get('metadata', {})
            snippet = doc.get('content', '')[:300].replace('\n', ' ')
            candidates_text += f"ID:{i} [Source:{meta.get('source', '?')}]\n{snippet}\n\n"

        formatted_prompt = PROMPT_RERANK.format(query=query, candidates_text=candidates_text)

        try:
            model = genai.GenerativeModel(USE_MODEL)
            resp = await api_request_with_retry(
                model.generate_content_async,
                formatted_prompt,
                generation_config=GenerationConfig(
                    response_mime_type="application/json",
                    response_schema=RerankResponse
                ),
                safety_settings=SAFETY_SETTINGS
            )
            
            data = json.loads(resp.text)
            
            reranked = []
            for item in data.get("ranked_items", []):
                idx = item.get("id")
                score = item.get("score")
                
                if idx is not None and 0 <= idx < len(documents):
                    if score >= PARAMS["RERANK_SCORE_THRESHOLD"]:
                        doc = documents[idx]
                        doc['rerank_score'] = score
                        reranked.append(doc)
            
            reranked.sort(key=lambda x: x['rerank_score'], reverse=True)
            return reranked[:top_k]

        except Exception as e:
            logging.error(f"Rerank Error: {e}")
            return documents[:top_k]

    @staticmethod
    async def filter_diversity(documents: List[Dict], threshold: float = 0.7) -> List[Dict]:
        loop = asyncio.get_running_loop()
        unique_docs = []
        
        def _calc_sim(a, b):
            return SequenceMatcher(None, a, b).ratio()

        for doc in documents:
            content = doc.get('content', '')
            is_duplicate = False
            for selected in unique_docs:
                sim = await loop.run_in_executor(executor, _calc_sim, content, selected.get('content', ''))
                if sim > threshold:
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_docs.append(doc)
        return unique_docs

# -----------------------------------------------------------------------------
# 5. ãƒ¡ã‚¤ãƒ³: ãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯
# -----------------------------------------------------------------------------
async def enhanced_chat_logic(request: Request, chat_req: ChatQuery):
    session_id = get_or_create_session_id(request)
    feedback_id = str(uuid.uuid4())
    user_input = chat_req.query.strip()
    
    yield send_sse({'feedback_id': feedback_id, 'status_message': 'ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™...'})

    try:
        embedding_task = asyncio.create_task(
            genai.embed_content_async(
                model=chat_req.embedding_model,
                content=user_input,
                task_type="retrieval_query"
            )
        )

        try:
            raw_emb_result = await embedding_task
            query_embedding = raw_emb_result["embedding"]
        except Exception as e:
            log_context(session_id, f"Embedding Failed: {e}", "error")
            yield send_sse({'content': AI_MESSAGES["SYSTEM_ERROR"]})
            return

        # FAQãƒã‚§ãƒƒã‚¯
        if qa_hits := core_database.db_client.search_fallback_qa(query_embedding, match_count=1):
            top_qa = qa_hits[0]
            if top_qa.get('similarity', 0) >= PARAMS["QA_SIMILARITY_THRESHOLD"]:
                resp = format_urls_as_links(f"ã‚ˆãã‚ã‚‹ã”è³ªå•ã«å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n\n---\n{top_qa['content']}")
                history_manager.add(session_id, "assistant", resp)
                yield send_sse({'content': resp, 'show_feedback': True, 'feedback_id': feedback_id})
                return

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢
        raw_docs = core_database.db_client.search_documents_hybrid(
            collection_name=chat_req.collection,
            query_text=user_input, 
            query_embedding=query_embedding,
            match_count=30
        )

        if not raw_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        yield send_sse({'status_message': 'ğŸ§ AIãŒæ–‡çŒ®ã‚’èª­ã‚“ã§é¸å®šä¸­...'})
        
        unique_docs = await SearchPipeline.filter_diversity(raw_docs)
        relevant_docs = await SearchPipeline.rerank(user_input, unique_docs[:15], top_k=chat_req.top_k)

        if not relevant_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        yield send_sse({'status_message': 'âœï¸ å›ç­”ã‚’åŸ·ç­†ã—ã¦ã„ã¾ã™...'})
        
        context_parts = []
        sources_map = {} 
        
        for idx, doc in enumerate(relevant_docs, 1):
            src = doc.get('metadata', {}).get('source', 'ä¸æ˜')
            sources_map[idx] = src
            context_parts.append(f"<doc id='{idx}' src='{src}'>\n{doc.get('content','')}\n</doc>")
        
        context_str = "\n".join(context_parts)
        full_system_prompt = f"{PROMPT_SYSTEM_GENERATION}\n<context>\n{context_str}\n</context>"

        model = genai.GenerativeModel(USE_MODEL)
        stream = await api_request_with_retry(
            model.generate_content_async,
            [full_system_prompt, f"è³ªå•: {user_input}"],
            stream=True,
            safety_settings=SAFETY_SETTINGS
        )
        
        full_resp = ""
        async for chunk in stream:
            if chunk.text:
                full_resp += chunk.text
                yield send_sse({'content': chunk.text})
        
        if not full_resp:
             yield send_sse({'content': AI_MESSAGES["BLOCKED"]})
             history_manager.add(session_id, "assistant", "[[BLOCKED]]")
             return

        # â˜…ä¿®æ­£: éåŒæœŸã§å‚ç…§å…ƒãƒªãƒ³ã‚¯ï¼ˆSigned URLï¼‰ã‚’ç”Ÿæˆã—ã¦è¿½è¨˜
        if "æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" not in full_resp:
            yield send_sse({'status_message': 'ğŸ”— å‚ç…§ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆä¸­...'})
            refs_text = await _build_references_async(full_resp, sources_map)
            if refs_text:
                yield send_sse({'content': refs_text})
                full_resp += refs_text
        
        history_manager.add(session_id, "assistant", full_resp)

    except Exception as e:
        log_context(session_id, f"Critical Pipeline Error: {e}", "error")
        
        error_str = str(e)
        if "429" in error_str or "Quota" in error_str:
            msg = AI_MESSAGES["RATE_LIMIT"]
        elif "finish_reason" in error_str:
            msg = AI_MESSAGES["BLOCKED"]
        else:
            msg = AI_MESSAGES["SYSTEM_ERROR"]
            
        yield send_sse({'content': msg})
        
    finally:
        yield send_sse({'show_feedback': True, 'feedback_id': feedback_id})

# -----------------------------------------------------------------------------
# 6. åˆ†ææ©Ÿèƒ½
# -----------------------------------------------------------------------------
async def analyze_feedback_trends(logs: List[Dict[str, Any]]) -> AsyncGenerator[str, None]:
    if not logs:
        yield send_sse({'content': 'åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚'})
        return
    
    summary = "\n".join([f"- è©•ä¾¡:{l.get('rating','-')} | {l.get('comment','-')[:100]}" for l in logs[:50]])
    prompt = f"""
    ä»¥ä¸‹ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆåˆ©ç”¨ãƒ­ã‚°ã‚’åˆ†æã—ã€Markdownã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    
    # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿
    {summary}
    
    # å‡ºåŠ›é …ç›®
    1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¸»ãªé–¢å¿ƒäº‹ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
    2. ä½è©•ä¾¡ã®åŸå› ã¨æ”¹å–„ç­–
    3. æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³
    """
    
    try:
        model = genai.GenerativeModel(USE_MODEL)
        stream = await api_request_with_retry(model.generate_content_async, prompt, stream=True)
        async for chunk in stream:
            if chunk.text:
                yield send_sse({'content': chunk.text})
    except Exception as e:
        yield send_sse({'content': f'åˆ†æã‚¨ãƒ©ãƒ¼: {e}'})