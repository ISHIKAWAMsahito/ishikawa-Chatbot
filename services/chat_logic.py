import logging
import uuid
import json
import asyncio
import re
import os
from typing import List, Dict, Any, AsyncGenerator, Optional, Union
from concurrent.futures import ThreadPoolExecutor
from difflib import SequenceMatcher
import typing_extensions as typing

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold, GenerationConfig
from fastapi import Request
from dotenv import load_dotenv

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from core.config import GEMINI_API_KEY
from core import database as core_database
from models.schemas import ChatQuery
from services.utils import format_urls_as_links

# -----------------------------------------------------------------------------
# 1. è¨­å®š & å®šæ•°å®šç¾©
# -----------------------------------------------------------------------------
load_dotenv()
genai.configure(api_key=GEMINI_API_KEY)

# ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
USE_MODEL = "gemini-2.5-flash"

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š (ãƒãƒ©ãƒ³ã‚¹èª¿æ•´æ¸ˆã¿)
PARAMS = {
    "QA_SIMILARITY_THRESHOLD": 0.90,  # DBå†…FAQã®å³ç­”ãƒ©ã‚¤ãƒ³
    "RERANK_SCORE_THRESHOLD": 4.0,    # ãƒªãƒ©ãƒ³ã‚¯è¶³åˆ‡ã‚Šãƒ©ã‚¤ãƒ³ (0-10)
    "DIVERSITY_THRESHOLD": 0.7,       # é‡è¤‡æ’é™¤ã®é¡ä¼¼åº¦ãƒ©ã‚¤ãƒ³
    "MAX_HISTORY_LENGTH": 20,
}

# ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®š
SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
}

# ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å®šç¾©
AI_MESSAGES = {
    "NOT_FOUND": (
        "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã”è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆè³‡æ–™ï¼‰å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        "ä¸ç¢ºã‹ãªå›ç­”ã‚’é¿ã‘ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ãŠç­”ãˆã‚’æ§ãˆã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚"
        "\n\nå¤§å­¦çª“å£ã¸ç›´æ¥ãŠå•ã„åˆã‚ã›ã„ãŸã ãã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
    ),
    "RATE_LIMIT": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ç¾åœ¨ã‚¢ã‚¯ã‚»ã‚¹ãŒé›†ä¸­ã—ã¦ã„ã¾ã™ã€‚1åˆ†ã»ã©å¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
    "SYSTEM_ERROR": "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
    "BLOCKED": "ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãŒã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«æŠµè§¦ã—ãŸãŸã‚ã€è¡¨ç¤ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
}

# ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ï¼ˆCPUãƒã‚¦ãƒ³ãƒ‰ãªå‡¦ç†ç”¨ï¼‰
executor = ThreadPoolExecutor(max_workers=4)

# -----------------------------------------------------------------------------
# 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾© & ã‚¹ã‚­ãƒ¼ãƒ (Structured Outputsç”¨)
# -----------------------------------------------------------------------------

# ãƒªãƒ©ãƒ³ã‚¯å‡ºåŠ›ç”¨ã®å‹å®šç¾©
class RankedItem(typing.TypedDict):
    id: int
    score: float
    reason: str

class RerankResponse(typing.TypedDict):
    ranked_items: list[RankedItem]

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ (ãƒªãƒ©ãƒ³ã‚¯ç”¨)
PROMPT_RERANK = """
ã‚ãªãŸã¯å³æ ¼ãªæŸ»èª­è€…ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã€Œå›ç­”ã®æ ¹æ‹ ã€ã¨ã—ã¦ä½¿ç”¨ã§ãã‚‹ã‹ã‚’0-10ç‚¹ã§æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚

è©•ä¾¡åŸºæº–:
- 10ç‚¹: è³ªå•ã«å¯¾ã™ã‚‹ç›´æ¥çš„ãªç­”ãˆãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€‚
- 5-9ç‚¹: é–¢é€£æƒ…å ±ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€å›ç­”ã®æ§‹æˆã«å½¹ç«‹ã¤ã€‚
- 0-4ç‚¹: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ä¼¼ã¦ã„ã‚‹ãŒã€æ–‡è„ˆãŒç•°ãªã‚‹ã€ã¾ãŸã¯ç„¡é–¢ä¿‚ã€‚

è³ªå•: {query}
å€™è£œ:
{candidates_text}
"""

PROMPT_SYSTEM_GENERATION = """
ã‚ãªãŸã¯**æœ­å¹Œå­¦é™¢å¤§å­¦ã®å­¦ç”Ÿã‚µãƒãƒ¼ãƒˆAI**ã§ã™ã€‚
ä»¥ä¸‹ã® <context> ã‚¿ã‚°å†…ã®æƒ…å ±**ã®ã¿**ã‚’ä½¿ç”¨ã—ã¦ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

# å³å®ˆã™ã¹ããƒ«ãƒ¼ãƒ«ï¼ˆã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ï¼‰

1. **æƒ…å ±ã®é™å®šï¼ˆZero-Inferenceï¼‰**:
   - ã‚ãªãŸãŒå…ƒã€…æŒã£ã¦ã„ã‚‹çŸ¥è­˜ï¼ˆä¸€èˆ¬å¸¸è­˜ã‚„ä»–å¤§å­¦ã®äº‹ä¾‹ï¼‰ã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚
   - **ç¦æ­¢äº‹é …**: ã€Œä¸€èˆ¬çš„ã«ã¯ã€ã€Œé€šå¸¸ã¯ã€ã€Œä¸€èˆ¬è«–ã¨ã—ã¦ã€ã¨ã„ã£ãŸè¡¨ç¾ã¯çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚
   - æ–‡è„ˆã«ç­”ãˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€æ­£ç›´ã«ã€Œæä¾›ã•ã‚ŒãŸè³‡æ–™å†…ã«ã¯ã€ãã®æƒ…å ±ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

2. **å¼•ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å¾¹åº•**:
   - å›ç­”ã®æ ¹æ‹ ã¨ãªã‚‹éƒ¨åˆ†ã«ã¯ã€å¿…ãš `[1]` ã‚„ `[1][2]` ã¨ã„ã†å½¢å¼ã§ç•ªå·ã‚’æŒ¯ã£ã¦ãã ã•ã„ã€‚
   - **æ³¨æ„**: `(1)` ã‚„ `Source: 1` ã¯ä¸å¯ã§ã™ã€‚å¿…ãš `[` ã¨ `]` ã§å›²ã‚“ã§ãã ã•ã„ã€‚ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãŒãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«å¿…é ˆã§ã™ï¼‰

3. **ãƒˆãƒ¼ãƒ³ã¨ãƒãƒŠãƒ¼**:
   - å­¦ç”Ÿã«å¯„ã‚Šæ·»ã£ãŸã€è¦ªã—ã¿ã‚„ã™ã„ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã§è©±ã—ã¦ãã ã•ã„ã€‚
   - å†’é ­ã¯ã€Œã“ã‚“ã«ã¡ã¯ï¼æœ­å¹Œå­¦é™¢å¤§å­¦ã®å­¦ç”Ÿã‚µãƒãƒ¼ãƒˆAIã§ã™ã€‚ã€ã§å§‹ã‚ã¦ãã ã•ã„ã€‚
   - å°‚é–€ç”¨èªã‚„æ¡ä»¶åˆ†å²ãŒå¤šã„å ´åˆã¯ã€ç®‡æ¡æ›¸ãã‚„å¤ªå­—ã‚’ä½¿ã£ã¦è¦–è¦šçš„ã«æ•´ç†ã—ã¦ãã ã•ã„ã€‚

4. **å›ç­”ãƒ—ãƒ­ã‚»ã‚¹**:
   - ã¾ãšè³‡æ–™ã‚’èª­ã¿ã€è³ªå•ã«é–¢é€£ã™ã‚‹éƒ¨åˆ†ãŒã‚ã‚‹ã‹ç¢ºèªã™ã‚‹ã€‚
   - ä¸€èˆ¬è«–ã‚’æ··ãœãªã„ã‚ˆã†ã€è³‡æ–™ã«ã‚ã‚‹äº‹å®Ÿã ã‘ã‚’æŠ½å‡ºã—ã¦å›ç­”ã‚’æ§‹æˆã™ã‚‹ã€‚
   - å¼•ç”¨ç•ªå· `[x]` ãŒæ­£ã—ã„ä½ç½®ã«ã‚ã‚‹ã‹ç¢ºèªã—ã¦ã‹ã‚‰å‡ºåŠ›ã™ã‚‹ã€‚
"""

# -----------------------------------------------------------------------------
# 3. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° & ã‚¯ãƒ©ã‚¹
# -----------------------------------------------------------------------------

def get_or_create_session_id(
    source: Union[str, Request, None] = None, 
    query_obj: Optional[ChatQuery] = None
) -> str:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’å–å¾—ã¾ãŸã¯ç”Ÿæˆã—ã¾ã™ã€‚"""
    if isinstance(source, str):
        return source
    if query_obj and hasattr(query_obj, 'session_id') and query_obj.session_id:
        return query_obj.session_id
    if isinstance(source, Request):
        if hasattr(source, "session"):
            sid = source.session.get('chat_session_id')
            if not sid:
                sid = str(uuid.uuid4())
                source.session['chat_session_id'] = sid
            return sid
    return str(uuid.uuid4())

def log_context(session_id: str, message: str, level: str = "info"):
    msg = f"[Session: {session_id}] {message}"
    getattr(logging, level, logging.info)(msg)

def send_sse(data: Dict[str, Any]) -> str:
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

async def api_request_with_retry(func, *args, **kwargs):
    """APIåˆ¶é™(429)å¯¾ç­–: ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯"""
    max_retries = 3
    default_delay = 4
    for attempt in range(max_retries):
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            error_str = str(e)
            if "429" in error_str or "Quota" in error_str:
                if attempt == max_retries - 1:
                    logging.error(f"API Quota Exceeded after {max_retries} retries.")
                    raise e
                
                wait_time = default_delay
                match = re.search(r"retry in (\d+\.?\d*)s", error_str)
                if match:
                    wait_time = float(match.group(1)) + 1.0
                else:
                    wait_time = default_delay * (2 ** attempt)

                logging.warning(f"Rate limit hit. Waiting {wait_time:.1f}s. Retrying...")
                await asyncio.sleep(wait_time)
            else:
                raise e

# --- HistoryManager ---
class ChatHistoryManager:
    def __init__(self):
        self._histories: Dict[str, List[Dict[str, str]]] = {}

    @property
    def supabase(self):
        if core_database.db_client is None or getattr(core_database.db_client, 'client', None) is None:
            return None
        return core_database.db_client.client

    def add(self, session_id: str, role: str, content: str):
        if self.supabase:
            try:
                self.supabase.table("chat_history").insert({
                    "session_id": session_id,
                    "role": role,
                    "content": content
                }).execute()
            except Exception as e:
                logging.error(f"History add failed: {e}")
        
        if session_id not in self._histories:
            self._histories[session_id] = []
        self._histories[session_id].append({"role": role, "content": content})
        if len(self._histories[session_id]) > PARAMS["MAX_HISTORY_LENGTH"]:
            self._histories[session_id] = self._histories[session_id][-PARAMS["MAX_HISTORY_LENGTH"]:]

    def get_context_string(self, session_id: str, limit: int = 10) -> str:
        if self.supabase:
            try:
                res = self.supabase.table("chat_history")\
                    .select("role, content, created_at")\
                    .eq("session_id", session_id)\
                    .order("created_at", desc=True)\
                    .limit(limit)\
                    .execute()
                if res.data:
                    history = sorted(res.data, key=lambda x: x['created_at'])
                    return "\n".join([f"{h['role']}: {h['content']}" for h in history])
            except Exception as e:
                logging.error(f"History fetch failed: {e}")
        
        hist = self._histories.get(session_id, [])[-limit:]
        return "\n".join([f"{h['role']}: {h['content']}" for h in hist])

history_manager = ChatHistoryManager()

# -----------------------------------------------------------------------------
# 4. æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# -----------------------------------------------------------------------------
class SearchPipeline:
    @staticmethod
    async def rerank(query: str, documents: List[Dict], top_k: int = 5) -> List[Dict]:
        """Gemini Structured Outputs ã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿãƒ»ç¢ºå®Ÿãªãƒªãƒ©ãƒ³ã‚¯"""
        if not documents:
            return []
        
        candidates_text = ""
        for i, doc in enumerate(documents):
            meta = doc.get('metadata', {})
            snippet = doc.get('content', '')[:300].replace('\n', ' ')
            candidates_text += f"ID:{i} [Source:{meta.get('source', '?')}]\n{snippet}\n\n"

        formatted_prompt = PROMPT_RERANK.format(query=query, candidates_text=candidates_text)

        try:
            model = genai.GenerativeModel(USE_MODEL)
            resp = await api_request_with_retry(
                model.generate_content_async,
                formatted_prompt,
                generation_config=GenerationConfig(
                    response_mime_type="application/json",
                    response_schema=RerankResponse
                ),
                safety_settings=SAFETY_SETTINGS
            )
            
            data = json.loads(resp.text)
            
            reranked = []
            for item in data.get("ranked_items", []):
                idx = item.get("id")
                score = item.get("score")
                
                if idx is not None and 0 <= idx < len(documents):
                    if score >= PARAMS["RERANK_SCORE_THRESHOLD"]:
                        doc = documents[idx]
                        doc['rerank_score'] = score
                        reranked.append(doc)
            
            reranked.sort(key=lambda x: x['rerank_score'], reverse=True)
            return reranked[:top_k]

        except Exception as e:
            logging.error(f"Rerank Error: {e}")
            return documents[:top_k]

    @staticmethod
    async def filter_diversity(documents: List[Dict], threshold: float = 0.7) -> List[Dict]:
        loop = asyncio.get_running_loop()
        unique_docs = []
        def _calc_sim(a, b): return SequenceMatcher(None, a, b).ratio()

        for doc in documents:
            content = doc.get('content', '')
            is_duplicate = False
            for selected in unique_docs:
                sim = await loop.run_in_executor(executor, _calc_sim, content, selected.get('content', ''))
                if sim > threshold:
                    is_duplicate = True; break
            if not is_duplicate: unique_docs.append(doc)
        return unique_docs

    @staticmethod
    def reorder_documents(documents: List[Dict]) -> List[Dict]:
        if not documents: return []
        first_half = documents[0::2]
        second_half = documents[1::2][::-1]
        return first_half + second_half

# -----------------------------------------------------------------------------
# 5. å‚ç…§ãƒªãƒ³ã‚¯ç”Ÿæˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ (Supabaseå¯¾å¿œ)
# -----------------------------------------------------------------------------

def get_signed_url(file_path: str, bucket_name: str = "images"):
    """
    éå…¬é–‹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦ã€1æ™‚é–“æœ‰åŠ¹ãªç½²åä»˜ãURLã‚’ç™ºè¡Œã—ã¾ã™ã€‚
    """
    try:
        if core_database.db_client is None:
            logging.error("db_client is not initialized")
            return None

        # ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã¾ã‚Œã‚‹ä½™åˆ†ãªç©ºç™½ã‚’é™¤å»
        clean_path = file_path.strip()

        # éå…¬é–‹ã® 'images' ãƒã‚±ãƒƒãƒˆã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹æ¨©ä»˜ãã®URLã‚’ç”Ÿæˆ(1æ™‚é–“æœ‰åŠ¹)
        response = core_database.db_client.client.storage.from_(bucket_name).create_signed_url(clean_path, 3600)
        
        if isinstance(response, dict) and "signedURL" in response:
            return response["signedURL"]
        return response 
    except Exception as e:
        logging.error(f"Failed to get signed URL for {file_path}: {e}")
        return None

def _build_references(response_text: str, sources_map: Dict[int, Any]) -> str:
    """
    å‚ç…§å…ƒã®ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    sources_mapã®å½¢å¼: {idx: {'source': str, 'metadata': dict}} ã¾ãŸã¯ {idx: str} (å¾Œæ–¹äº’æ›æ€§)
    """
    unique_refs = []
    seen_sources = set()
    cited_ids = set(map(int, re.findall(r'\[(\d+)\]', response_text)))
    
    for idx, source_info in sources_map.items():
        # å¾Œæ–¹äº’æ›æ€§: æ–‡å­—åˆ—ã®å ´åˆ
        if isinstance(source_info, str):
            src = source_info
            metadata = {}
        else:
            src = source_info.get('source', 'ä¸æ˜')
            metadata = source_info.get('metadata', {})
        
        # å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹ã€ã¾ãŸã¯ä¸Šä½2ã¤ä»¥å†…ã®å ´åˆã«è¡¨ç¤º
        if idx in cited_ids or idx <= 2:
            if src in seen_sources:
                continue
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰URLæƒ…å ±ã‚’å–å¾—
            url = metadata.get('url')
            source_display = src
            
            # URLãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼ˆWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãªã©ï¼‰
            if url:
                # URLã‚’ç›´æ¥ãƒªãƒ³ã‚¯ã¨ã—ã¦ç”Ÿæˆ
                unique_refs.append(
                    f"* <a href='{url}' target='_blank' class='source-link' rel='noopener noreferrer'>"
                    f"{source_display}</a>"
                )
            else:
                # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ç½²åä»˜ãURLã‚’è©¦ã™
                signed_url = get_signed_url(src)
                if signed_url:
                    unique_refs.append(
                        f"* <a href='#' class='source-link' data-url='{signed_url}' "
                        f"onclick='event.preventDefault(); showSourceImage(this.dataset.url); return false;'>"
                        f"{source_display}</a>"
                    )
                else:
                    # ãƒªãƒ³ã‚¯ãŒãªã„å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿
                    unique_refs.append(f"* {source_display}")
            
            seen_sources.add(src)
            
    if unique_refs:
        return "\n\n### å‚ç…§å…ƒãƒ‡ãƒ¼ã‚¿\n" + "\n".join(unique_refs)
    return ""

# -----------------------------------------------------------------------------
# 6. ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯
# -----------------------------------------------------------------------------
async def enhanced_chat_logic(request: Request, query_obj: ChatQuery):
    session_id = get_or_create_session_id(request, query_obj)
    feedback_id = str(uuid.uuid4())
    user_input = query_obj.query.strip()
    full_resp = ""
    
    yield send_sse({
        'feedback_id': feedback_id, 
        'status_message': 'ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™...',
        'type': 'status'
    })

    try:
        # Step 1: Embedding
        embedding_task = asyncio.create_task(
            genai.embed_content_async(
                model=query_obj.embedding_model,
                content=user_input,
                task_type="retrieval_query"
            )
        )
        
        try:
            raw_emb_result = await embedding_task
            query_embedding = raw_emb_result["embedding"]
        except Exception as e:
            log_context(session_id, f"Embedding Failed: {e}", "error")
            yield send_sse({'content': AI_MESSAGES["SYSTEM_ERROR"]})
            return

        # Step 2: Supabase QA (FAQ) Check
        if core_database.db_client:
            qa_hits = core_database.db_client.search_fallback_qa(query_embedding, match_count=1)
            if qa_hits and qa_hits[0].get('similarity', 0) >= PARAMS["QA_SIMILARITY_THRESHOLD"]:
                top_qa = qa_hits[0]
                resp = format_urls_as_links(f"ã‚ˆãã‚ã‚‹ã”è³ªå•ã«æƒ…å ±ãŒã‚ã‚Šã¾ã—ãŸã€‚\n\n---\n{top_qa['content']}")
                history_manager.add(session_id, "assistant", resp)
                yield send_sse({'content': resp, 'show_feedback': True, 'feedback_id': feedback_id})
                return

            # Step 3: Supabase Document Search (Hybrid)
            # ã€èª¿æ•´ã€‘30ä»¶å–å¾—ï¼ˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢è¦–ç‚¹ã§ã®æœ€é©åŒ–ï¼‰
            raw_docs = core_database.db_client.search_documents_hybrid(
                collection_name=query_obj.collection,
                query_text=user_input, 
                query_embedding=query_embedding,
                match_count=30
            )
        else:
            raw_docs = []

        if not raw_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        yield send_sse({'status_message': 'ğŸ§ æ–‡çŒ®ã®é‡è¤‡ã‚’é™¤å»ã—ã€ç²¾æŸ»ä¸­...', 'type': 'status'})

        # Step 4: Pipeline (Filter -> Rerank -> Reorder)
        # 4-1. é‡è¤‡æ’é™¤ (MMR)
        unique_docs = await SearchPipeline.filter_diversity(raw_docs, threshold=PARAMS["DIVERSITY_THRESHOLD"])
        
        # 4-2. ãƒªãƒ©ãƒ³ã‚¯ (Geminiã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°)
        # ã€èª¿æ•´ã€‘ä¸Šä½15ä»¶ã‚’ãƒªãƒ©ãƒ³ã‚¯ï¼ˆãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹é‡è¦–ï¼‰
        reranked_docs = await SearchPipeline.rerank(user_input, unique_docs[:15], top_k=query_obj.top_k)
        
        # 4-3. å†é…ç½®
        relevant_docs = SearchPipeline.reorder_documents(reranked_docs)

        if not relevant_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        # Step 5: Generation
        yield send_sse({'status_message': 'âœï¸ å›ç­”ã‚’åŸ·ç­†ã—ã¦ã„ã¾ã™...', 'type': 'status'})
        
        context_parts = []
        sources_map = {}
        for idx, doc in enumerate(relevant_docs, 1):
            metadata = doc.get('metadata', {})
            src = metadata.get('source', 'ä¸æ˜')
            # sources_mapã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‚’å«ã‚ã¦ã€URLæƒ…å ±ãªã©ã‚‚å‚ç…§ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
            sources_map[idx] = {
                'source': src,
                'metadata': metadata
            }
            context_parts.append(f"<doc id='{idx}' src='{src}'>\n{doc.get('content','')}\n</doc>")
        
        context_str = "\n".join(context_parts)
        history_str = history_manager.get_context_string(session_id)
        
        full_system_prompt = f"""{PROMPT_SYSTEM_GENERATION}
        
### æ¤œç´¢ã•ã‚ŒãŸè³‡æ–™ (Supabase)
{context_str}

### ã“ã‚Œã¾ã§ã®ä¼šè©±
{history_str}
"""
        model = genai.GenerativeModel(USE_MODEL)
        stream = await api_request_with_retry(
            model.generate_content_async,
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {user_input}",
            stream=True,
            generation_config=GenerationConfig(temperature=0.0), # äº‹å®Ÿæ€§é‡è¦–
            safety_settings=SAFETY_SETTINGS
        )
        
        yield send_sse({'status_message': '', 'type': 'status'})

        async for chunk in stream:
            if chunk.text:
                full_resp += chunk.text
                yield send_sse({'content': chunk.text})
        
        if not full_resp:
             yield send_sse({'content': AI_MESSAGES["BLOCKED"]})
             return

        # Step 6: References
        if "æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" not in full_resp:
            refs_text = _build_references(full_resp, sources_map)
            if refs_text:
                yield send_sse({'content': refs_text})
                full_resp += refs_text
        
        history_manager.add(session_id, "assistant", full_resp)

    except Exception as e:
        log_context(session_id, f"Critical Pipeline Error: {e}", "error")
        error_str = str(e)
        if "429" in error_str or "Quota" in error_str:
            msg = AI_MESSAGES["RATE_LIMIT"]
        elif "finish_reason" in error_str:
            msg = AI_MESSAGES["BLOCKED"]
        else:
            msg = AI_MESSAGES["SYSTEM_ERROR"]
            
        if not full_resp:
            yield send_sse({'content': msg})
            
    finally:
        yield send_sse({'show_feedback': True, 'feedback_id': feedback_id})

# -----------------------------------------------------------------------------
# 7. åˆ†ææ©Ÿèƒ½ (ç®¡ç†è€…ç”¨)
# -----------------------------------------------------------------------------
async def analyze_feedback_trends(logs: List[Dict[str, Any]]) -> AsyncGenerator[str, None]:
    if not logs:
        yield send_sse({'content': 'ãƒ‡ãƒ¼ã‚¿ãªã—'})
        return
    summary = "\n".join([f"- {l.get('rating','-')} | {l.get('comment','-')[:50]}" for l in logs[:30]])
    try:
        model = genai.GenerativeModel(USE_MODEL)
        stream = await api_request_with_retry(
            model.generate_content_async, 
            f"åˆ†æã¨æ”¹å–„ææ¡ˆ:\n{summary}", 
            stream=True
        )
        async for chunk in stream:
            if chunk.text: yield send_sse({'content': chunk.text})
    except Exception as e:
        yield send_sse({'content': str(e)})