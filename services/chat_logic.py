import logging
import uuid
import json
import asyncio
import re
import os
from typing import List, Dict, Any, AsyncGenerator, Optional, Union
from concurrent.futures import ThreadPoolExecutor
from difflib import SequenceMatcher
import typing_extensions as typing

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold, GenerationConfig
from fastapi import Request
from dotenv import load_dotenv

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from core.config import GEMINI_API_KEY
from core import database as core_database
from models.schemas import ChatQuery
from services.utils import format_urls_as_links

# -----------------------------------------------------------------------------
# 1. è¨­å®š & å®šæ•°å®šç¾©
# -----------------------------------------------------------------------------
load_dotenv()
genai.configure(api_key=GEMINI_API_KEY)

# ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
USE_MODEL = "gemini-2.5-flash"

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
PARAMS = {
    "QA_SIMILARITY_THRESHOLD": 0.90, # FAQã®å³ç­”ãƒ©ã‚¤ãƒ³
    "RERANK_SCORE_THRESHOLD": 4.0,   # ãƒªãƒ©ãƒ³ã‚¯è¶³åˆ‡ã‚Šãƒ©ã‚¤ãƒ³
    "MAX_HISTORY_LENGTH": 20,
}

# ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®š
SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
}

# ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
AI_MESSAGES = {
    "NOT_FOUND": (
        "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã”è³ªå•ã«é–¢é€£ã™ã‚‹ç¢ºå®Ÿãªæƒ…å ±ãŒè³‡æ–™å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        "å¤§å­¦çª“å£ã¸ç›´æ¥ãŠå•ã„åˆã‚ã›ã„ãŸã ãã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
    ),
    "RATE_LIMIT": "ç¾åœ¨ã‚¢ã‚¯ã‚»ã‚¹ãŒé›†ä¸­ã—ã¦ã„ã¾ã™ã€‚1åˆ†ã»ã©å¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
    "SYSTEM_ERROR": "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
    "BLOCKED": "ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãŒã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«æŠµè§¦ã—ãŸãŸã‚è¡¨ç¤ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
}

executor = ThreadPoolExecutor(max_workers=4)

# -----------------------------------------------------------------------------
# 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾©
# -----------------------------------------------------------------------------

class RankedItem(typing.TypedDict):
    id: int
    score: float
    reason: str

class RerankResponse(typing.TypedDict):
    ranked_items: list[RankedItem]

PROMPT_RERANK = """
ã‚ãªãŸã¯æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡AIã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå›ç­”ã®æ ¹æ‹ ã¨ã—ã¦ã©ã‚Œã»ã©é©åˆ‡ã‹ã€0ç‚¹ã‹ã‚‰10ç‚¹ã§æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚
è³ªå•: {query}
å€™è£œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:
{candidates_text}
"""

PROMPT_SYSTEM_GENERATION = """
ã‚ãªãŸã¯**æœ­å¹Œå­¦é™¢å¤§å­¦ã®å­¦ç”Ÿã‚µãƒãƒ¼ãƒˆAI**ã§ã™ã€‚
æä¾›ã•ã‚ŒãŸ <context> ã‚¿ã‚°å†…ã®æƒ…å ±**ã®ã¿**ã‚’ä½¿ç”¨ã—ã¦ã€è¦ªã—ã¿ã‚„ã™ãä¸å¯§ãªè¨€è‘‰é£ã„ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

# é‡è¦ãªå›ç­”ãƒ«ãƒ¼ãƒ«ï¼ˆå³å®ˆï¼‰
1. **æƒ…å ±æºã®é™å®š**:
   - å¿…ãšæä¾›ã•ã‚ŒãŸ <context> å†…ã®æƒ…å ±ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
   - **<context> ã«è¨˜è¼‰ãŒãªã„äº‹é …ã«ã¤ã„ã¦ã¯ã€è‡ªèº«ã®çŸ¥è­˜ã‚„ä¸€èˆ¬å¸¸è­˜ã§è£œå®Œã›ãšã€å¿…ãšã€Œè³‡æ–™å†…ã«æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€æ—¨ã‚’ä¼ãˆã¦ãã ã•ã„ã€‚**
   - æ¨æ¸¬ã‚„ã€Œä¸€èˆ¬çš„ã«ã¯ã€œã€ã¨ã„ã£ãŸå›ç­”ã¯ç¦æ­¢ã—ã¾ã™ã€‚

2. **å¼•ç”¨ï¼ˆã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å¼•ç”¨ï¼‰**:
   - å›ç­”ã®æ ¹æ‹ ã¨ãªã‚‹äº‹å®Ÿã®æœ«å°¾ã«ã€å¿…ãš `[1]` ã‚„ `[1][2]` ã®å½¢å¼ã§è³‡æ–™IDã‚’ä»˜è¨˜ã—ã¦ãã ã•ã„ã€‚
   - æ–‡æœ«ã ã‘ã§ãªãã€é‡è¦ãªæ•°å€¤ã‚„æ¡ä»¶ã®ã™ãå¾Œã‚ã«ä»˜ã‘ã¦ãã ã•ã„ã€‚

3. **å›ç­”ã®ãƒˆãƒ¼ãƒ³ã¨æ§‹æˆ**:
   - å†’é ­ã«ã€Œã“ã‚“ã«ã¡ã¯ï¼æœ­å¹Œå­¦é™¢å¤§å­¦ã®å­¦ç”Ÿã‚µãƒãƒ¼ãƒˆAIã§ã™ã€‚ã€ã¨ã„ã†æŒ¨æ‹¶ã¨ã€å…±æ„Ÿçš„ãªä¸€è¨€ã‚’æ·»ãˆã¦ãã ã•ã„ã€‚
   - å°‚é–€ç”¨èªã‚„è¤‡é›‘ãªè¨ˆç®—å¼ã¯ã€å¤ªå­—ã€ç®‡æ¡æ›¸ãã€æ°´å¹³ç·šï¼ˆ---ï¼‰ã‚’æ´»ç”¨ã—ã€è¦–è¦šçš„ã«ã‚ã‹ã‚Šã‚„ã™ãæ•´ç†ã—ã¦ãã ã•ã„ã€‚
   - ä¾‹ï¼šè¨ˆç®—å¼ã¯æ°´å¹³ç·šã§æŒŸã‚€ãªã©ã—ã¦å¼·èª¿ã—ã¦ãã ã•ã„ã€‚

4. **ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®å¾¹åº•é˜²æ­¢**:
   - å¤§å­¦åã‚„åˆ¶åº¦åãŒ <context> å†…ã§ç‰¹å®šã§ããªã„å ´åˆã¯ã€æ–­å®šã‚’é¿ã‘ã¦ãã ã•ã„ã€‚
"""

# -----------------------------------------------------------------------------
# 3. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ & ã‚¯ãƒ©ã‚¹
# -----------------------------------------------------------------------------

def get_or_create_session_id(
    source: Union[str, Request, None] = None, 
    query_obj: Optional[ChatQuery] = None
) -> str:
    """
    ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’å–å¾—ã¾ãŸã¯ç”Ÿæˆã—ã¾ã™ã€‚
    """
    # 1. æ–‡å­—åˆ—ãŒç›´æ¥æ¸¡ã•ã‚ŒãŸå ´åˆ
    if isinstance(source, str):
        return source

    # 2. ChatQueryã«IDãŒã‚ã‚‹å ´åˆ (æœ€å„ªå…ˆ)
    if query_obj and hasattr(query_obj, 'session_id') and query_obj.session_id:
        return query_obj.session_id
    
    # 3. Requestã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰å–å¾—
    if isinstance(source, Request):
        if hasattr(source, "session"):
            sid = source.session.get('chat_session_id')
            if not sid:
                sid = str(uuid.uuid4())
                source.session['chat_session_id'] = sid
            return sid

    # 4. è§£æ±ºã§ããªã„å ´åˆã¯æ–°è¦ç™ºè¡Œ
    return str(uuid.uuid4())

def log_context(session_id: str, message: str, level: str = "info"):
    msg = f"[Session: {session_id}] {message}"
    getattr(logging, level, logging.info)(msg)

def send_sse(data: Dict[str, Any]) -> str:
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

async def api_request_with_retry(func, *args, **kwargs):
    max_retries = 3
    default_delay = 4
    for attempt in range(max_retries):
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            error_str = str(e)
            if "429" in error_str or "Quota" in error_str:
                if attempt == max_retries - 1:
                    logging.error(f"API Quota Exceeded: {e}")
                    raise e
                match = re.search(r"retry in (\d+\.?\d*)s", error_str)
                wait_time = float(match.group(1)) + 1.0 if match else default_delay * (2 ** attempt)
                logging.warning(f"Rate limit hit. Waiting {wait_time:.1f}s...")
                await asyncio.sleep(wait_time)
            else:
                raise e

# --- HistoryManager ---
class ChatHistoryManager:
    def __init__(self):
        pass
    
    @property
    def supabase(self):
        """å®Ÿéš›ã«å¿…è¦ã«ãªã£ãŸã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—"""
        if core_database.db_client is None or getattr(core_database.db_client, 'client', None) is None:
            logging.error("Database client is not initialized.")
            return None
        return core_database.db_client.client

    def add(self, session_id: str, role: str, content: str):
        if not self.supabase: return
        try:
            self.supabase.table("chat_history").insert({
                "session_id": session_id,
                "role": role,
                "content": content
            }).execute()
        except Exception as e:
            logging.error(f"History add failed: {e}")

    def get_context_string(self, session_id: str, limit: int = 10) -> str:
        if not self.supabase: return ""
        try:
            res = self.supabase.table("chat_history")\
                .select("role, content, created_at")\
                .eq("session_id", session_id)\
                .order("created_at", desc=True)\
                .limit(limit)\
                .execute()
            if not res.data: return ""
            history = sorted(res.data, key=lambda x: x['created_at'])
            return "\n".join([f"{h['role']}: {h['content']}" for h in history])
        except Exception as e:
            logging.error(f"History fetch failed: {e}")
            return ""

history_manager = ChatHistoryManager()

# -----------------------------------------------------------------------------
# 4. æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# -----------------------------------------------------------------------------
class SearchPipeline:
    @staticmethod
    async def rerank(query: str, documents: List[Dict], top_k: int = 5) -> List[Dict]:
        if not documents: return []
        
        candidates_text = ""
        for i, doc in enumerate(documents):
            meta = doc.get('metadata', {})
            snippet = doc.get('content', '')[:300].replace('\n', ' ')
            candidates_text += f"ID:{i} [Source:{meta.get('source', '?')}]\n{snippet}\n\n"

        formatted_prompt = PROMPT_RERANK.format(query=query, candidates_text=candidates_text)

        try:
            model = genai.GenerativeModel(USE_MODEL)
            resp = await api_request_with_retry(
                model.generate_content_async,
                formatted_prompt,
                generation_config=GenerationConfig(
                    response_mime_type="application/json",
                    response_schema=RerankResponse
                ),
                safety_settings=SAFETY_SETTINGS
            )
            data = json.loads(resp.text)
            reranked = []
            for item in data.get("ranked_items", []):
                idx = item.get("id")
                score = item.get("score")
                if idx is not None and 0 <= idx < len(documents):
                    if score >= PARAMS["RERANK_SCORE_THRESHOLD"]:
                        doc = documents[idx]
                        doc['rerank_score'] = score
                        reranked.append(doc)
            reranked.sort(key=lambda x: x['rerank_score'], reverse=True)
            return reranked[:top_k]
        except Exception as e:
            logging.error(f"Rerank Error: {e}")
            return documents[:top_k]

    @staticmethod
    async def filter_diversity(documents: List[Dict], threshold: float = 0.7) -> List[Dict]:
        loop = asyncio.get_running_loop()
        unique_docs = []
        def _calc_sim(a, b): return SequenceMatcher(None, a, b).ratio()

        for doc in documents:
            content = doc.get('content', '')
            is_duplicate = False
            for selected in unique_docs:
                sim = await loop.run_in_executor(executor, _calc_sim, content, selected.get('content', ''))
                if sim > threshold:
                    is_duplicate = True; break
            if not is_duplicate: unique_docs.append(doc)
        return unique_docs

    @staticmethod
    def reorder_documents(documents: List[Dict]) -> List[Dict]:
        if not documents: return []
        first_half = documents[0::2]
        second_half = documents[1::2][::-1]
        return first_half + second_half

# -----------------------------------------------------------------------------
# 5. æ¤œç´¢ãƒ»å‚ç…§ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -----------------------------------------------------------------------------

def get_signed_url(file_path: str, bucket_name: str = "images"):
    """
    éå…¬é–‹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦ã€1æ™‚é–“æœ‰åŠ¹ãªç½²åä»˜ãURLã‚’ç™ºè¡Œã—ã¾ã™ã€‚
    """
    try:
        # éå…¬é–‹ã® 'images' ãƒã‚±ãƒƒãƒˆã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹æ¨©ä»˜ãã®URLã‚’ç”Ÿæˆ
        response = core_database.db_client.client.storage.from_(bucket_name).create_signed_url(file_path, 3600)
        
        if isinstance(response, dict) and "signedURL" in response:
            return response["signedURL"]
        return response 
    except Exception as e:
        logging.error(f"ç½²åä»˜ãURLã®ç™ºè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ (Path: {file_path}): {e}")
        return None

def _build_references(response_text: str, sources_map: Dict[int, str]) -> str:
    """
    å›ç­”å†…ã® [1] ãªã©ã®å¼•ç”¨ã‚¿ã‚°ã«åŸºã¥ãã€ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªç”»åƒãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    unique_refs = []
    seen_sources = set()
    # æœ¬æ–‡ä¸­ã® [1] ãªã©ã®æ•°å­—ã‚’ã™ã¹ã¦æŠ½å‡º
    cited_ids = set(map(int, re.findall(r'\[(\d+)\]', response_text)))
    
    for idx, src in sources_map.items():
        # å¼•ç”¨ã•ã‚ŒãŸIDã€ã¾ãŸã¯æœ€åˆã®2ä»¶ã‚’å¸¸ã«è¡¨ç¤º
        if idx in cited_ids or idx <= 2:
            if src in seen_sources: continue
            
            # éå…¬é–‹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å¯¾å¿œï¼šç½²åä»˜ãURLã‚’å–å¾—
            signed_url = get_signed_url(src)
            
            if signed_url:
                # onclickã‚¤ãƒ™ãƒ³ãƒˆã§JavaScriptã«URLã‚’æ¸¡ã™
                unique_refs.append(
                    f"* <a href='#' class='source-link' "
                    f"data-url='{signed_url}' "
                    f"onclick='event.preventDefault(); showSourceImage(this.dataset.url); return false;'>"
                    f"{src}</a>"
                )
            else:
                # URLå–å¾—å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                unique_refs.append(f"* {src} (ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸å¯)")
                
            seen_sources.add(src)
            
    if unique_refs:
        # è³‡æ–™ã®ã€Œã©ã“ã«æ›¸ã„ã¦ã‚ã£ãŸã‹ã‚’è¡¨ç¤ºã™ã‚‹ã€ãƒ«ãƒ¼ãƒ«ã«åŸºã¥ã 
        return "\n\n### å‚ç…§å…ƒãƒ‡ãƒ¼ã‚¿\n" + "\n".join(unique_refs)
    return ""

# -----------------------------------------------------------------------------
# 6. ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯
# -----------------------------------------------------------------------------
async def enhanced_chat_logic(request: Request, query_obj: ChatQuery):
    """
    ã€é‡è¦ã€‘å¼•æ•°ã®é †åºã¯ (request, query_obj) ã§ã™ã€‚
    """
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã®å–å¾—
    session_id = get_or_create_session_id(request, query_obj)
    
    feedback_id = str(uuid.uuid4())
    user_input = query_obj.query.strip()
    
    yield send_sse({
        'feedback_id': feedback_id, 
        'status_message': 'ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™...',
        'type': 'status'
    })

    full_resp = ""

    try:
        # Step 1: Embedding
        embedding_task = asyncio.create_task(
            genai.embed_content_async(
                model=query_obj.embedding_model,
                content=user_input,
                task_type="retrieval_query"
            )
        )
        
        try:
            raw_emb_result = await embedding_task
            query_embedding = raw_emb_result["embedding"]
        except Exception as e:
            log_context(session_id, f"Embedding Failed: {e}", "error")
            yield send_sse({'content': AI_MESSAGES["SYSTEM_ERROR"]})
            return

        # Step 2: FAQ Check
        if core_database.db_client:
            qa_hits = core_database.db_client.search_fallback_qa(query_embedding, match_count=1)
            if qa_hits and qa_hits[0].get('similarity', 0) >= PARAMS["QA_SIMILARITY_THRESHOLD"]:
                top_qa = qa_hits[0]
                resp = format_urls_as_links(f"ã‚ˆãã‚ã‚‹ã”è³ªå•ã«æƒ…å ±ãŒã‚ã‚Šã¾ã—ãŸã€‚\n\n---\n{top_qa['content']}")
                history_manager.add(session_id, "assistant", resp)
                yield send_sse({'content': resp, 'show_feedback': True, 'feedback_id': feedback_id})
                return

            # Step 3: Search
            raw_docs = core_database.db_client.search_documents_hybrid(
                collection_name=query_obj.collection,
                query_text=user_input, 
                query_embedding=query_embedding,
                match_count=30
            )
        else:
            raw_docs = []

        if not raw_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        yield send_sse({'status_message': 'ğŸ§ AIãŒæ–‡çŒ®ã‚’èª­ã‚“ã§é¸å®šä¸­...', 'type': 'status'})

        # Step 4: Pipeline
        unique_docs = await SearchPipeline.filter_diversity(raw_docs)
        reranked_docs = await SearchPipeline.rerank(user_input, unique_docs[:15], top_k=query_obj.top_k)
        relevant_docs = SearchPipeline.reorder_documents(reranked_docs)

        if not relevant_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        # Step 5: Generation
        yield send_sse({'status_message': 'âœï¸ å›ç­”ã‚’åŸ·ç­†ã—ã¦ã„ã¾ã™...', 'type': 'status'})
        
        context_parts = []
        sources_map = {}
        
        for idx, doc in enumerate(relevant_docs, 1):
            src = doc.get('metadata', {}).get('source', 'ä¸æ˜')
            sources_map[idx] = src
            context_parts.append(f"<doc id='{idx}' src='{src}'>\n{doc.get('content','')}\n</doc>")
        
        context_str = "\n".join(context_parts)
        history_str = history_manager.get_context_string(session_id)
        
        full_system_prompt = f"""{PROMPT_SYSTEM_GENERATION}
        
### æ¤œç´¢ã•ã‚ŒãŸè³‡æ–™
{context_str}

### ã“ã‚Œã¾ã§ã®ä¼šè©±
{history_str}
"""
        model = genai.GenerativeModel(USE_MODEL)
        stream = await api_request_with_retry(
            model.generate_content_async,
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {user_input}",
            stream=True,
            generation_config=GenerationConfig(temperature=0.0), # è³‡æ–™ã«å¿ å®Ÿã«ã™ã‚‹
            safety_settings=SAFETY_SETTINGS
        )
        
        yield send_sse({'status_message': '', 'type': 'status'})

        async for chunk in stream:
            if chunk.text:
                full_resp += chunk.text
                yield send_sse({'content': chunk.text})
        
        if not full_resp:
             yield send_sse({'content': AI_MESSAGES["BLOCKED"]})
             return

        # Step 6: References
        if "æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" not in full_resp:
            # ã“ã“ã§ä¿®æ­£ç‰ˆã® _build_references ã‚’å‘¼ã³å‡ºã—ã¾ã™
            refs_text = _build_references(full_resp, sources_map)
            if refs_text:
                yield send_sse({'content': refs_text})
                full_resp += refs_text
        
        history_manager.add(session_id, "assistant", full_resp)

    except Exception as e:
        log_context(session_id, f"Critical Pipeline Error: {e}", "error")
        if not full_resp:
            yield send_sse({'content': AI_MESSAGES["SYSTEM_ERROR"]})
            
    finally:
        yield send_sse({'show_feedback': True, 'feedback_id': feedback_id})

# -----------------------------------------------------------------------------
# 7. åˆ†ææ©Ÿèƒ½
# -----------------------------------------------------------------------------
async def analyze_feedback_trends(logs: List[Dict[str, Any]]) -> AsyncGenerator[str, None]:
    if not logs:
        yield send_sse({'content': 'ãƒ‡ãƒ¼ã‚¿ãªã—'})
        return
    summary = "\n".join([f"- {l.get('rating','-')} | {l.get('comment','-')[:50]}" for l in logs[:30]])
    try:
        model = genai.GenerativeModel(USE_MODEL)
        stream = await api_request_with_retry(
            model.generate_content_async, 
            f"åˆ†æã¨æ”¹å–„ææ¡ˆ:\n{summary}", 
            stream=True
        )
        async for chunk in stream:
            if chunk.text: yield send_sse({'content': chunk.text})
    except Exception as e:
        yield send_sse({'content': str(e)})