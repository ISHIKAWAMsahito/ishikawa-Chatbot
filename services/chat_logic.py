# logic/chat.py
import logging
from fastapi import Request
import uuid
# ä¾å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from typing import List, Dict, Any, AsyncGenerator
from core import database as core_database
from core.constants import PARAMS, AI_MESSAGES
from models.schemas import ChatQuery
from services.llm import LLMService
from services.search import SearchService
from services.storage import StorageService
from services.utils import get_or_create_session_id, send_sse, log_context, ChatHistoryManager
from services import prompts
from services.utils import format_urls_as_links # å…ƒã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã®æ—¢å­˜importã‚’æƒ³å®š

# DIï¼ˆä¾å­˜æ€§ã®æ³¨å…¥ï¼‰ã®æº–å‚™
# æœ¬ç•ªç’°å¢ƒã§ã¯FastAPIã®Dependsç­‰ã§æ³¨å…¥ã™ã‚‹ã®ãŒãƒ™ã‚¹ãƒˆã§ã™ãŒã€ã“ã“ã§ã¯åˆæœŸåŒ–
llm_service = LLMService()
search_service = SearchService(llm_service)
storage_service = StorageService()
history_manager = ChatHistoryManager(max_length=PARAMS["MAX_HISTORY_LENGTH"])

async def enhanced_chat_logic(request: Request, chat_req: ChatQuery):
    """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯"""
    session_id = get_or_create_session_id(request)
    user_input = chat_req.query.strip()
    feedback_id = str(uuid.uuid4()) # å¿…è¦ã«å¿œã˜ã¦ç”Ÿæˆ
    
    yield send_sse({'feedback_id': feedback_id, 'status_message': 'ğŸ” è³ªå•ã‚’åˆ†æã—ã¦ã„ã¾ã™...'})

    try:
        # 1. ã‚¯ã‚¨ãƒªæ‹¡å¼µ
        expanded_query = await search_service.expand_query(user_input)
        
        # 2. Embeddingç”Ÿæˆ
        query_embedding = await llm_service.get_embedding(expanded_query)

        # 3. FAQãƒã‚§ãƒƒã‚¯ (æ—¢å­˜DBãƒ­ã‚¸ãƒƒã‚¯åˆ©ç”¨)
        if qa_hits := core_database.db_client.search_fallback_qa(query_embedding, match_count=1):
            top_qa = qa_hits[0]
            if top_qa.get('similarity', 0) >= PARAMS["QA_SIMILARITY_THRESHOLD"]:
                resp = format_urls_as_links(f"ã‚ˆãã‚ã‚‹ã”è³ªå•ã«å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n\n---\n{top_qa['content']}")
                history_manager.add(session_id, "assistant", resp)
                yield send_sse({'content': resp, 'show_feedback': True, 'feedback_id': feedback_id})
                return

        yield send_sse({'status_message': 'ğŸ“š è³‡æ–™ã‚’åºƒãé›†ã‚ã¦ã„ã¾ã™...'})

        # 4. DBæ¤œç´¢ (Hybrid)
        raw_docs = core_database.db_client.search_documents_hybrid(
            collection_name=chat_req.collection,
            query_text=expanded_query,
            query_embedding=query_embedding,
            match_count=50
        )

        if not raw_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        yield send_sse({'status_message': 'ğŸ§ AIãŒæ–‡çŒ®ã‚’ç²¾èª­ãƒ»é¸åˆ¥ä¸­...'})

        # 5. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° & ãƒªãƒ©ãƒ³ã‚¯ & ä¸¦ã¹æ›¿ãˆ
        unique_docs = search_service.filter_diversity(raw_docs)
        
        # ãƒªãƒ©ãƒ³ã‚¯å…¥åŠ›ï¼šå®šæ•°ã§å®šç¾©ã—ãŸä¸Šä½30ä»¶
        rerank_input = unique_docs[:PARAMS["RERANK_TOP_K_INPUT"]]
        
        # ãƒªãƒ©ãƒ³ã‚¯å®Ÿè¡Œ
        relevant_docs = await search_service.rerank(
            query=user_input, 
            documents=rerank_input, 
            top_k=chat_req.top_k
        )

        if not relevant_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        # Lost in the Middle å¯¾ç­–
        final_docs = search_service.reorder_litm(relevant_docs)

        yield send_sse({'status_message': 'âœï¸ å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...'})

        # 6. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        context_parts = []
        sources_map = {}
        for idx, doc in enumerate(final_docs, 1):
            meta = doc.get('metadata', {})
            src_display = meta.get('source', 'ä¸æ˜')
            src_storage = meta.get('image_path', src_display)
            
            sources_map[idx] = {'display': src_display, 'storage': src_storage}
            context_parts.append(f"<doc id='{idx}' src='{src_display}'>\n{doc.get('content','')}\n</doc>")
        
        context_str = "\n".join(context_parts)
        
        # 7. å›ç­”ç”Ÿæˆ (Chain of Thought ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½¿ç”¨)
        full_system_prompt = f"{prompts.SYSTEM_GENERATION}\n<context>\n{context_str}\n</context>"
        
        stream = await llm_service.generate_stream(
            prompt=f"è³ªå•: {user_input}",
            system_prompt=full_system_prompt
        )
        
        full_resp = ""
        async for chunk in stream:
            if chunk.text:
                full_resp += chunk.text
                yield send_sse({'content': chunk.text})

        if not full_resp:
             yield send_sse({'content': AI_MESSAGES["BLOCKED"]})
             history_manager.add(session_id, "assistant", "[[BLOCKED]]")
             return

        # 8. å‚ç…§ãƒªãƒ³ã‚¯ç”Ÿæˆ
        if "æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" not in full_resp:
            yield send_sse({'status_message': 'ğŸ”— å‚ç…§ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆä¸­...'})
            refs_text = await storage_service.build_references_async(full_resp, sources_map)
            if refs_text:
                yield send_sse({'content': refs_text})
                full_resp += refs_text
        
        history_manager.add(session_id, "assistant", full_resp)

    except Exception as e:
        log_context(session_id, f"Critical Pipeline Error: {e}", "error")
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚‚å…±é€šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å®šæ•°ã‚’ä½¿ç”¨
        error_str = str(e)
        if "429" in error_str or "Quota" in error_str:
            msg = AI_MESSAGES["RATE_LIMIT"]
        elif "finish_reason" in error_str:
            msg = AI_MESSAGES["BLOCKED"]
        else:
            msg = AI_MESSAGES["SYSTEM_ERROR"]
        yield send_sse({'content': msg})
        
    finally:
        yield send_sse({'show_feedback': True, 'feedback_id': feedback_id})

# services/chat_logic.py ã®æœ«å°¾ã«è¿½åŠ 

async def analyze_feedback_trends(logs: List[Dict[str, Any]]) -> AsyncGenerator[str, None]:
    """
    ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ­ã‚°ã‚’åˆ†æã—ã€æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    (api/chat.py ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã¾ã™)
    """
    if not logs:
        yield send_sse({'content': 'åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚'})
        return
    
    # æœ€æ–°50ä»¶ã®ã¿ã‚’åˆ†æå¯¾è±¡ã¨ã™ã‚‹ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ã®ãŸã‚ï¼‰
    summary = "\n".join([f"- è©•ä¾¡:{l.get('rating','-')} | {l.get('comment','-')[:100]}" for l in logs[:50]])
    
    prompt = f"""
    ä»¥ä¸‹ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆåˆ©ç”¨ãƒ­ã‚°ã‚’åˆ†æã—ã€Markdownã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    
    # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿
    {summary}
    
    # å‡ºåŠ›é …ç›®
    1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¸»ãªé–¢å¿ƒäº‹ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
    2. ä½è©•ä¾¡ã®åŸå› ã¨æ”¹å–„ç­–
    3. æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³
    """
    
    try:
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã«ã‚ã‚‹ llm_service ã‚’ä½¿ç”¨
        stream = await llm_service.generate_stream(prompt)
        async for chunk in stream:
            if chunk.text:
                yield send_sse({'content': chunk.text})
    except Exception as e:
        yield send_sse({'content': f'åˆ†æã‚¨ãƒ©ãƒ¼: {e}'})