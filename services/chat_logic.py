# services/chat_logic.py
import logging
import uuid
from datetime import datetime, timedelta, timezone  # æ—¥æ™‚æ“ä½œç”¨ã«è¿½åŠ 
from typing import List, Dict, Any, AsyncGenerator
from fastapi import Request

# LangSmith ãƒˆãƒ¬ãƒ¼ã‚¹ç”¨
from langsmith import traceable
from langsmith.run_helpers import get_current_run_tree

# ä¾å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from core import database as core_database
from core.constants import PARAMS, AI_MESSAGES
from models.schemas import ChatQuery
from services.llm import LLMService
from services.search import SearchService
from services.storage import StorageService
from services.utils import (
    get_or_create_session_id, 
    send_sse, 
    log_context, 
    ChatHistoryManager, 
    format_urls_as_links
)
from services import prompts

# DIï¼ˆä¾å­˜æ€§ã®æ³¨å…¥ï¼‰ã®æº–å‚™
llm_service = LLMService()
search_service = SearchService(llm_service)
storage_service = StorageService()
history_manager = ChatHistoryManager(max_length=PARAMS["MAX_HISTORY_LENGTH"])

@traceable(name="Chat_Pipeline_Parent", run_type="chain")
async def enhanced_chat_logic(request: Request, chat_req: ChatQuery):
    """
    ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯
    
    ä¸»ãªå¤‰æ›´ç‚¹:
    - æ—¥æœ¬æ™‚é–“ (JST) ã§ã®ç¾åœ¨æ—¥æ™‚å–å¾—ã‚’è¿½åŠ 
    - ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¸ç¾åœ¨æ—¥æ™‚ (current_date) ã‚’æ³¨å…¥ã™ã‚‹å‡¦ç†ã‚’è¿½åŠ 
    """
    session_id = get_or_create_session_id(request)
    user_input = chat_req.query.strip()
    feedback_id = str(uuid.uuid4())
    
    # ---------------------------------------------------------
    # æ—¥æ™‚å–å¾—ãƒ­ã‚¸ãƒƒã‚¯ (JSTå›ºå®š)
    # ---------------------------------------------------------
    # ã‚µãƒ¼ãƒãƒ¼ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è¨­å®šã«ä¾å­˜ã›ãšã€å¸¸ã«æ—¥æœ¬æ™‚é–“ã‚’å–å¾—ã—ã¾ã™ã€‚
    # ã“ã‚Œã«ã‚ˆã‚Šã€Œç¾åœ¨ãŒ2025å¹´åº¦ã‹ã€ãªã©ã‚’AIãŒæ­£ç¢ºã«åˆ¤æ–­ã§ãã¾ã™ã€‚
    JST = timezone(timedelta(hours=9), 'JST')
    now = datetime.now(JST)
    current_date_str = now.strftime("%Yå¹´%mæœˆ%dæ—¥") # ä¾‹: 2025å¹´10æœˆ27æ—¥
    
    # LangSmith: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³IDãªã©ï¼‰ã‚’è¿½åŠ 
    run_tree = get_current_run_tree()
    if run_tree:
        run_tree.add_metadata({
            "session_id": session_id, 
            "user_query": user_input,
            "current_date_jst": current_date_str
        })

    yield send_sse({'feedback_id': feedback_id, 'status_message': 'ğŸ” è³ªå•ã‚’åˆ†æã—ã¦ã„ã¾ã™...'})

    try:
        # -----------------------------------------------------
        # 1. ã‚¯ã‚¨ãƒªæ‹¡å¼µ
        # -----------------------------------------------------
        expanded_query = await search_service.expand_query(user_input)
        
        # -----------------------------------------------------
        # 2. Embeddingç”Ÿæˆ
        # -----------------------------------------------------
        query_embedding = await llm_service.get_embedding(
            text=expanded_query, 
            model=chat_req.embedding_model
        )

        # -----------------------------------------------------
        # 3. FAQãƒã‚§ãƒƒã‚¯ (Fallback)
        # -----------------------------------------------------
        if qa_hits := core_database.db_client.search_fallback_qa(query_embedding, match_count=1):
            top_qa = qa_hits[0]
            if top_qa.get('similarity', 0) >= PARAMS["QA_SIMILARITY_THRESHOLD"]:
                # FAQå›ç­”ã«ã‚‚ãƒªãƒ³ã‚¯åŒ–å‡¦ç†ã‚’é©ç”¨
                resp_content = top_qa['content']
                resp_formatted = format_urls_as_links(resp_content)
                
                formatted_response = f"ã‚ˆãã‚ã‚‹ã”è³ªå•ã«å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n\n---\n{resp_formatted}"
                history_manager.add(session_id, "assistant", formatted_response)
                yield send_sse({'content': formatted_response, 'show_feedback': True, 'feedback_id': feedback_id})
                return

        yield send_sse({'status_message': 'ğŸ“š è³‡æ–™ã‚’åºƒãé›†ã‚ã¦ã„ã¾ã™...'})

        # -----------------------------------------------------
        # 4. DBæ¤œç´¢ (Hybrid Search)
        # -----------------------------------------------------
        raw_docs = core_database.db_client.search_documents_hybrid(
            collection_name=chat_req.collection,
            query_text=expanded_query,
            query_embedding=query_embedding,
            match_count=50
        )

        if not raw_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        yield send_sse({'status_message': 'ğŸ§ AIãŒæ–‡çŒ®ã‚’ç²¾èª­ãƒ»é¸åˆ¥ä¸­...'})

        # -----------------------------------------------------
        # 5. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° & ãƒªãƒ©ãƒ³ã‚¯ & ä¸¦ã¹æ›¿ãˆ
        # -----------------------------------------------------
        unique_docs = search_service.filter_diversity(raw_docs)
        rerank_input = unique_docs[:PARAMS["RERANK_TOP_K_INPUT"]]
        
        # ãƒªãƒ©ãƒ³ã‚¯å‡¦ç†
        # (æ³¨: prompts.py ã® RERANK ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚‚æœ€æ–°æƒ…å ±ã®å„ªå…ˆæŒ‡ç¤ºãŒã‚ã‚‹ã“ã¨ãŒæœ›ã¾ã—ã„)
        relevant_docs = await search_service.rerank(
            query=user_input, 
            documents=rerank_input, 
            top_k=chat_req.top_k
        )

        if not relevant_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        # Lost in the Middle å¯¾ç­– (é‡è¦ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å…ˆé ­ã¨æœ«å°¾ã«é…ç½®)
        final_docs = search_service.reorder_litm(relevant_docs)

        yield send_sse({'status_message': 'âœï¸ å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...'})

        # -----------------------------------------------------
        # 6. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        # -----------------------------------------------------
        context_parts = []
        sources_map = {}
        for idx, doc in enumerate(final_docs, 1):
            meta = doc.get('metadata', {})
            src_display = meta.get('source', 'ä¸æ˜')
            src_storage = meta.get('image_path', src_display)
            
            # AIã¸ã®ãƒ’ãƒ³ãƒˆã¨ã—ã¦ Source URL ã‚’æ˜ç¤º
            # ã“ã‚Œã«ã‚ˆã‚Šãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã¾ã‚Œã‚‹æ—¥ä»˜ã‚„å¹´åº¦æƒ…å ±ã‚‚AIãŒèªè­˜ã—ã‚„ã™ããªã‚Šã¾ã™
            doc_context = f"<doc id='{idx}' src='{src_display}'>\n"
            doc_context += f"Source Reference: {src_display}\n" 
            doc_context += f"Content: {doc.get('content','')}\n"
            doc_context += "</doc>"
            
            sources_map[idx] = {'display': src_display, 'storage': src_storage}
            context_parts.append(doc_context)
        
        context_str = "\n".join(context_parts)
        
        # -----------------------------------------------------
        # 7. å›ç­”ç”Ÿæˆ (LLM)
        # -----------------------------------------------------
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã® {context_text} ã¨ {current_date} ã‚’åŸ‹ã‚è¾¼ã‚€
        # æ³¨: prompts.SYSTEM_GENERATION ã« {current_date} ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãŒå¿…è¦ã§ã™
        try:
            full_system_prompt = prompts.SYSTEM_GENERATION.format(
                context_text=context_str,
                current_date=current_date_str
            )
        except KeyError:
            # ä¸‡ãŒä¸€ prompts.py ãŒæ›´æ–°ã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            logging.warning("SYSTEM_GENERATION prompt does not have 'current_date' placeholder.")
            full_system_prompt = prompts.SYSTEM_GENERATION.format(context_text=context_str)
        
        stream = await llm_service.generate_stream(
            prompt=f"è³ªå•: {user_input}",
            system_prompt=full_system_prompt
        )
        
        full_resp = ""
        async for chunk in stream:
            if chunk.text:
                full_resp += chunk.text
                yield send_sse({'content': chunk.text})

        if not full_resp:
             yield send_sse({'content': AI_MESSAGES["BLOCKED"]})
             history_manager.add(session_id, "assistant", "[[BLOCKED]]")
             return

        # -----------------------------------------------------
        # 8. å‚ç…§ãƒªãƒ³ã‚¯ç”Ÿæˆã¨æœ€çµ‚æ•´å½¢
        # -----------------------------------------------------
        final_content_updates = ""
        
        if "æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" not in full_resp:
            yield send_sse({'status_message': 'ğŸ”— å‚ç…§ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆä¸­...'})
            
            # StorageServiceã«ã‚ˆã‚‹ç½²åä»˜ãURLç­‰ã®å‡¦ç†
            refs_text = await storage_service.build_references_async(full_resp, sources_map)
            
            if refs_text:
                full_resp += refs_text
                final_content_updates += refs_text

        # æœ€å¾Œã«å…¨æ–‡ã«å¯¾ã—ã¦URLãƒªãƒ³ã‚¯åŒ–å‡¦ç†ã‚’é©ç”¨ã—ã¦å±¥æ­´ã«ä¿å­˜
        formatted_full_resp = format_urls_as_links(full_resp)
        history_manager.add(session_id, "assistant", formatted_full_resp)

        # ã‚‚ã— `refs_text` ãŒã‚ã£ãŸå ´åˆã€ãã‚Œã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«è¿½é€
        if final_content_updates:
            yield send_sse({'content': final_content_updates})

    except Exception as e:
        log_context(session_id, f"Critical Pipeline Error: {e}", "error")
        if run_tree:
            run_tree.end(error=str(e))
            
        error_str = str(e)
        if "429" in error_str or "Quota" in error_str:
            msg = AI_MESSAGES["RATE_LIMIT"]
        elif "finish_reason" in error_str:
            msg = AI_MESSAGES["BLOCKED"]
        else:
            msg = AI_MESSAGES["SYSTEM_ERROR"]
        yield send_sse({'content': msg})
        
    finally:
        # å®Œäº†ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼æ™‚ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯UIã‚’è¡¨ç¤º
        yield send_sse({'show_feedback': True, 'feedback_id': feedback_id})

@traceable(name="Feedback_Analysis_Job", run_type="chain")
async def analyze_feedback_trends(logs: List[Dict[str, Any]]) -> AsyncGenerator[str, None]:
    """
    ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æç”¨ãƒ­ã‚¸ãƒƒã‚¯ (å¤‰æ›´ãªã—)
    """
    if not logs:
        yield send_sse({'content': 'åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚'})
        return
    
    summary = "\n".join([f"- è©•ä¾¡:{l.get('rating','-')} | {l.get('comment','-')[:100]}" for l in logs[:50]])
    prompt = f"""
    ä»¥ä¸‹ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆåˆ©ç”¨ãƒ­ã‚°ã‚’åˆ†æã—ã€Markdownã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿
    {summary}
    # å‡ºåŠ›é …ç›®
    1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¸»ãªé–¢å¿ƒäº‹ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
    2. ä½è©•ä¾¡ã®åŸå› ã¨æ”¹å–„ç­–
    3. æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³
    """
    try:
        stream = await llm_service.generate_stream(prompt)
        async for chunk in stream:
            if chunk.text:
                yield send_sse({'content': chunk.text})
    except Exception as e:
        yield send_sse({'content': f'åˆ†æã‚¨ãƒ©ãƒ¼: {e}'})