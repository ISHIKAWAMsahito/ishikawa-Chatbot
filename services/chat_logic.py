import logging
import uuid
import json
import asyncio
import re
from typing import List, Dict, Any, AsyncGenerator, Optional, Union
from concurrent.futures import ThreadPoolExecutor
from difflib import SequenceMatcher

import google.generativeai as genai
from google.generativeai.types import GenerationConfig, HarmCategory, HarmBlockThreshold
from fastapi import Request

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from core.config import GEMINI_API_KEY
from core import database as core_database
from models.schemas import ChatQuery
from services.utils import format_urls_as_links

# -----------------------------------------------------------------------------
# è¨­å®š & å®šæ•°
# -----------------------------------------------------------------------------
genai.configure(api_key=GEMINI_API_KEY)

# ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
PARAMS = {
    "STRICT_THRESHOLD": 0.80,
    "QA_SIMILARITY_THRESHOLD": 0.95,
    "RERANK_SCORE_THRESHOLD": 6.5,
    "MAX_HISTORY_LENGTH": 20,
    "MAX_CONTEXT_CHAR_LENGTH": 60000,
}

SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
}

AI_MESSAGES = {
    "NOT_FOUND": (
        "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã”è³ªå•ã«é–¢é€£ã™ã‚‹ç¢ºå®Ÿãªæƒ…å ±ãŒè³‡æ–™å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        "å¤§å­¦çª“å£ã¸ç›´æ¥ãŠå•ã„åˆã‚ã›ã„ãŸã ãã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
    ),
    "ERROR": "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ç®¡ç†è€…ã«é€£çµ¡ã—ã¦ãã ã•ã„ã€‚",
}

# CPUå‡¦ç†ç”¨ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«
executor = ThreadPoolExecutor(max_workers=4)

# -----------------------------------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# -----------------------------------------------------------------------------
def get_or_create_session_id(request: Request) -> str:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã®ç®¡ç†"""
    session_id = request.session.get('chat_session_id')
    if not session_id:
        session_id = str(uuid.uuid4())
        request.session['chat_session_id'] = session_id
    return session_id

def log_context(session_id: str, message: str, level: str = "info"):
    """æ§‹é€ åŒ–ãƒ­ã‚°å‡ºåŠ›"""
    msg = f"[Session: {session_id}] {message}"
    getattr(logging, level, logging.info)(msg)

def send_sse(data: Dict[str, Any]) -> str:
    """SSEå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆãƒ˜ãƒ«ãƒ‘ãƒ¼"""
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

def clean_and_parse_json(text: str) -> Dict[str, Any]:
    """Geminiã®å‡ºåŠ›ã‚’å®‰å…¨ã«JSONãƒ‘ãƒ¼ã‚¹"""
    text = re.sub(r'^```json\s*', '', text)
    text = re.sub(r'^```\s*', '', text)
    text = re.sub(r'\s*```$', '', text)
    try:
        return json.loads(text.strip())
    except json.JSONDecodeError:
        return {}

class ChatHistoryManager:
    """ç°¡æ˜“ãƒ¡ãƒ¢ãƒªå†…å±¥æ­´ç®¡ç†"""
    def __init__(self):
        self._histories: Dict[str, List[Dict[str, str]]] = {}

    def add(self, session_id: str, role: str, content: str):
        if session_id not in self._histories:
            self._histories[session_id] = []
        self._histories[session_id].append({"role": role, "content": content})
        if len(self._histories[session_id]) > PARAMS["MAX_HISTORY_LENGTH"]:
            self._histories[session_id] = self._histories[session_id][-PARAMS["MAX_HISTORY_LENGTH"]:]

history_manager = ChatHistoryManager()

# -----------------------------------------------------------------------------
# ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯: æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# -----------------------------------------------------------------------------
class SearchPipeline:
    """æ¤œç´¢ãƒ»ãƒªãƒ©ãƒ³ã‚¯ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ä¸€å…ƒç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    @staticmethod
    async def optimize_query(user_query: str, session_id: str) -> str:
        """HyDE + Query Expansion"""
        prompt = f"""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«åŸºã¥ã„ã¦ã€å¤§å­¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«æœ€é©ãªã€Œæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        å°‚é–€ç”¨èªã¸ã®è¨€ã„æ›ãˆï¼ˆä¾‹: "å–ã‚Šæ¶ˆã—" -> "å±¥ä¿®ä¸­æ­¢"ï¼‰ã‚’å«ã‚ã€å‡ºåŠ›ã¯æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã«ã—ã¦ãã ã•ã„ã€‚
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: "{user_query}"
        """
        try:
            model = genai.GenerativeModel("gemini-2.5-flash")
            resp = await model.generate_content_async(prompt, safety_settings=SAFETY_SETTINGS)
            optimized = resp.text.strip()
            log_context(session_id, f"ã‚¯ã‚¨ãƒªæ‹¡å¼µ: {optimized}")
            return optimized
        except Exception:
            return user_query

    @staticmethod
    async def check_ambiguity(query: str) -> Dict[str, Any]:
        """æ„å›³ã®æ›–æ˜§æ€§åˆ¤å®š"""
        # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®é«˜é€Ÿåˆ¤å®š
        if len(query) > 10 and any(x in query for x in ["æ–¹æ³•", "å ´æ‰€", "ç”³è«‹", "ã«ã¤ã„ã¦", "æ•™ãˆ"]):
            return {"is_ambiguous": False}

        prompt = f"""
        ã‚ãªãŸã¯ãƒ˜ãƒ«ãƒ—ãƒ‡ã‚¹ã‚¯AIã§ã™ã€‚ä»¥ä¸‹ã®è³ªå•ãŒå›ç­”ã«ååˆ†ãªå…·ä½“æ€§ã‚’æŒã£ã¦ã„ã‚‹ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
        è³ªå•: "{query}"
        å‡ºåŠ›å½¢å¼(JSON): {{ "is_ambiguous": bool, "response_text": str, "candidates": [str] }}
        - å˜èªã®ã¿ç­‰ã®å ´åˆã¯ true ã¨ã—ã€èª˜å°å°‹å•ã‚’ response_text ã«è¨˜è¿°ã€‚
        - candidates ã«ã¯æƒ³å®šã•ã‚Œã‚‹å…·ä½“çš„ãªè³ªå•ä¾‹ã‚’åˆ—æŒ™ã€‚
        """
        try:
            model = genai.GenerativeModel("gemini-2.5-flash")
            resp = await model.generate_content_async(prompt, safety_settings=SAFETY_SETTINGS)
            return clean_and_parse_json(resp.text)
        except Exception:
            return {"is_ambiguous": False}

    @staticmethod
    async def rerank(query: str, documents: List[Dict], top_k: int = 5) -> List[Dict]:
        """æ¤œç´¢çµæœã®ãƒªãƒ©ãƒ³ã‚¯å‡¦ç†"""
        if not documents:
            return []
        candidates_text = ""
        for i, doc in enumerate(documents):
            meta = doc.get('metadata', {})
            snippet = doc.get('content', '')[:2000].replace('\n', ' ')
            candidates_text += f"ID:{i} [Source:{meta.get('source', '?')}]\n{snippet}\n\n"

        prompt = f"""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå›ç­”æ ¹æ‹ ã¨ã—ã¦é©åˆ‡ã‹0-10ç‚¹ã§æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚
        è³ªå•: {query}
        å€™è£œ:
        {candidates_text}
        å‡ºåŠ›å½¢å¼(JSON): {{ "ranked_items": [{{ "id": int, "score": float, "reason": str }}] }}
        """
        try:
            model = genai.GenerativeModel("gemini-2.5-flash")
            resp = await model.generate_content_async(prompt, safety_settings=SAFETY_SETTINGS)
            data = clean_and_parse_json(resp.text)
            reranked = []
            for item in data.get("ranked_items", []):
                idx, score = int(item.get("id", -1)), float(item.get("score", 0))
                if 0 <= idx < len(documents) and score >= PARAMS["RERANK_SCORE_THRESHOLD"]:
                    doc = documents[idx]
                    doc['rerank_score'] = score
                    reranked.append(doc)
            reranked.sort(key=lambda x: x['rerank_score'], reverse=True)
            return reranked[:top_k]
        except Exception as e:
            logging.error(f"Rerank Error: {e}")
            return documents[:top_k]

    @staticmethod
    async def filter_diversity(documents: List[Dict], threshold: float = 0.7) -> List[Dict]:
        """MMRé¢¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé‡è¤‡æ’é™¤ï¼‰"""
        loop = asyncio.get_running_loop()
        unique_docs = []
        def _calc_sim(a, b):
            return SequenceMatcher(None, a, b).ratio()

        for doc in documents:
            content = doc.get('content', '')
            is_duplicate = False
            for selected in unique_docs:
                sim = await loop.run_in_executor(executor, _calc_sim, content, selected.get('content', ''))
                if sim > threshold:
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_docs.append(doc)
        return unique_docs

# -----------------------------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³: ãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯
# -----------------------------------------------------------------------------
async def enhanced_chat_logic(request: Request, chat_req: ChatQuery):
    """
    ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ•ãƒ­ãƒ¼
    1. æ„å›³ç†è§£ -> 2. æ¤œç´¢(FAQ/DB) -> 3. ãƒªãƒ©ãƒ³ã‚¯ -> 4. å›ç­”ç”Ÿæˆ
    """
    session_id = get_or_create_session_id(request)
    feedback_id = str(uuid.uuid4())
    user_input = chat_req.query.strip()
    # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰åˆæœŸåŒ–ç”¨
    yield send_sse({'feedback_id': feedback_id})

    try:
        # --- 1. æ„å›³ç†è§£ãƒ•ã‚§ãƒ¼ã‚º ---
        yield send_sse({'status_message': 'ğŸ¤” è³ªå•ã®æ„å›³ã‚’åˆ†æã—ã¦ã„ã¾ã™...'})
        ambiguity = await SearchPipeline.check_ambiguity(user_input)
        if ambiguity.get("is_ambiguous"):
            resp = ambiguity.get("response_text", "ã‚‚ã†å°‘ã—å…·ä½“çš„ã«æ•™ãˆã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ")
            if candidates := ambiguity.get("candidates"):
                resp += "\n\n**ã‚‚ã—ã‹ã—ã¦:**\n" + "\n".join([f"- {c}" for c in candidates])
            yield send_sse({'content': resp, 'show_feedback': True, 'feedback_id': feedback_id})
            return

        # --- 2. æ¤œç´¢ãƒ•ã‚§ãƒ¼ã‚º (FAQ & DB) ---
        yield send_sse({'status_message': 'ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™...'})
        # ã‚¯ã‚¨ãƒªæ‹¡å¼µã¨Embeddingã‚’ä¸¦åˆ—å®Ÿè¡Œ
        task_query = asyncio.create_task(SearchPipeline.optimize_query(user_input, session_id))
        task_embed = asyncio.create_task(
            genai.embed_content_async(model=chat_req.embedding_model, content=user_input, task_type="retrieval_query")
        )

        # A. FAQãƒã‚§ãƒƒã‚¯
        try:
            raw_emb = (await task_embed)["embedding"]
            if qa_hits := core_database.db_client.search_fallback_qa(raw_emb, match_count=1):
                top_qa = qa_hits[0]
                if top_qa.get('similarity', 0) >= PARAMS["QA_SIMILARITY_THRESHOLD"]:
                    task_query.cancel() # DBæ¤œç´¢ä¸è¦
                    resp = format_urls_as_links(f"ã‚ˆãã‚ã‚‹ã”è³ªå•ã«å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n\n---\n{top_qa['content']}")
                    history_manager.add(session_id, "assistant", resp)
                    yield send_sse({'content': resp, 'show_feedback': True, 'feedback_id': feedback_id})
                    return
        except Exception as e:
            log_context(session_id, f"FAQ Search Skip: {e}", "warning")

        # B. DBæ¤œç´¢
        search_query = await task_query
        optimized_emb = (await genai.embed_content_async(
            model=chat_req.embedding_model, content=search_query, task_type="retrieval_query"
        ))["embedding"]

        raw_docs = core_database.db_client.search_documents_hybrid(
            collection_name=chat_req.collection,
            query_text=search_query,
            query_embedding=optimized_emb,
            match_count=30
        )
        # å¤šæ§˜æ€§ãƒ•ã‚£ãƒ«ã‚¿ã¨ãƒªãƒ©ãƒ³ã‚¯
        yield send_sse({'status_message': 'ğŸ§ æ–‡çŒ®ã®é‡è¦åº¦ã‚’AIãŒç²¾æŸ»ä¸­...'})
        unique_docs = await SearchPipeline.filter_diversity(raw_docs)
        relevant_docs = await SearchPipeline.rerank(user_input, unique_docs[:12], top_k=chat_req.top_k)

        # --- 3. å›ç­”ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º ---
        if not relevant_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
        else:
            yield send_sse({'status_message': 'âœï¸ å›ç­”ã‚’åŸ·ç­†ã—ã¦ã„ã¾ã™...'})
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
            context_parts = []
            sources_map = {}
            for idx, doc in enumerate(relevant_docs, 1):
                src = doc.get('metadata', {}).get('source', 'ä¸æ˜')
                sources_map[idx] = src
                context_parts.append(f"<doc id='{idx}' src='{src}'>\n{doc.get('content','')}\n</doc>")
            system_prompt = f"""
            ã‚ãªãŸã¯æœ­å¹Œå­¦é™¢å¤§å­¦ã®å­¦ç”Ÿã‚µãƒãƒ¼ãƒˆAIã§ã™ã€‚
            ä»¥ä¸‹ã®<context>å†…ã®æƒ…å ±**ã®ã¿**ã‚’ä½¿ç”¨ã—ã¦ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

            # å›ç­”ã®ãƒ«ãƒ¼ãƒ«
            1. **æ ¹æ‹ ã®ç´ä»˜ã‘**:
            æ–‡ç« ä¸­ã®é‡è¦ãªäº‹å®Ÿã«ã¯ã€æ–‡æœ«ã« `[1]` ã®ã‚ˆã†ã«**çŸ­ã„ç•ªå·ã®ã¿**ã‚’ä»˜è¨˜ã—ã¦ãã ã•ã„ã€‚
            2. **å½¢å¼**:
            - å­¦ç”Ÿã«å¯„ã‚Šæ·»ã£ãŸã€ä¸å¯§ã§è¦ªã—ã¿ã‚„ã™ã„ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã€‚
            - èª­ã¿ã‚„ã™ã„ã‚ˆã†ã«ç®‡æ¡æ›¸ãã‚„**å¤ªå­—**ã‚’æ´»ç”¨ã™ã‚‹ã€‚
            - æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã‚‹ã€‚
            <context>
            {chr(10).join(context_parts)}
            </context>
            """
            model = genai.GenerativeModel(chat_req.model)
            stream = await model.generate_content_async(
                [system_prompt, f"è³ªå•: {user_input}"], stream=True, safety_settings=SAFETY_SETTINGS
            )
            full_resp = ""
            async for chunk in stream:
                if chunk.text:
                    full_resp += chunk.text
                    yield send_sse({'content': chunk.text})
            # å‚ç…§å…ƒã®è¿½è¨˜
            if "æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" not in full_resp:
                refs_header = "\n\n## å‚ç…§å…ƒ\n"
                unique_refs = []
                seen_sources = set()

                for idx, src in sources_map.items():
                    if src in seen_sources:
                        continue
                    # æ–‡ä¸­ã§å¼•ç”¨ã•ã‚ŒãŸ([n])ã€ã¾ãŸã¯æ¤œç´¢ã‚¹ã‚³ã‚¢ä¸Šä½3ä»¶ã®ã¿ã‚’è¡¨ç¤º
                    if f"[{idx}]" in full_resp or idx <= 3:
                        unique_refs.append(f"* [{idx}] {src}")
                        seen_sources.add(src)

                if unique_refs:
                    refs_text = refs_header + "\n".join(unique_refs)
                    yield send_sse({'content': refs_text})
                    full_resp += refs_text
            history_manager.add(session_id, "assistant", full_resp)

    except Exception as e:
        log_context(session_id, f"Critical Error: {e}", "error")
        yield send_sse({'content': AI_MESSAGES["ERROR"]})
    finally:
        yield send_sse({'show_feedback': True, 'feedback_id': feedback_id})

# -----------------------------------------------------------------------------
# ç®¡ç†è€…ç”¨æ©Ÿèƒ½
# -----------------------------------------------------------------------------
async def analyze_feedback_trends(logs: List[Dict[str, Any]]) -> AsyncGenerator[str, None]:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æï¼ˆç®¡ç†è€…ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ï¼‰"""
    if not logs:
        yield send_sse({'content': 'åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚'})
        return

    summary = "\n".join([f"- è©•ä¾¡:{l.get('rating','-')} | {l.get('comment','-')[:100]}" for l in logs[:50]])
    prompt = f"""
    ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆåˆ©ç”¨ãƒ­ã‚°ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownã§ä½œæˆã—ã¦ãã ã•ã„ã€‚
    ãƒ‡ãƒ¼ã‚¿:
    {summary}
    é …ç›®: 1.ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒˆãƒ¬ãƒ³ãƒ‰, 2.ä½è©•ä¾¡ã®åŸå› , 3.æ”¹å–„æ¡ˆ
    """
    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
        async for chunk in await model.generate_content_async(prompt, stream=True):
            if chunk.text:
                yield send_sse({'content': chunk.text})
    except Exception as e:
        yield send_sse({'content': f'åˆ†æã‚¨ãƒ©ãƒ¼: {e}'})