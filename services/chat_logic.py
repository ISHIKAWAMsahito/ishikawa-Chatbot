# services/chat_logic.py
import logging
import uuid
import asyncio
from typing import List, Dict, Any, AsyncGenerator
from fastapi import Request

# LangSmith ãƒˆãƒ¬ãƒ¼ã‚¹ç”¨
from langsmith import traceable
from langsmith.run_helpers import get_current_run_tree

# ä¾å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from core import database as core_database
from core.constants import PARAMS, AI_MESSAGES
from models.schemas import ChatQuery
from services.llm import LLMService
from services.search import SearchService
from services.storage import StorageService
from services.utils import get_or_create_session_id, send_sse, log_context, ChatHistoryManager
from services import prompts
from services.utils import format_urls_as_links

# DIï¼ˆä¾å­˜æ€§ã®æ³¨å…¥ï¼‰ã®æº–å‚™
llm_service = LLMService()
search_service = SearchService(llm_service)
storage_service = StorageService()
history_manager = ChatHistoryManager(max_length=PARAMS["MAX_HISTORY_LENGTH"])

@traceable(name="Chat_Pipeline_Parent", run_type="chain")
async def enhanced_chat_logic(request: Request, chat_req: ChatQuery):
    """
    ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯
    LangSmith: ã“ã®é–¢æ•°ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã¨ã€é…ä¸‹ã® llm_service ã‚„ search_service ã®å‘¼ã³å‡ºã—ãŒ
    è‡ªå‹•çš„ã«å­ãƒˆãƒ¬ãƒ¼ã‚¹ã¨ã—ã¦è¨˜éŒ²ã•ã‚Œã€ãƒ„ãƒªãƒ¼æ§‹é€ ã«ãªã‚Šã¾ã™ã€‚
    """
    session_id = get_or_create_session_id(request)
    user_input = chat_req.query.strip()
    feedback_id = str(uuid.uuid4())
    
    # LangSmith: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³IDãªã©ï¼‰ã‚’è¿½åŠ 
    run_tree = get_current_run_tree()
    if run_tree:
        run_tree.add_metadata({"session_id": session_id, "user_query": user_input})

    yield send_sse({'feedback_id': feedback_id, 'status_message': 'ğŸ” è³ªå•ã‚’åˆ†æã—ã¦ã„ã¾ã™...'})

    try:
        # 1. ã‚¯ã‚¨ãƒªæ‹¡å¼µ
        expanded_query = await search_service.expand_query(user_input)
        
        # 2. Embeddingç”Ÿæˆ
        query_embedding = await llm_service.get_embedding(
            text=expanded_query, 
            model=chat_req.embedding_model
        )

        # 3. FAQãƒã‚§ãƒƒã‚¯
        if qa_hits := core_database.db_client.search_fallback_qa(query_embedding, match_count=1):
            top_qa = qa_hits[0]
            if top_qa.get('similarity', 0) >= PARAMS["QA_SIMILARITY_THRESHOLD"]:
                resp = format_urls_as_links(f"ã‚ˆãã‚ã‚‹ã”è³ªå•ã«å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n\n---\n{top_qa['content']}")
                history_manager.add(session_id, "assistant", resp)
                yield send_sse({'content': resp, 'show_feedback': True, 'feedback_id': feedback_id})
                return

        yield send_sse({'status_message': 'ğŸ“š è³‡æ–™ã‚’åºƒãé›†ã‚ã¦ã„ã¾ã™...'})

        # 4. DBæ¤œç´¢ (Hybrid)
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œè‡ªä½“ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã—ãŸã„å ´åˆã¯ã€core/database.py ã« @traceable ã‚’ã¤ã‘ã‚‹ã®ãŒãƒ™ã‚¹ãƒˆã§ã™ãŒã€
        # ã“ã“ã§ã¯æ¤œç´¢çµæœã®ä»¶æ•°ãªã©ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«æ®‹ã™ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚
        raw_docs = core_database.db_client.search_documents_hybrid(
            collection_name=chat_req.collection,
            query_text=expanded_query,
            query_embedding=query_embedding,
            match_count=50
        )

        if not raw_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        yield send_sse({'status_message': 'ğŸ§ AIãŒæ–‡çŒ®ã‚’ç²¾èª­ãƒ»é¸åˆ¥ä¸­...'})

        # 5. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° & ãƒªãƒ©ãƒ³ã‚¯ & ä¸¦ã¹æ›¿ãˆ
        unique_docs = search_service.filter_diversity(raw_docs)
        rerank_input = unique_docs[:PARAMS["RERANK_TOP_K_INPUT"]]
        
        # ãƒªãƒ©ãƒ³ã‚¯å‡¦ç†ï¼ˆsearch_serviceå´ã§æ—¢ã«ãƒˆãƒ¬ãƒ¼ã‚¹è¨­å®šæ¸ˆã¿ï¼‰
        relevant_docs = await search_service.rerank(
            query=user_input, 
            documents=rerank_input, 
            top_k=chat_req.top_k
        )

        if not relevant_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        # Lost in the Middle å¯¾ç­–
        final_docs = search_service.reorder_litm(relevant_docs)

        yield send_sse({'status_message': 'âœï¸ å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...'})

        # 6. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        context_parts = []
        sources_map = {}
        for idx, doc in enumerate(final_docs, 1):
            meta = doc.get('metadata', {})
            src_display = meta.get('source', 'ä¸æ˜')
            src_storage = meta.get('image_path', src_display)
            
            sources_map[idx] = {'display': src_display, 'storage': src_storage}
            context_parts.append(f"<doc id='{idx}' src='{src_display}'>\n{doc.get('content','')}\n</doc>")
        
        context_str = "\n".join(context_parts)
        
        # 7. å›ç­”ç”Ÿæˆ
        full_system_prompt = f"{prompts.SYSTEM_GENERATION}\n<context>\n{context_str}\n</context>"
        
        # LLMå‘¼ã³å‡ºã—ï¼ˆllm_serviceå´ã§ãƒˆãƒ¬ãƒ¼ã‚¹è¨­å®šæ¸ˆã¿ï¼‰
        stream = await llm_service.generate_stream(
            prompt=f"è³ªå•: {user_input}",
            system_prompt=full_system_prompt
        )
        
        full_resp = ""
        async for chunk in stream:
            if chunk.text:
                full_resp += chunk.text
                yield send_sse({'content': chunk.text})

        if not full_resp:
             yield send_sse({'content': AI_MESSAGES["BLOCKED"]})
             history_manager.add(session_id, "assistant", "[[BLOCKED]]")
             return

        # 8. å‚ç…§ãƒªãƒ³ã‚¯ç”Ÿæˆ
        if "æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" not in full_resp:
            yield send_sse({'status_message': 'ğŸ”— å‚ç…§ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆä¸­...'})
            refs_text = await storage_service.build_references_async(full_resp, sources_map)
            if refs_text:
                yield send_sse({'content': refs_text})
                full_resp += refs_text
        
        history_manager.add(session_id, "assistant", full_resp)

    except Exception as e:
        log_context(session_id, f"Critical Pipeline Error: {e}", "error")
        # ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã«æ®‹ã™
        if run_tree:
            run_tree.end(error=str(e))
            
        error_str = str(e)
        if "429" in error_str or "Quota" in error_str:
            msg = AI_MESSAGES["RATE_LIMIT"]
        elif "finish_reason" in error_str:
            msg = AI_MESSAGES["BLOCKED"]
        else:
            msg = AI_MESSAGES["SYSTEM_ERROR"]
        yield send_sse({'content': msg})
        
    finally:
        yield send_sse({'show_feedback': True, 'feedback_id': feedback_id})

@traceable(name="Feedback_Analysis_Job", run_type="chain")
async def analyze_feedback_trends(logs: List[Dict[str, Any]]) -> AsyncGenerator[str, None]:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æç”¨"""
    if not logs:
        yield send_sse({'content': 'åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚'})
        return
    
    summary = "\n".join([f"- è©•ä¾¡:{l.get('rating','-')} | {l.get('comment','-')[:100]}" for l in logs[:50]])
    prompt = f"""
    ä»¥ä¸‹ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆåˆ©ç”¨ãƒ­ã‚°ã‚’åˆ†æã—ã€Markdownã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿
    {summary}
    # å‡ºåŠ›é …ç›®
    1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¸»ãªé–¢å¿ƒäº‹ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
    2. ä½è©•ä¾¡ã®åŸå› ã¨æ”¹å–„ç­–
    3. æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³
    """
    try:
        stream = await llm_service.generate_stream(prompt)
        async for chunk in stream:
            if chunk.text:
                yield send_sse({'content': chunk.text})
    except Exception as e:
        yield send_sse({'content': f'åˆ†æã‚¨ãƒ©ãƒ¼: {e}'})