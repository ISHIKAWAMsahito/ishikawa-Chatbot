import logging
import uuid
import json
import asyncio
import re
import os
from typing import List, Dict, Any, AsyncGenerator, Optional
from concurrent.futures import ThreadPoolExecutor
from difflib import SequenceMatcher
import typing_extensions as typing

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold, GenerationConfig
from fastapi import Request
from dotenv import load_dotenv

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from core.config import GEMINI_API_KEY
from core import database as core_database
from models.schemas import ChatQuery
from services.utils import format_urls_as_links

# -----------------------------------------------------------------------------
# 1. è¨­å®š & å®šæ•°å®šç¾©
# -----------------------------------------------------------------------------
load_dotenv()
genai.configure(api_key=GEMINI_API_KEY)

# ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
USE_MODEL = "gemini-2.5-flash"

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
PARAMS = {
    "QA_SIMILARITY_THRESHOLD": 0.95, # FAQã®å³ç­”ãƒ©ã‚¤ãƒ³
    "RERANK_SCORE_THRESHOLD": 6.0,   # ãƒªãƒ©ãƒ³ã‚¯è¶³åˆ‡ã‚Šãƒ©ã‚¤ãƒ³(0-10)
    "MAX_HISTORY_LENGTH": 20,
}

# ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®š
SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
}

# â˜…ä¿®æ­£ç‚¹1: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ­£ç¢ºãªå®šç¾©
AI_MESSAGES = {
    "NOT_FOUND": (
        "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã”è³ªå•ã«é–¢é€£ã™ã‚‹ç¢ºå®Ÿãªæƒ…å ±ãŒè³‡æ–™å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        "å¤§å­¦çª“å£ã¸ç›´æ¥ãŠå•ã„åˆã‚ã›ã„ãŸã ãã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
    ),
    "RATE_LIMIT": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ç¾åœ¨ã‚¢ã‚¯ã‚»ã‚¹ãŒé›†ä¸­ã—ã¦ã„ã¾ã™ã€‚1åˆ†ã»ã©å¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
    "SYSTEM_ERROR": "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
    "BLOCKED": "ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãŒã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«æŠµè§¦ã—ãŸãŸã‚ã€è¡¨ç¤ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚è¨€ã„å›ã—ã‚’å¤‰ãˆã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
}

# ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ï¼ˆCPUãƒã‚¦ãƒ³ãƒ‰ãªå‡¦ç†ç”¨ï¼‰
executor = ThreadPoolExecutor(max_workers=4)

# -----------------------------------------------------------------------------
# 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ & ã‚¹ã‚­ãƒ¼ãƒå®šç¾© (Structured Outputsç”¨)
# -----------------------------------------------------------------------------

# ãƒªãƒ©ãƒ³ã‚¯å‡ºåŠ›ç”¨ã®å‹å®šç¾©
class RankedItem(typing.TypedDict):
    id: int
    score: float
    reason: str

class RerankResponse(typing.TypedDict):
    ranked_items: list[RankedItem]

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
PROMPT_RERANK = """
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå›ç­”æ ¹æ‹ ã¨ã—ã¦é©åˆ‡ã‹0-10ç‚¹ã§æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚
è³ªå•: {query}
å€™è£œ:
{candidates_text}
"""

PROMPT_SYSTEM_GENERATION = """
ã‚ãªãŸã¯æœ­å¹Œå­¦é™¢å¤§å­¦ã®å­¦ç”Ÿã‚µãƒãƒ¼ãƒˆAIã§ã™ã€‚
ä»¥ä¸‹ã®<context>å†…ã®æƒ…å ±**ã®ã¿**ã‚’ä½¿ç”¨ã—ã¦ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

# å›ç­”ã®ãƒ«ãƒ¼ãƒ«
1. **æ ¹æ‹ ã®ç´ä»˜ã‘**:
   æ–‡ç« ä¸­ã®é‡è¦ãªäº‹å®Ÿã«ã¯ã€æ–‡æœ«ã« `[1]` ã®ã‚ˆã†ã«**çŸ­ã„ç•ªå·ã®ã¿**ã‚’ä»˜è¨˜ã—ã¦ãã ã•ã„ã€‚
2. **å½¢å¼**:
   - å­¦ç”Ÿã«å¯„ã‚Šæ·»ã£ãŸã€ä¸å¯§ã§è¦ªã—ã¿ã‚„ã™ã„ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã€‚
   - èª­ã¿ã‚„ã™ã„ã‚ˆã†ã«ç®‡æ¡æ›¸ãã‚„**å¤ªå­—**ã‚’æ´»ç”¨ã™ã‚‹ã€‚
   - æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã‚‹ã€‚
"""

# -----------------------------------------------------------------------------
# 3. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# -----------------------------------------------------------------------------
def get_or_create_session_id(request: Request) -> str:
    session_id = request.session.get('chat_session_id')
    if not session_id:
        session_id = str(uuid.uuid4())
        request.session['chat_session_id'] = session_id
    return session_id

def log_context(session_id: str, message: str, level: str = "info"):
    msg = f"[Session: {session_id}] {message}"
    getattr(logging, level, logging.info)(msg)

def send_sse(data: Dict[str, Any]) -> str:
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

async def api_request_with_retry(func, *args, **kwargs):
    """APIåˆ¶é™(429)å¯¾ç­–: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰å¾…æ©Ÿæ™‚é–“ã‚’è§£æã—ã¦ãƒªãƒˆãƒ©ã‚¤"""
    max_retries = 3
    default_delay = 5
    for attempt in range(max_retries):
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            error_str = str(e)
            if "429" in error_str or "Quota" in error_str:
                if attempt == max_retries - 1:
                    logging.error(f"API Quota Exceeded after {max_retries} retries.")
                    raise e
                
                wait_time = default_delay
                match = re.search(r"retry in (\d+\.?\d*)s", error_str)
                if match:
                    wait_time = float(match.group(1)) + 1.0
                else:
                    wait_time = default_delay * (2 ** attempt)

                logging.warning(f"Rate limit hit. Waiting {wait_time:.1f}s. Retrying...")
                await asyncio.sleep(wait_time)
            else:
                raise e

class ChatHistoryManager:
    def __init__(self):
        self._histories: Dict[str, List[Dict[str, str]]] = {}

    def add(self, session_id: str, role: str, content: str):
        if session_id not in self._histories:
            self._histories[session_id] = []
        self._histories[session_id].append({"role": role, "content": content})
        if len(self._histories[session_id]) > PARAMS["MAX_HISTORY_LENGTH"]:
            self._histories[session_id] = self._histories[session_id][-PARAMS["MAX_HISTORY_LENGTH"]:]

history_manager = ChatHistoryManager()

# -----------------------------------------------------------------------------
# 4. ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯: æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# -----------------------------------------------------------------------------
class SearchPipeline:
    @staticmethod
    async def optimize_query(user_query: str, session_id: str) -> str:
        """HyDE + Query Expansion (å¿…è¦ã«å¿œã˜ã¦æœ‰åŠ¹åŒ–)"""
        # â€»APIç¯€ç´„ã®ãŸã‚ã€ç¾åœ¨ã¯ä½¿ç”¨ã—ã¦ã„ãªã„ãŒæ©Ÿèƒ½ã¨ã—ã¦æ®‹ã™
        prompt = f"""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«åŸºã¥ã„ã¦ã€å¤§å­¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«æœ€é©ãªã€Œæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        å°‚é–€ç”¨èªã¸ã®è¨€ã„æ›ãˆã‚’å«ã‚ã€å‡ºåŠ›ã¯æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã«ã—ã¦ãã ã•ã„ã€‚
        è³ªå•: "{user_query}"
        """
        try:
            model = genai.GenerativeModel(USE_MODEL)
            resp = await api_request_with_retry(
                model.generate_content_async, prompt, safety_settings=SAFETY_SETTINGS
            )
            return resp.text.strip()
        except Exception:
            return user_query

    @staticmethod
    async def rerank(query: str, documents: List[Dict], top_k: int = 5) -> List[Dict]:
        """Gemini Structured Outputs ã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿãƒ»ç¢ºå®Ÿãªãƒªãƒ©ãƒ³ã‚¯"""
        if not documents:
            return []
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ (ãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ã®ãŸã‚ã€å…ˆé ­1000æ–‡å­—ç¨‹åº¦ã«åˆ¶é™)
        candidates_text = ""
        for i, doc in enumerate(documents):
            meta = doc.get('metadata', {})
            snippet = doc.get('content', '')[:1000].replace('\n', ' ')
            candidates_text += f"ID:{i} [Source:{meta.get('source', '?')}]\n{snippet}\n\n"

        formatted_prompt = PROMPT_RERANK.format(query=query, candidates_text=candidates_text)

        try:
            model = genai.GenerativeModel(USE_MODEL)
            # â˜…æ”¹å–„: response_schemaã§å‹å®‰å…¨ã«JSONã‚’å–å¾—
            resp = await api_request_with_retry(
                model.generate_content_async,
                formatted_prompt,
                generation_config=GenerationConfig(
                    response_mime_type="application/json",
                    response_schema=RerankResponse
                ),
                safety_settings=SAFETY_SETTINGS
            )
            
            # JSONãƒ‘ãƒ¼ã‚¹å‡¦ç†
            data = json.loads(resp.text)
            
            reranked = []
            for item in data.get("ranked_items", []):
                idx = item.get("id")
                score = item.get("score")
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å¦¥å½“æ€§ã¨ã‚¹ã‚³ã‚¢ãƒã‚§ãƒƒã‚¯
                if idx is not None and 0 <= idx < len(documents):
                    if score >= PARAMS["RERANK_SCORE_THRESHOLD"]:
                        doc = documents[idx]
                        doc['rerank_score'] = score
                        reranked.append(doc)
            
            reranked.sort(key=lambda x: x['rerank_score'], reverse=True)
            return reranked[:top_k]

        except Exception as e:
            logging.error(f"Rerank Error: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®é †åºã®ä¸Šä½ã‚’ãã®ã¾ã¾è¿”ã™ï¼ˆãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ï¼‰
            return documents[:top_k]

    @staticmethod
    async def filter_diversity(documents: List[Dict], threshold: float = 0.7) -> List[Dict]:
        """MMRé¢¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé‡è¤‡æ’é™¤ï¼‰"""
        loop = asyncio.get_running_loop()
        unique_docs = []
        
        def _calc_sim(a, b):
            return SequenceMatcher(None, a, b).ratio()

        for doc in documents:
            content = doc.get('content', '')
            is_duplicate = False
            for selected in unique_docs:
                sim = await loop.run_in_executor(executor, _calc_sim, content, selected.get('content', ''))
                if sim > threshold:
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_docs.append(doc)
        return unique_docs

def _build_references(response_text: str, sources_map: Dict[int, str]) -> str:
    """å›ç­”ç”Ÿæˆå¾Œã«å‚ç…§å…ƒãƒªãƒ³ã‚¯ã‚’ä½œæˆã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
    unique_refs = []
    seen_sources = set()
    
    for idx, src in sources_map.items():
        if src in seen_sources: continue
        # ãƒ†ã‚­ã‚¹ãƒˆå†…ã§å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ã€ã¾ãŸã¯ä¸Šä½3ä»¶ãªã‚‰è¡¨ç¤º
        if f"[{idx}]" in response_text or idx <= 3:
            unique_refs.append(f"* [{idx}] {src}")
            seen_sources.add(src)
            
    if unique_refs:
        return "\n\n## å‚ç…§å…ƒ\n" + "\n".join(unique_refs)
    return ""

# -----------------------------------------------------------------------------
# 5. ãƒ¡ã‚¤ãƒ³: ãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯
# -----------------------------------------------------------------------------
async def enhanced_chat_logic(request: Request, chat_req: ChatQuery):
    session_id = get_or_create_session_id(request)
    feedback_id = str(uuid.uuid4())
    user_input = chat_req.query.strip()
    
    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¸åˆæœŸãƒ¬ã‚¹ãƒãƒ³ã‚¹
    yield send_sse({'feedback_id': feedback_id, 'status_message': 'ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™...'})

    try:
        # Step 1: Embeddingã®éåŒæœŸå®Ÿè¡Œ
        embedding_task = asyncio.create_task(
            genai.embed_content_async(
                model=chat_req.embedding_model,
                content=user_input,
                task_type="retrieval_query"
            )
        )

        # Step 2: Embeddingçµæœã®å–å¾—
        try:
            raw_emb_result = await embedding_task
            query_embedding = raw_emb_result["embedding"]
        except Exception as e:
            log_context(session_id, f"Embedding Failed: {e}", "error")
            # â˜…ä¿®æ­£ç‚¹2: Embeddingå¤±æ•—ã¯ã€Œã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ã€ã¨ã—ã¦é€šçŸ¥ï¼ˆã‚¢ã‚¯ã‚»ã‚¹é›†ä¸­ã§ã¯ãªã„ï¼‰
            yield send_sse({'content': AI_MESSAGES["SYSTEM_ERROR"]})
            return

        # Step 3: FAQ (QA Database) ãƒã‚§ãƒƒã‚¯
        # é«˜ã‚¹ã‚³ã‚¢ã§ãƒ’ãƒƒãƒˆã™ã‚Œã°å³return
        if qa_hits := core_database.db_client.search_fallback_qa(query_embedding, match_count=1):
            top_qa = qa_hits[0]
            if top_qa.get('similarity', 0) >= PARAMS["QA_SIMILARITY_THRESHOLD"]:
                resp = format_urls_as_links(f"ã‚ˆãã‚ã‚‹ã”è³ªå•ã«å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n\n---\n{top_qa['content']}")
                history_manager.add(session_id, "assistant", resp)
                yield send_sse({'content': resp, 'show_feedback': True, 'feedback_id': feedback_id})
                return

        # Step 4: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ (Hybrid)
        # å‡¦ç†ç¯€ç´„ã®ãŸã‚ã‚¯ã‚¨ãƒªæ‹¡å¼µã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã€ç”Ÿã®å…¥åŠ›ã‚’ä½¿ç”¨
        raw_docs = core_database.db_client.search_documents_hybrid(
            collection_name=chat_req.collection,
            query_text=user_input, 
            query_embedding=query_embedding,
            match_count=30
        )

        if not raw_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        yield send_sse({'status_message': 'ğŸ§ AIãŒæ–‡çŒ®ã‚’èª­ã‚“ã§é¸å®šä¸­...'})
        
        # Step 5: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° & ãƒªãƒ©ãƒ³ã‚¯
        unique_docs = await SearchPipeline.filter_diversity(raw_docs)
        
        # Geminiã«ã‚ˆã‚‹ãƒªãƒ©ãƒ³ã‚¯å®Ÿè¡Œ
        relevant_docs = await SearchPipeline.rerank(user_input, unique_docs[:15], top_k=chat_req.top_k)

        if not relevant_docs:
            yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
            return

        # Step 6: å›ç­”ç”Ÿæˆ
        yield send_sse({'status_message': 'âœï¸ å›ç­”ã‚’åŸ·ç­†ã—ã¦ã„ã¾ã™...'})
        
        context_parts = []
        sources_map = {} # {doc_id: source_name}
        
        for idx, doc in enumerate(relevant_docs, 1):
            src = doc.get('metadata', {}).get('source', 'ä¸æ˜')
            sources_map[idx] = src
            context_parts.append(f"<doc id='{idx}' src='{src}'>\n{doc.get('content','')}\n</doc>")
        
        context_str = "\n".join(context_parts)
        full_system_prompt = f"{PROMPT_SYSTEM_GENERATION}\n<context>\n{context_str}\n</context>"

        model = genai.GenerativeModel(USE_MODEL)
        stream = await api_request_with_retry(
            model.generate_content_async,
            [full_system_prompt, f"è³ªå•: {user_input}"],
            stream=True,
            safety_settings=SAFETY_SETTINGS
        )
        
        full_resp = ""
        async for chunk in stream:
            if chunk.text:
                full_resp += chunk.text
                yield send_sse({'content': chunk.text})
        
        # â˜…ä¿®æ­£ç‚¹3: å›ç­”ãŒç©ºï¼ˆã‚»ãƒ¼ãƒ•ãƒ†ã‚£ç­‰ã§ãƒ–ãƒ­ãƒƒã‚¯ï¼‰ã®å ´åˆã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¿½åŠ 
        if not full_resp:
             yield send_sse({'content': AI_MESSAGES["BLOCKED"]})
             history_manager.add(session_id, "assistant", "[[BLOCKED]]")
             return

        # Step 7: å‚ç…§å…ƒãƒªãƒ³ã‚¯ã®è¿½è¨˜
        if "æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" not in full_resp:
            refs_text = _build_references(full_resp, sources_map)
            if refs_text:
                yield send_sse({'content': refs_text})
                full_resp += refs_text
        
        history_manager.add(session_id, "assistant", full_resp)

    except Exception as e:
        log_context(session_id, f"Critical Pipeline Error: {e}", "error")
        
        # â˜…ä¿®æ­£ç‚¹4: ã‚¨ãƒ©ãƒ¼ç¨®åˆ¥ã«ã‚ˆã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ­£ç¢ºãªå‡ºã—åˆ†ã‘
        error_str = str(e)
        if "429" in error_str or "Quota" in error_str:
            msg = AI_MESSAGES["RATE_LIMIT"]
        elif "finish_reason" in error_str: # Geminiå›ºæœ‰ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ãªã©
            msg = AI_MESSAGES["BLOCKED"]
        else:
            msg = AI_MESSAGES["SYSTEM_ERROR"]
            
        yield send_sse({'content': msg})
        
    finally:
        # ã©ã®ã‚ˆã†ãªçµ‚äº†ãƒ•ãƒ­ãƒ¼ã§ã‚‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒœã‚¿ãƒ³ã¯è¡¨ç¤ºã™ã‚‹
        yield send_sse({'show_feedback': True, 'feedback_id': feedback_id})

# -----------------------------------------------------------------------------
# 6. åˆ†ææ©Ÿèƒ½ (ç®¡ç†è€…ç”¨)
# -----------------------------------------------------------------------------
async def analyze_feedback_trends(logs: List[Dict[str, Any]]) -> AsyncGenerator[str, None]:
    if not logs:
        yield send_sse({'content': 'åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚'})
        return
    
    # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã®è¦ç´„
    summary = "\n".join([f"- è©•ä¾¡:{l.get('rating','-')} | {l.get('comment','-')[:100]}" for l in logs[:50]])
    prompt = f"""
    ä»¥ä¸‹ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆåˆ©ç”¨ãƒ­ã‚°ã‚’åˆ†æã—ã€Markdownã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    
    # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿
    {summary}
    
    # å‡ºåŠ›é …ç›®
    1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¸»ãªé–¢å¿ƒäº‹ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
    2. ä½è©•ä¾¡ã®åŸå› ã¨æ”¹å–„ç­–
    3. æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³
    """
    
    try:
        model = genai.GenerativeModel(USE_MODEL)
        stream = await api_request_with_retry(model.generate_content_async, prompt, stream=True)
        async for chunk in stream:
            if chunk.text:
                yield send_sse({'content': chunk.text})
    except Exception as e:
        yield send_sse({'content': f'åˆ†æã‚¨ãƒ©ãƒ¼: {e}'})