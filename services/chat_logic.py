import logging
import uuid
import asyncio
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, AsyncGenerator, Optional
from fastapi import Request

from langsmith import traceable
from langsmith.run_helpers import get_current_run_tree

from core.constants import PARAMS, AI_MESSAGES
from models.schemas import ChatQuery, ChatLogCreate
from services.llm import LLMService
from services.search import SearchService
from services.storage import StorageService
from services.chat_log import ChatLogService
from services.utils import (
    get_or_create_session_id,
    send_sse,
    log_context,
    ChatHistoryManager,
    format_references,
)
from services import prompts

llm_service = LLMService()
search_service = SearchService(llm_service)
storage_service = StorageService()
history_manager = ChatHistoryManager(max_length=PARAMS["MAX_HISTORY_LENGTH"])


@traceable(name="Chat_Pipeline_Parent", run_type="chain")
async def enhanced_chat_logic(
    request: Request, chat_req: ChatQuery
) -> AsyncGenerator[str, None]:
    """
    RAGãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯ï¼ˆFAQå„ªå…ˆãƒ»ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·å¯¾å¿œç‰ˆï¼‰
    - search_mode ã‚’ ChatQuery ã‹ã‚‰å—ã‘å–ã‚Šæ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«æ¸¡ã™
    - è‡ªå‹•ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã¯å‰Šé™¤ï¼ˆæ¬¡å…ƒæ•°ä¸ä¸€è‡´ã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
    """
    session_id = get_or_create_session_id(request)
    user_input = chat_req.question

    collection_name = getattr(chat_req, "collection", "student-knowledge-base")
    top_k = getattr(chat_req, "top_k", 5)
    embedding_model = getattr(chat_req, "embedding_model", "models/gemini-embedding-001")
    # â˜… search_mode ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: hybridï¼‰
    search_mode = getattr(chat_req, "search_mode", "hybrid")

    run_tree = get_current_run_tree()

    # ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·: ã‚¯ã‚¨ãƒªå†…å®¹ã‚’ãƒ­ã‚°ã«å‡ºã•ãªã„
    log_context(session_id, "Start processing query")

    JST = timezone(timedelta(hours=9), "JST")
    now = datetime.now(JST)
    current_date_str = now.strftime("%Yå¹´%mæœˆ%dæ—¥")

    search_results = []
    is_faq_match = False
    ai_response_full = ""

    try:
        history_manager.add(session_id, "user", user_input)

        yield send_sse({"status_message": "ğŸ” è³ªå•ã‚’åˆ†æã—ã¦ã„ã¾ã™..."})

        # 1. ã‚¯ã‚¨ãƒªæ‹¡å¼µ
        expanded_query = await search_service.expand_query(user_input)

        # 2. æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆsearch_mode ã‚’æ¸¡ã™ï¼‰
        search_result_obj = await search_service.search(
            query=expanded_query,
            session_id=session_id,
            collection_name=collection_name,
            top_k=top_k,
            embedding_model=embedding_model,
            search_mode=search_mode,
        )
        search_results = search_result_obj.get("documents", [])
        is_faq_match = search_result_obj.get("is_faq_match", False)

        # ãƒ’ãƒƒãƒˆã—ãªã‹ã£ãŸå ´åˆã€å…ƒã‚¯ã‚¨ãƒªã§å†æ¤œç´¢
        if not search_results and expanded_query != user_input:
            search_result_obj = await search_service.search(
                query=user_input,
                session_id=session_id,
                collection_name=collection_name,
                top_k=top_k,
                embedding_model=embedding_model,
                search_mode=search_mode,
            )
            search_results = search_result_obj.get("documents", [])
            is_faq_match = search_result_obj.get("is_faq_match", False)

        if not search_results:
            not_found_msg = AI_MESSAGES.get(
                "NOT_FOUND", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚é–¢é€£æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            )
            yield send_sse({"content": not_found_msg})
            history_manager.add(session_id, "assistant", not_found_msg)

            log_entry = ChatLogCreate(
                session_id=session_id,
                user_query=user_input,
                ai_response=not_found_msg,
                metadata={
                    "collection": collection_name,
                    "result": "not_found",
                    "top_k": top_k,
                    "search_mode": search_mode,
                },
            )
            # â˜… éåŒæœŸã‚¿ã‚¹ã‚¯ã¨ã—ã¦ä¿å­˜ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¯è¡Œã‚ãªã„ï¼‰
            asyncio.create_task(_save_log_only(log_entry))

            yield send_sse({"done": True, "feedback_id": str(uuid.uuid4())})
            return

        yield send_sse({"status_message": "âœï¸ å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™..."})

        # 3. å›ç­”ç”Ÿæˆ
        chat_history = history_manager.get_history(session_id)

        context_parts = []
        for idx, doc in enumerate(search_results, 1):
            doc_content = doc.get("content", "")
            context_parts.append(f"<doc id='{idx}'>{doc_content}</doc>")
        context_str = "\n".join(context_parts)

        system_prompt_base = prompts.SYSTEM_GENERATION
        if is_faq_match:
            system_prompt_base += (
                "\n\n**é‡è¦: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å®Œå…¨ã«åˆè‡´ã™ã‚‹FAQè³‡æ–™ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚"
                "<doc id='1'>ã®å†…å®¹ã‚’æœ€å„ªå…ˆã—ã€ãã®å›ç­”ã‚’æ­£ç¢ºã«ä¼ãˆã¦ãã ã•ã„ã€‚**"
            )

        try:
            full_system_prompt = system_prompt_base.format(
                context_text=context_str, current_date=current_date_str
            )
        except Exception:
            full_system_prompt = f"ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n{context_str}"

        async for chunk in llm_service.generate_response_stream(
            query=user_input,
            context_docs=search_results,
            history=chat_history,
            system_prompt=full_system_prompt,
        ):
            text_chunk = chunk if isinstance(chunk, str) else chunk.get("content", "")
            ai_response_full += text_chunk
            yield send_sse({"content": text_chunk})

        # 4. å‚ç…§å…ƒãƒªã‚¹ãƒˆ
        references_text = format_references(search_results)
        if references_text:
            yield send_sse({"content": references_text})
            ai_response_full += "\n" + references_text

        # 5. å±¥æ­´ä¿å­˜
        history_manager.add(session_id, "assistant", ai_response_full)

        # 6. ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ä¿å­˜ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ãªã—ï¼‰
        log_entry = ChatLogCreate(
            session_id=session_id,
            user_query=user_input,
            ai_response=ai_response_full,
            metadata={
                "collection": collection_name,
                "top_k": top_k,
                "is_faq_match": is_faq_match,
                "result": "success",
                "doc_count": len(search_results),
                "search_mode": search_mode,
            },
        )
        asyncio.create_task(_save_log_only(log_entry))

        feedback_id = str(uuid.uuid4())
        yield send_sse({"done": True, "feedback_id": feedback_id})

    except Exception as e:
        log_context(session_id, f"Pipeline Error: {type(e).__name__}", "error", exc_info=True)
        if run_tree:
            run_tree.end(error=str(e))

        error_str = str(e)
        msg = AI_MESSAGES.get("SYSTEM_ERROR", "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
        yield send_sse({"content": f"\n\n{msg}"})

    finally:
        log_context(session_id, "Response generation finished.")


async def _save_log_only(log_entry: ChatLogCreate) -> None:
    """
    ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’DBã«ä¿å­˜ã™ã‚‹ã ã‘ã®ã‚¿ã‚¹ã‚¯ã€‚
    â˜… ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¯è¡Œã‚ãªã„ï¼ˆchat_logs.embeddingã®æ¬¡å…ƒæ•°ä¸ä¸€è‡´ã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
    ã€€ã€€ç®¡ç†è€…ãŒ stats.html ã®ã€Œä¸€æ‹¬ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã€ã‹ã‚‰æ‰‹å‹•å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã€‚
    """
    try:
        data = log_entry.model_dump(mode="json")
        from core.database import db_client
        response = db_client.client.table("chat_logs").insert(data).execute()
        if response.data:
            log_id = response.data[0].get("id")
            logging.getLogger(__name__).info(f"Chat log saved. ID: {log_id}")
        else:
            logging.getLogger(__name__).warning("Chat log saved but no data returned.")
    except Exception as e:
        logging.getLogger(__name__).error(f"Failed to save chat log: {e}", exc_info=True)


@traceable(name="Feedback_Analysis_Job", run_type="chain")
async def analyze_feedback_trends(
    logs: List[Dict[str, Any]]
) -> AsyncGenerator[str, None]:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æç”¨ãƒ­ã‚¸ãƒƒã‚¯"""
    if not logs:
        yield send_sse({"content": "åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"})
        return

    summary = "\n".join(
        [
            f"- è©•ä¾¡:{l.get('rating','-')} | {l.get('comment','-')[:100]}"
            for l in logs[:50]
        ]
    )
    prompt = prompts.FEEDBACK_ANALYSIS.format(summary=summary)

    try:
        stream = await llm_service.generate_stream(prompt)
        async for chunk in stream:
            text = chunk.text if hasattr(chunk, "text") else str(chunk)
            if text:
                yield send_sse({"content": text})
    except Exception as e:
        logging.error(f"Feedback analysis error: {e}", exc_info=True)
        yield send_sse({"content": f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}"})