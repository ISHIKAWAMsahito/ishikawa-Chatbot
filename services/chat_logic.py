# services/chat_logic.py
import logging
import uuid
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, AsyncGenerator
from fastapi import Request

# LangSmith ãƒˆãƒ¬ãƒ¼ã‚¹ç”¨
from langsmith import traceable
from langsmith.run_helpers import get_current_run_tree

# ä¾å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from core import database as core_database
from core.constants import PARAMS, AI_MESSAGES
from models.schemas import ChatQuery
from services.llm import LLMService
from services.search import SearchService
from services.storage import StorageService
# utilsã‹ã‚‰æ–°ã—ã„é–¢æ•° format_references ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from services.utils import (
    get_or_create_session_id, 
    send_sse, 
    log_context, 
    ChatHistoryManager, 
    format_urls_as_links,
    format_references 
)
from services import prompts

# DIï¼ˆä¾å­˜æ€§ã®æ³¨å…¥ï¼‰ã®æº–å‚™
llm_service = LLMService()
search_service = SearchService(llm_service)
storage_service = StorageService()
history_manager = ChatHistoryManager(max_length=PARAMS["MAX_HISTORY_LENGTH"])

@traceable(name="Chat_Pipeline_Parent", run_type="chain")
async def enhanced_chat_logic(request: Request, chat_req: ChatQuery) -> AsyncGenerator[str, None]:
    """
    RAGãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯
    """
    session_id = get_or_create_session_id(request)
    user_input = chat_req.question # models.schemas.ChatQueryã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã«åˆã‚ã›ã¦ãã ã•ã„(question or query)
    
    # LangSmithç”¨ã®RunTreeå–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç”¨ï¼‰
    run_tree = get_current_run_tree()

    log_context(session_id, f"Start processing query: {user_input}")

    # æ—¥æ™‚å–å¾—ï¼ˆJSTï¼‰
    JST = timezone(timedelta(hours=9), 'JST')
    now = datetime.now(JST)
    current_date_str = now.strftime("%Yå¹´%mæœˆ%dæ—¥")

    # æ¤œç´¢çµæœã‚’ä¿æŒã™ã‚‹å¤‰æ•°
    search_results = []
    
    try:
        # 1. å±¥æ­´ã®è¿½åŠ 
        history_manager.add(session_id, "user", user_input)
        
        yield send_sse({'status_message': 'ğŸ” è³ªå•ã‚’åˆ†æã—ã¦ã„ã¾ã™...'})

        # 2. æ¤œç´¢ (Search Service)
        # SearchServiceã®å®Ÿè£…ã«åˆã‚ã›ã¦å‘¼ã³å‡ºã—ã‚’èª¿æ•´ã—ã¦ãã ã•ã„
        # ã“ã“ã§ã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’è¡Œã„ã€documentsãƒªã‚¹ãƒˆãŒè¿”ã£ã¦ãã‚‹ã¨ä»®å®šã—ã¾ã™
        
        # ã¾ãšã‚¯ã‚¨ãƒªæ‹¡å¼µ
        expanded_query = await search_service.expand_query(user_input)
        
        # æ¤œç´¢å®Ÿè¡Œï¼ˆå†…éƒ¨ã§EmbeddingåŒ–ã€DBæ¤œç´¢ã€ãƒªãƒ©ãƒ³ã‚¯ãªã©ã‚’è¡Œã†æƒ³å®šï¼‰
        # â€» search_service.search ãƒ¡ã‚½ãƒƒãƒ‰ãŒå­˜åœ¨ã—ã€å¿…è¦ãªå‡¦ç†ã‚’ãƒ©ãƒƒãƒ—ã—ã¦ã„ã‚‹å ´åˆ
        search_result_obj = await search_service.search(
            query=user_input,
            session_id=session_id
        )
        search_results = search_result_obj.get("documents", [])
        
        # ã‚‚ã— search_service.search ãŒãªã„å ´åˆã¯ã€å…ƒã®ã‚³ãƒ¼ãƒ‰ã®ã‚ˆã†ã«ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«è¨˜è¿°ã—ã¾ã™ï¼š
        if not search_results:
             # Embeddingç”Ÿæˆãªã©ï¼ˆçœç•¥ï¼šå…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦ãªã‚‰ã“ã“ã«æˆ»ã™ï¼‰
             # ç°¡æ˜“çš„ãªå®Ÿè£…ä¾‹ã¨ã—ã¦ search_service ã«å§”è­²ã—ã¦ã„ã¾ã™
             pass

        if not search_results:
             yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
             return

        yield send_sse({'status_message': 'âœï¸ å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...'})

        # 3. å›ç­”ç”Ÿæˆ (LLM Service)
        chat_history = history_manager.get_history(session_id)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        context_parts = []
        for idx, doc in enumerate(search_results, 1):
            doc_content = doc.get('content', '')
            context_parts.append(f"<doc id='{idx}'>{doc_content}</doc>")
        context_str = "\n".join(context_parts)

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™
        try:
            full_system_prompt = prompts.SYSTEM_GENERATION.format(
                context_text=context_str,
                current_date=current_date_str
            )
        except KeyError:
             full_system_prompt = prompts.SYSTEM_GENERATION.format(context_text=context_str)

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å›ç­”ã®é–‹å§‹
        ai_response_full = ""
        
        async for chunk in llm_service.generate_response_stream(
            query=user_input,
            context_docs=search_results, # äº’æ›æ€§ã®ãŸã‚æ¸¡ã™
            history=chat_history,
            system_prompt=full_system_prompt # ç”Ÿæˆã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¸¡ã™
        ):
            text_chunk = chunk if isinstance(chunk, str) else chunk.get("content", "")
            ai_response_full += text_chunk
            yield send_sse({'content': text_chunk})

        # 4. å‚ç…§å…ƒãƒªã‚¹ãƒˆã®ç”Ÿæˆã¨é€ä¿¡ (â˜…ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ)
        # metadataã«urlãŒã‚ã‚‹å ´åˆã¯ãƒªãƒ³ã‚¯åŒ–ã•ã‚ŒãŸå‚ç…§ãƒªã‚¹ãƒˆãŒç”Ÿæˆã•ã‚Œã‚‹
        references_text = format_references(search_results)
        
        if references_text:
            # AIã®å›ç­”ã®å¾Œã«æ”¹è¡Œã‚’å…¥ã‚Œã¦å‚ç…§å…ƒã‚’è¿½è¨˜é€ä¿¡
            yield send_sse({'content': references_text})
            # ãƒ­ã‚°ä¿å­˜ç”¨ã«å…¨æ–‡ã«ã‚‚çµåˆã—ã¦ãŠã
            ai_response_full += references_text

        # 5. å±¥æ­´ã«AIã®å›ç­”ã‚’ä¿å­˜
        history_manager.add(session_id, "assistant", ai_response_full)

        # 6. å®Œäº†ã‚·ã‚°ãƒŠãƒ«
        feedback_id = str(uuid.uuid4())
        yield send_sse({'done': True, 'feedback_id': feedback_id})

    except Exception as e:
        log_context(session_id, f"Critical Pipeline Error: {e}", "error")
        if run_tree:
            run_tree.end(error=str(e))
            
        error_str = str(e)
        msg = AI_MESSAGES["SYSTEM_ERROR"]
        yield send_sse({'content': f"\n\n{msg} (Error: {error_str})"})
        
    finally:
        log_context(session_id, "Response generation finished.")