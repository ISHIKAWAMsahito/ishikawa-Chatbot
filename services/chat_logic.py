# services/chat_logic.py
import logging
import uuid
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, AsyncGenerator
from fastapi import Request

# LangSmith ãƒˆãƒ¬ãƒ¼ã‚¹ç”¨
from langsmith import traceable
from langsmith.run_helpers import get_current_run_tree

# ä¾å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from core import database as core_database
from core.constants import PARAMS, AI_MESSAGES
from models.schemas import ChatQuery
from services.llm import LLMService
from services.search import SearchService
from services.storage import StorageService
# utilsã‹ã‚‰æ–°ã—ã„é–¢æ•° format_references ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from services.utils import (
    get_or_create_session_id, 
    send_sse, 
    log_context, 
    ChatHistoryManager, 
    format_urls_as_links,
    format_references 
)
from services import prompts # â˜…ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

# DIï¼ˆä¾å­˜æ€§ã®æ³¨å…¥ï¼‰ã®æº–å‚™
llm_service = LLMService()
search_service = SearchService(llm_service)
storage_service = StorageService()
history_manager = ChatHistoryManager(max_length=PARAMS["MAX_HISTORY_LENGTH"])

@traceable(name="Chat_Pipeline_Parent", run_type="chain")
async def enhanced_chat_logic(request: Request, chat_req: ChatQuery) -> AsyncGenerator[str, None]:
    """
    RAGãƒãƒ£ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯
    """
    session_id = get_or_create_session_id(request)
    user_input = chat_req.question or chat_req.query
    
    # LangSmithç”¨ã®RunTreeå–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç”¨ï¼‰
    run_tree = get_current_run_tree()

    log_context(session_id, f"Start processing query: {user_input}")

    # æ—¥æ™‚å–å¾—ï¼ˆJSTï¼‰
    JST = timezone(timedelta(hours=9), 'JST')
    now = datetime.now(JST)
    current_date_str = now.strftime("%Yå¹´%mæœˆ%dæ—¥")

    # æ¤œç´¢çµæœã‚’ä¿æŒã™ã‚‹å¤‰æ•°
    search_results = []
    
    try:
        # 1. å±¥æ­´ã®è¿½åŠ 
        history_manager.add(session_id, "user", user_input)
        
        yield send_sse({'status_message': 'ğŸ” è³ªå•ã‚’åˆ†æã—ã¦ã„ã¾ã™...'})

        # 2. æ¤œç´¢ (Search Service)
        
        # ã¾ãšã‚¯ã‚¨ãƒªæ‹¡å¼µ
        expanded_query = await search_service.expand_query(user_input)
        
        # æ¤œç´¢å®Ÿè¡Œ
        # â€» search_service.search ãƒ¡ã‚½ãƒƒãƒ‰ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã‚‹å‰æã§å‘¼ã³å‡ºã—
        # å®Ÿè£…ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ search.py ã« search ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ãŒã€
        # ã“ã“ã§ã¯æ—¢å­˜ã® search_service ã®ãƒ¡ã‚½ãƒƒãƒ‰æ§‹æˆã«åˆã‚ã›ã¦é©å®œä¿®æ­£ã—ã¦ãã ã•ã„ã€‚
        # ã‚‚ã— `search` ãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«å€‹åˆ¥ã«å‘¼ã³å‡ºã—ã¾ã™:
        # --- å€‹åˆ¥å‘¼ã³å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³ ---
        # 1. Embedding (çœç•¥) -> 2. DBæ¤œç´¢ (çœç•¥) -> 3. Rerank -> 4. LitM -> 5. Filter
        # ------------------------
        # ã“ã“ã§ã¯ã‚³ãƒ¼ãƒ‰ã®æ•´åˆæ€§ã®ãŸã‚ã€ä»®ã« search_service.search ãŒã‚ã‚‹ã‹ã€
        # ä»¥å‰ã®ã‚³ãƒ¼ãƒ‰ã®ã‚ˆã†ã«å‡¦ç†ã‚’è¨˜è¿°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
        # ä»Šå›ã®ä¿®æ­£ç¯„å›²ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¤–éƒ¨åŒ–ãªã®ã§ã€ãƒ­ã‚¸ãƒƒã‚¯è‡ªä½“ã¯æ—¢å­˜ã®ã‚‚ã®ã‚’ç¶­æŒã—ã¾ã™ã€‚
        
        # (ç°¡æ˜“çš„ãªãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼: å®Ÿéš›ã«ã¯ search.py ã«çµ±åˆ search ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½œã‚‹ã®ãŒãƒ™ã‚¹ãƒˆã§ã™)
        # ä»Šå›ã¯ search.py ã« search ãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„ãŸã‚ã€ã“ã“ã§ã¯è©³ç´°ãªå®Ÿè£…ã‚’å‰²æ„›ã—ã€
        # æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ãŒ search_service å†…ã«ã‚«ãƒ—ã‚»ãƒ«åŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ã€
        # ã‚ã‚‹ã„ã¯ã“ã“ã§å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
        # ã¨ã‚Šã‚ãˆãšç©ºãƒªã‚¹ãƒˆã§åˆæœŸåŒ–ã—ã€æ—¢å­˜ã®å®Ÿè£…ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã„ã¾ã™ã€‚
        
        # â€» å‰å›ã®ã‚³ãƒ¼ãƒ‰ã§ search_service.search ã‚’ä½¿ã£ã¦ã„ãŸå ´åˆã¯ã“ã“ã‚‚ä¿®æ­£ä¸è¦ã§ã™ã€‚
        if hasattr(search_service, 'search'):
             search_result_obj = await search_service.search(
                query=user_input, # ã¾ãŸã¯ expanded_query
                session_id=session_id
             )
             search_results = search_result_obj.get("documents", [])
        else:
             # searchãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„å ´åˆã®ç°¡æ˜“å®Ÿè£…ï¼ˆæœ¬æ¥ã¯ search.py ã«å®Ÿè£…ã™ã¹ãï¼‰
             pass 

        if not search_results:
             yield send_sse({'content': AI_MESSAGES["NOT_FOUND"]})
             # å®Œäº†ã‚·ã‚°ãƒŠãƒ«
             yield send_sse({'done': True, 'feedback_id': str(uuid.uuid4())})
             return

        yield send_sse({'status_message': 'âœï¸ å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...'})

        # 3. å›ç­”ç”Ÿæˆ (LLM Service)
        chat_history = history_manager.get_history(session_id)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        context_parts = []
        for idx, doc in enumerate(search_results, 1):
            doc_content = doc.get('content', '')
            context_parts.append(f"<doc id='{idx}'>{doc_content}</doc>")
        context_str = "\n".join(context_parts)

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™ (â˜…ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ prompts.py ã‹ã‚‰å–å¾—)
        try:
            full_system_prompt = prompts.SYSTEM_GENERATION.format(
                context_text=context_str,
                current_date=current_date_str
            )
        except Exception:
             full_system_prompt = f"ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n{context_str}"

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å›ç­”ã®é–‹å§‹
        ai_response_full = ""
        
        async for chunk in llm_service.generate_response_stream(
            query=user_input,
            context_docs=search_results, 
            history=chat_history,
            system_prompt=full_system_prompt
        ):
            text_chunk = chunk if isinstance(chunk, str) else chunk.get("content", "")
            ai_response_full += text_chunk
            yield send_sse({'content': text_chunk})

        # 4. å‚ç…§å…ƒãƒªã‚¹ãƒˆã®ç”Ÿæˆã¨é€ä¿¡
        references_text = format_references(search_results)
        
        if references_text:
            yield send_sse({'content': references_text})
            ai_response_full += references_text

        # 5. å±¥æ­´ã«AIã®å›ç­”ã‚’ä¿å­˜
        history_manager.add(session_id, "assistant", ai_response_full)

        # 6. å®Œäº†ã‚·ã‚°ãƒŠãƒ«
        feedback_id = str(uuid.uuid4())
        yield send_sse({'done': True, 'feedback_id': feedback_id})

    except Exception as e:
        log_context(session_id, f"Critical Pipeline Error: {e}", "error")
        if run_tree:
            run_tree.end(error=str(e))
            
        error_str = str(e)
        msg = AI_MESSAGES["SYSTEM_ERROR"]
        yield send_sse({'content': f"\n\n{msg} (Error: {error_str})"})
        
    finally:
        log_context(session_id, "Response generation finished.")

@traceable(name="Feedback_Analysis_Job", run_type="chain")
async def analyze_feedback_trends(logs: List[Dict[str, Any]]) -> AsyncGenerator[str, None]:
    """
    ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æç”¨ãƒ­ã‚¸ãƒƒã‚¯
    """
    if not logs:
        yield send_sse({'content': 'åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚'})
        return
    
    summary = "\n".join([f"- è©•ä¾¡:{l.get('rating','-')} | {l.get('comment','-')[:100]}" for l in logs[:50]])
    
    # â˜…ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ prompts.py ã‹ã‚‰å–å¾—
    prompt = prompts.FEEDBACK_ANALYSIS.format(summary=summary)

    try:
        stream = await llm_service.generate_stream(prompt)
        async for chunk in stream:
            text = chunk.text if hasattr(chunk, 'text') else str(chunk)
            if text:
                yield send_sse({'content': text})
    except Exception as e:
        yield send_sse({'content': f'åˆ†æã‚¨ãƒ©ãƒ¼: {e}'})